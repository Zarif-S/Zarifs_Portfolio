{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cNnDarkzKmYY",
        "3x_eMpzna8PU",
        "sXOAfyRJePms",
        "3zkMXq5EeY-6",
        "yTOvOQNh52vX",
        "fQtdEA6k87z-",
        "LkZcg9CSBBf0",
        "0j5_bewdKJ1D",
        "TXMEFq-fe1nd",
        "lKfpwmdr-9pt",
        "4b-hu5DL6vzr",
        "LOAsFyEh6xhX"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic project 4.1 Applying NLP for topic modelling in a real-life context**\n",
        "\n",
        "# Foreword: For anonymytiy & confidentiality reasons the data and outputs are cleared, with only the code and general summaries remaining\n",
        "\n",
        "In this project, I  bridge the gap between theory and practical application by developing automated topic modelling tools tailored to a specific industry context.\n",
        "\n",
        "Applying NLP for topic modelling is crucial for data analysis in business because it enables companies to identify and understand key themes and patterns within large volumes of text data. This efficiency allows businesses to extract essential insights and trends without manually sifting through extensive documents. Automated topic modelling helps businesses make informed decisions faster, which helps to improve productivity and gain a competitive edge. Additionally, it supports better information management by uncovering underlying topics in reports, emails, customer feedback, and market research, which enhances overall business intelligence and strategic planning.\n",
        "\n",
        "Approximately **19 hours** to complete this project.\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Business context**\n",
        "\n",
        "This project was on an unnamed gym, that offers customers high-quality, low-cost, and flexible fitness facilities. The company’s customer-centric proposition – affordable membership fees, no fixed-term contracts, and 24/7 access to high-quality gyms – differentiates it from more traditional gyms and elevates it as a market leader within the space.\n",
        "\n",
        "This focus on the customer is centred on wanting to understand what motivates members to join and what factors influence their behaviours once they have joined.\n",
        "\n",
        "Understanding how to leverage innovative technology to influence, improve, and simplify their experience allows this gym to foster an open, welcoming, and diverse environment for its members while maintaining their value proposition.\n",
        "\n",
        "With the shift in focus to value-for-money memberships across the gym industry,\n",
        "this gym aims provideg members with affordable access to the benefits being healthy can offer’.\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "To review the gyms data to uncover key drivers that provide actionable insights for enhancing customer experience.\n",
        "\n",
        "In this project I:\n",
        "\n",
        "- Use two data sets containing customer reviews from Google and Trustpilot.\n",
        "- Perform basic level analysis by finding the frequently used words in both data sets.\n",
        "- Generate a wordcloud to visualise the most frequently used words in the reviews.\n",
        "- Apply BERTopic for topic modelling, keeping track of gym locations, to identify common topics and words in the negative reviews.\n",
        "- Identify the locations that have the most negative reviews.\n",
        "- Use the built-in visualisation functions in BERTopic to cluster and visually represent the topics and words in these reviews, thereby helping to identify specific themes from the reviews.\n",
        "- Conduct a comparison with Gensim’s LDA model to validate the topic modelling results.\n",
        "- Perform emotion analysis to identify the emotions associated with customer reviews.\n",
        "- Filter out angry reviews and apply BERTopic to discover prevalent topics and words being discussed these negative reviews.\n",
        "- Leverage the multi-purpose capability of the state-of-the-art Falcon-7b-instruct model, with the help of prompts, to identify top topics in each review.\n",
        "- Use a different prompt with the Falcon-7b-instruct model to further generate suggestions for improvements, based on the top topics identified from the negative reviews.\n",
        "\n",
        "<br>\n",
        "\n",
        "## **This project demonstrates that I can:**\n",
        "\n",
        "- Investigate real-world data to find potential trends for deeper investigation.\n",
        "-Preprocess and refine textual data for visualisations.\n",
        "-Apply topic modelling using various techniques.\n",
        "-Apply emotion analysis using BERT.\n",
        "-Evaluate the outcomes of my investigation.\n",
        "-Communicate actionable insights."
      ],
      "metadata": {
        "id": "9j5Bse6izKKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import packages and data:\n",
        "\n",
        "* Along with some simpple data cleaning: Removing any rows with missing values in the Comment column (Google review) and Review Content column (Trustpilot)"
      ],
      "metadata": {
        "id": "cNnDarkzKmYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "2-27HH3CRDj_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCG-oEq1Os8M",
        "outputId": "38f2457e-7abc-4360-9e23-b0275d576f3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78a8443c71b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import FreqDist"
      ],
      "metadata": {
        "collapsed": true,
        "id": "03QdstrDn1hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "destination_TP = 'Trustpilot.xlsx'\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = f''\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(download_url, destination_TP, quiet=False)\n",
        "\n",
        "destination_G = 'Google.xlsx'\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = f''\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(download_url, destination_G, quiet=False)"
      ],
      "metadata": {
        "id": "OW-0FMMsMze9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot = pd.read_excel('Trustpilot.xlsx', sheet_name='65bcde732b27bf001a58fef2_821ec1')\n",
        "\n",
        "#df_google = pd.read_excel('Google.xlsx', sheet_name='Sheet0')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HFTRME5GTUhf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Read all tables in the HTML file without specifying headers\n",
        "    df_list = pd.read_html('Google.xlsx')\n",
        "    df_google = df_list[0]  # Assuming the table you want is the first one\n",
        "\n",
        "    # Set the first row as header and reset index\n",
        "    df_google.columns = df_google.iloc[0]\n",
        "    df_google = df_google[1:].reset_index(drop=True)\n",
        "\n",
        "    # Print the first few rows of the DataFrame to verify the column headers\n",
        "    print(df_google.head())\n",
        "except Exception as e:\n",
        "    print(f\"Error reading HTML: {e}\")"
      ],
      "metadata": {
        "id": "7cwRXg8_VC2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_trustpilot.head(3))\n",
        "\n",
        "display(df_google.head(3))"
      ],
      "metadata": {
        "id": "Fd4eftLtRqeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a copy as this will be useful in part 6) where I tokenize using an autotokenizer from my transformer model rather than NLTK"
      ],
      "metadata": {
        "id": "fvoV9zHisS4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot_unprocessed = df_trustpilot.copy()\n",
        "df_google_unprocessed = df_google.copy()"
      ],
      "metadata": {
        "id": "_ax3SLIoOnNo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning - dropping of rows with missing values in the comments column"
      ],
      "metadata": {
        "id": "iut0AKi_YYEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the 'Comment' column\n",
        "missing_values_count = df_google['Comment'].isna().sum()\n",
        "print(f\"Number of missing values in 'Comment' column: {missing_values_count}\")\n",
        "\n",
        "# Remove rows where 'Comment' is missing\n",
        "df_google = df_google.dropna(subset=['Comment'])\n",
        "\n",
        "# Drop the column with integer name 1\n",
        "df_google = df_google.drop(columns=[1])"
      ],
      "metadata": {
        "id": "54JJQCD7YRmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google.head(3))"
      ],
      "metadata": {
        "id": "JYsqa3w_YWB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the 'Review Content' column\n",
        "missing_values_count = df_trustpilot['Review Content'].isna().sum()\n",
        "print(f\"Number of missing values in 'Review Content' column: {missing_values_count}\")\n",
        "\n",
        "# Remove rows where 'Review Content' is missing\n",
        "df_trustpilot = df_trustpilot.dropna(subset=['Review Content'])"
      ],
      "metadata": {
        "id": "6x-x3EipZ3HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values_count = df_trustpilot.isna().sum()\n",
        "print(f\"Number of missing values in 'df_trustpilot': {missing_values_count}\")"
      ],
      "metadata": {
        "id": "qqhUW3OjafHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Conducting initial data investigation:\n"
      ],
      "metadata": {
        "id": "3x_eMpzna8PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Review of data sets locations\n",
        "\n",
        "Finding the number of unique locations in the data sets\n",
        "\n",
        "Finding the no. common locations across the two data sets"
      ],
      "metadata": {
        "id": "sXOAfyRJePms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google.head(5))\n",
        "\n",
        "display(df_trustpilot.head(1))"
      ],
      "metadata": {
        "id": "hHUzAnakboFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the column name\n",
        "google_column_name = \"Club's Name\"\n",
        "trustpilot_column_name = \"Location Name\"\n",
        "\n",
        "# Print the number of unique locations\n",
        "print(f\"Number of unique locations in Google data set: {df_google[google_column_name].nunique()}\")\n",
        "\n",
        "# Print the number of unique locations\n",
        "print(f\"Number of unique locations in Trustpilot data set: {df_trustpilot[trustpilot_column_name].nunique()}\")"
      ],
      "metadata": {
        "id": "I2BZg0Jeb3gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google[\"Club's Name\"].unique()"
      ],
      "metadata": {
        "id": "uhkSZptMuUOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gyms not in the uk:\n",
        "\n",
        "Redacted\n"
      ],
      "metadata": {
        "id": "31Dv0tA1uzPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_trustpilot[\"Review Language\"].unique())\n",
        "\n",
        "# Assuming df_trustpilot is your DataFrame\n",
        "language_counts = df_trustpilot[\"Review Language\"].value_counts()\n",
        "print(language_counts)"
      ],
      "metadata": {
        "id": "LUIrhzN5ugtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_trustpilot is your DataFrame\n",
        "language_counts = df_trustpilot[\"Review Language\"].value_counts()\n",
        "\n",
        "# Sum the counts of all non-English languages\n",
        "non_en_count = language_counts.drop('en').sum()\n",
        "\n",
        "print(f\"Total count of non-English reviews: {non_en_count}\")"
      ],
      "metadata": {
        "id": "9BNjBljzeks3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Could remove these non english ones as so far using transformer only in en\n",
        "\n",
        "Saying that, when I create a df with common locations they are automatically removed as not present in google reviews / intersection"
      ],
      "metadata": {
        "id": "dCwPnkYzegqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot[\"Location Name\"].unique()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rgs8m7CJfVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of locations to check\n",
        "locations_to_check = [\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    ''\n",
        "]\n",
        "\n",
        "# Check if the locations are present in df_trustpilot[\"Location Name\"]\n",
        "locations_present = df_trustpilot[\"Location Name\"].isin(locations_to_check)\n",
        "\n",
        "# Get the actual locations found in the DataFrame\n",
        "found_locations = df_trustpilot.loc[locations_present, \"Location Name\"].unique()\n",
        "\n",
        "# Print the results\n",
        "for location in locations_to_check:\n",
        "    if location in found_locations:\n",
        "        print(f\"'{location}' is present in df_trustpilot['Location Name']\")\n",
        "    else:\n",
        "        print(f\"'{location}' is NOT present in df_trustpilot['Location Name']\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DebhrincfpF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique location names from each data frame\n",
        "google_locations = set(df_google[google_column_name].dropna().unique())\n",
        "trustpilot_locations = set(df_trustpilot[trustpilot_column_name].dropna().unique())\n",
        "\n",
        "# Find common locations\n",
        "common_locations = google_locations.intersection(trustpilot_locations)\n",
        "\n",
        "# Print the number of common locations and their names\n",
        "print(f\"Number of common locations: {len(common_locations)}\")\n",
        "print(\"Common locations:\")\n",
        "print(common_locations)"
      ],
      "metadata": {
        "id": "jvOG0AJdfg6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Preprocessing of the data\n",
        "\n",
        "Change to lower case, tokenise the data using word_tokenize from NLTK, remove stopwords, and remove numbers.\n",
        "\n"
      ],
      "metadata": {
        "id": "3zkMXq5EeY-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "delete below next time you run this and uncomment that lower bit"
      ],
      "metadata": {
        "id": "TTITAuLOqukk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google.head(2))\n",
        "\n",
        "display(df_trustpilot.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "collapsed": true,
        "id": "YSMC76byhj_6",
        "outputId": "65c92cb1-049b-47b8-97ae-c1b93563792f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0 Customer Name SurveyID for external use (e.g. tech support)  \\\n",
              "1            **                      e9b62vyxtkwrrrfyzc5hz6rk   \n",
              "2            **                      e2dkxvyxtkwrrrfyzc5hz6rk   \n",
              "\n",
              "0             Club's Name Social Media Source Creation Date  \\\n",
              "1  Cambridge Leisure Park      Google Reviews  5/9/24 22:48   \n",
              "2          London Holborn      Google Reviews  5/9/24 22:08   \n",
              "\n",
              "0                                            Comment Overall Score  \n",
              "1  Too many students from two local colleges go h...             1  \n",
              "2  Best range of equipment, cheaper than regular ...             5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6f3dd3f-65c3-4686-ab7f-13936540c126\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>SurveyID for external use (e.g. tech support)</th>\n",
              "      <th>Club's Name</th>\n",
              "      <th>Social Media Source</th>\n",
              "      <th>Creation Date</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Overall Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>**</td>\n",
              "      <td>e9b62vyxtkwrrrfyzc5hz6rk</td>\n",
              "      <td>Cambridge Leisure Park</td>\n",
              "      <td>Google Reviews</td>\n",
              "      <td>5/9/24 22:48</td>\n",
              "      <td>Too many students from two local colleges go h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>**</td>\n",
              "      <td>e2dkxvyxtkwrrrfyzc5hz6rk</td>\n",
              "      <td>London Holborn</td>\n",
              "      <td>Google Reviews</td>\n",
              "      <td>5/9/24 22:08</td>\n",
              "      <td>Best range of equipment, cheaper than regular ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6f3dd3f-65c3-4686-ab7f-13936540c126')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6f3dd3f-65c3-4686-ab7f-13936540c126 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6f3dd3f-65c3-4686-ab7f-13936540c126');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1316654f-495e-479f-8fec-2b6bb507c42d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1316654f-495e-479f-8fec-2b6bb507c42d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1316654f-495e-479f-8fec-2b6bb507c42d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                  Review ID Review Created (UTC)   Review Consumer User ID  \\\n",
              "0  663d40378de0a14c26c2f63c  2024-05-09 23:29:00  663d4036d5fa24c223106005   \n",
              "1  663d3c101ccfcc36fb28eb8c  2024-05-09 23:11:00  5f5e3434d53200fa6ac57238   \n",
              "\n",
              "                    Review Title  \\\n",
              "0        A very good environment   \n",
              "1  I love to be part of this gym   \n",
              "\n",
              "                                      Review Content  Review Stars  \\\n",
              "0                            A very good environment             5   \n",
              "1  I love to be part of this gym. Superb value fo...             5   \n",
              "\n",
              "  Source Of Review Review Language              Domain URL Webshop Name  \\\n",
              "0            AFSv2              en  http://www.puregym.com   PureGym UK   \n",
              "1            AFSv2              en  http://www.puregym.com   PureGym UK   \n",
              "\n",
              "           Business Unit ID  Tags Company Reply Date (UTC)  \\\n",
              "0  508df4ea00006400051dd7b1   NaN      2024-05-10 08:12:00   \n",
              "1  508df4ea00006400051dd7b1   NaN      2024-05-10 08:13:00   \n",
              "\n",
              "                Location Name                           Location ID  \n",
              "0  Solihull Sears Retail Park  7b03ccad-4a9d-4a33-9377-ea5bba442dfc  \n",
              "1                   Aylesbury  612d3f7e-18f9-492b-a36f-4a7b86fa5647  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5810066f-debb-43d9-9357-916585c28a85\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review ID</th>\n",
              "      <th>Review Created (UTC)</th>\n",
              "      <th>Review Consumer User ID</th>\n",
              "      <th>Review Title</th>\n",
              "      <th>Review Content</th>\n",
              "      <th>Review Stars</th>\n",
              "      <th>Source Of Review</th>\n",
              "      <th>Review Language</th>\n",
              "      <th>Domain URL</th>\n",
              "      <th>Webshop Name</th>\n",
              "      <th>Business Unit ID</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Company Reply Date (UTC)</th>\n",
              "      <th>Location Name</th>\n",
              "      <th>Location ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>663d40378de0a14c26c2f63c</td>\n",
              "      <td>2024-05-09 23:29:00</td>\n",
              "      <td>663d4036d5fa24c223106005</td>\n",
              "      <td>A very good environment</td>\n",
              "      <td>A very good environment</td>\n",
              "      <td>5</td>\n",
              "      <td>AFSv2</td>\n",
              "      <td>en</td>\n",
              "      <td>http://www.puregym.com</td>\n",
              "      <td>PureGym UK</td>\n",
              "      <td>508df4ea00006400051dd7b1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-05-10 08:12:00</td>\n",
              "      <td>Solihull Sears Retail Park</td>\n",
              "      <td>7b03ccad-4a9d-4a33-9377-ea5bba442dfc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>663d3c101ccfcc36fb28eb8c</td>\n",
              "      <td>2024-05-09 23:11:00</td>\n",
              "      <td>5f5e3434d53200fa6ac57238</td>\n",
              "      <td>I love to be part of this gym</td>\n",
              "      <td>I love to be part of this gym. Superb value fo...</td>\n",
              "      <td>5</td>\n",
              "      <td>AFSv2</td>\n",
              "      <td>en</td>\n",
              "      <td>http://www.puregym.com</td>\n",
              "      <td>PureGym UK</td>\n",
              "      <td>508df4ea00006400051dd7b1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-05-10 08:13:00</td>\n",
              "      <td>Aylesbury</td>\n",
              "      <td>612d3f7e-18f9-492b-a36f-4a7b86fa5647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5810066f-debb-43d9-9357-916585c28a85')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5810066f-debb-43d9-9357-916585c28a85 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5810066f-debb-43d9-9357-916585c28a85');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fe8e3653-c11b-4ce8-8670-16e76204e30c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe8e3653-c11b-4ce8-8670-16e76204e30c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fe8e3653-c11b-4ce8-8670-16e76204e30c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_trustpilot\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Review ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"663d3c101ccfcc36fb28eb8c\",\n          \"663d40378de0a14c26c2f63c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review Created (UTC)\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-05-09 23:11:00\",\n        \"max\": \"2024-05-09 23:29:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2024-05-09 23:11:00\",\n          \"2024-05-09 23:29:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review Consumer User ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"5f5e3434d53200fa6ac57238\",\n          \"663d4036d5fa24c223106005\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"I love to be part of this gym\",\n          \"A very good environment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review Content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"I love to be part of this gym. Superb value for money. Any time, any day. Love the app too, well organised building equipment, studio area, cycling area and more important feel safe.\",\n          \"A very good environment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review Stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Source Of Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AFSv2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review Language\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Domain URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"http://www.puregym.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Webshop Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"PureGym UK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Business Unit ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"508df4ea00006400051dd7b1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tags\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Company Reply Date (UTC)\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-05-10 08:12:00\",\n        \"max\": \"2024-05-10 08:13:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Location Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Location ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def check_and_convert_to_lowercase(df, column_name):\n",
        "    # Check if there are any uppercase characters before conversion\n",
        "    contains_uppercase_before = df[column_name].apply(lambda x: any(char.isupper() for char in x))\n",
        "\n",
        "    # Print the rows that contain uppercase characters before conversion\n",
        "    print(\"Rows with uppercase characters before conversion:\")\n",
        "    print(df[contains_uppercase_before])\n",
        "\n",
        "    # Convert the specified column to lowercase\n",
        "    df[column_name] = df[column_name].str.lower()\n",
        "\n",
        "    # Check if there are any uppercase characters after conversion\n",
        "    contains_uppercase_after = df[column_name].apply(lambda x: any(char.isupper() for char in x))\n",
        "\n",
        "    # Print the rows that still contain uppercase characters\n",
        "    print(\"Rows with uppercase characters after conversion:\")\n",
        "    print(df[contains_uppercase_after])\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "mlqyyCmSioVM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f4f90392-379e-4877-e757-988020921089"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef check_and_convert_to_lowercase(df, column_name):\\n    # Check if there are any uppercase characters before conversion\\n    contains_uppercase_before = df[column_name].apply(lambda x: any(char.isupper() for char in x))\\n\\n    # Print the rows that contain uppercase characters before conversion\\n    print(\"Rows with uppercase characters before conversion:\")\\n    print(df[contains_uppercase_before])\\n\\n    # Convert the specified column to lowercase\\n    df[column_name] = df[column_name].str.lower()\\n\\n    # Check if there are any uppercase characters after conversion\\n    contains_uppercase_after = df[column_name].apply(lambda x: any(char.isupper() for char in x))\\n\\n    # Print the rows that still contain uppercase characters\\n    print(\"Rows with uppercase characters after conversion:\")\\n    print(df[contains_uppercase_after])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check_and_convert_to_lowercase(df_google, column_name='Comment')"
      ],
      "metadata": {
        "id": "YsY5cRffkjZF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check_and_convert_to_lowercase(df_trustpilot, column_name='Review Content')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ShfUr8YCkqYb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display(df_google.head(2))\n",
        "\n",
        "#display(df_trustpilot.head(2))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QgNcpsGXlH3p"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Define the column names\n",
        "google_column_name = \"Comment\"\n",
        "trustpilot_column_name = \"Review Content\"\n",
        "\n",
        "# Print 3 rows from the 'Comment' column in df_google\n",
        "print(\"Sample rows from Google DataFrame:\")\n",
        "print(df_google[[google_column_name]].head(3))\n",
        "\n",
        "# Print 3 rows from the 'Review Content' column in df_trustpilot\n",
        "print(\" \\n Sample rows from Trustpilot DataFrame:\")\n",
        "print(df_trustpilot[[trustpilot_column_name]].head(3))\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "AElMtcq0mMLD",
        "outputId": "a14f43ef-8e1b-42d4-f13b-8252ae647f5c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Define the column names\\ngoogle_column_name = \"Comment\"\\ntrustpilot_column_name = \"Review Content\"\\n\\n# Print 3 rows from the \\'Comment\\' column in df_google\\nprint(\"Sample rows from Google DataFrame:\")\\nprint(df_google[[google_column_name]].head(3))\\n\\n# Print 3 rows from the \\'Review Content\\' column in df_trustpilot\\nprint(\" \\n Sample rows from Trustpilot DataFrame:\")\\nprint(df_trustpilot[[trustpilot_column_name]].head(3))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "dpsHzBZIpiPZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_nlp(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by tokenizing, converting to lowercase,\n",
        "    removing stopwords (including some custom ones), and filtering out numbers.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        list: The processed tokens.\n",
        "    \"\"\"\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Define stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Add custom stopwords inside the function\n",
        "    #custom_stopwords = {',', '\"', \"'\", '.', ':', ';', '-', '(', ')', '[', ']', '{', '}'}\n",
        "    #stop_words.update(custom_stopwords)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Remove numbers\n",
        "    filtered_numbers = [token for token in filtered_tokens if not token.isdigit()]\n",
        "\n",
        "    return filtered_numbers"
      ],
      "metadata": {
        "id": "XxQklGdulcTP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google.head(2))\n",
        "display(df_trustpilot.head(2))"
      ],
      "metadata": {
        "id": "1rjhNp_XrAiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google['Comment'] = df_google['Comment'].apply(preprocess_nlp)\n",
        "df_trustpilot['Review Content'] = df_trustpilot['Review Content'].apply(preprocess_nlp)"
      ],
      "metadata": {
        "id": "-GbUuqJtq0by"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google.head(2))\n",
        "display(df_trustpilot.head(2))"
      ],
      "metadata": {
        "id": "JxwHXgNVrGJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 EDA/ Visualisation\n",
        "\n",
        "- Frequency distribution of the words from each data set's reviews.\n",
        "- Histogram/bar plot showing the top 10 words from each data set."
      ],
      "metadata": {
        "id": "yTOvOQNh52vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_words(df, column_name, top_n=10, title=None):\n",
        "    \"\"\"\n",
        "    Plot a histogram of the top N words by frequency from a specified column in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The DataFrame containing the text data.\n",
        "    - column_name: The name of the column containing the tokenized text.\n",
        "    - top_n: The number of top words to display in the plot (default is 10).\n",
        "    - title: Optional custom title for the plot.\n",
        "    \"\"\"\n",
        "    # Flatten all tokens into a single list\n",
        "    all_tokens = [token for sublist in df[column_name] for token in sublist]\n",
        "\n",
        "    # Create a frequency distribution object\n",
        "    freq_dist = FreqDist(all_tokens)\n",
        "\n",
        "    # Get the top N most common tokens\n",
        "    top_n_tokens = freq_dist.most_common(top_n)\n",
        "\n",
        "    # Separate the tokens and their counts\n",
        "    words, counts = zip(*top_n_tokens)\n",
        "\n",
        "    # Determine the title\n",
        "    if title is None:\n",
        "        title = f'Top {top_n} Words by Frequency in DataFrame'\n",
        "    else:\n",
        "        title = f'Top {top_n} Words by Frequency in DataFrame - {title}'\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sRLd7DOcsMzW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_top_words(df_google, column_name='Comment', top_n=10, title='Google')\n",
        "\n",
        "plot_top_words(df_trustpilot, column_name='Review Content', top_n=10, title='Trustpilot')"
      ],
      "metadata": {
        "id": "_-xsymrkt_XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "OJ0iegEZ7oZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "34EMdve07paH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_wordcloud(df, column_name, title=None):\n",
        "    \"\"\"\n",
        "    Generate and plot a word cloud from a DataFrame column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The DataFrame containing the text data.\n",
        "    - column_name: The name of the column containing the tokenized text.\n",
        "    - title: Optional title for the word cloud plot.\n",
        "    \"\"\"\n",
        "    # Join all text data into a single string\n",
        "    text = ' '.join([' '.join(sublist) for sublist in df[column_name]])\n",
        "    # combines all lists of tokens into a single string suitable for generating the word cloud.\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "    # Plot the word cloud\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4nrXu_Gj7rDE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(df_google, 'Comment', title='Word Cloud for Google Reviews')\n",
        "plot_wordcloud(df_trustpilot, 'Review Content', title='Word Cloud for Trustpilot Reviews')"
      ],
      "metadata": {
        "id": "AB7-jshQ7-LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Wordcloud for negative only reviews\n",
        "\n",
        "Create a new dataframe by filtering out the data to extract only the negative reviews from both data sets.\n",
        "\n",
        "  • For Google reviews, overall scores < 3 can be considered negative scores.\n",
        "\n",
        "  • For Trustpilot reviews, stars < 3 can be considered negative scores.\n",
        "\n",
        "  Repeat the frequency distribution and wordcloud steps on the filtered data consisting of only negative reviews."
      ],
      "metadata": {
        "id": "fQtdEA6k87z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_google.head(2)"
      ],
      "metadata": {
        "id": "0lQG7YdD9OHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check current data type of 'Overall Score'\n",
        "print(\"Current data type of 'Overall Score':\")\n",
        "print(df_google['Overall Score'].dtype)\n",
        "\n",
        "print(df_google['Overall Score'].unique())\n",
        "# If scores were objects with decimal points would have converted to float, instead they are all int so its easy to handle\n",
        "\n",
        "# Check for missing values in 'Overall Score'\n",
        "print(\"Missing values in 'Overall Score':\")\n",
        "print(df_google['Overall Score'].isna().sum())\n",
        "# No missing values so did not need to handle that\n",
        "\n",
        "# Convert 'Overall Score' directly to integer\n",
        "df_google['Overall Score'] = df_google['Overall Score'].astype(int)\n",
        "\n",
        "# Verify the conversion\n",
        "print(\"\\nDataFrame after conversion to integer:\")\n",
        "print(df_google)\n",
        "print(\"\\nData type of 'Overall Score' after conversion:\")\n",
        "print(df_google['Overall Score'].dtype)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GZuqhJVl-vFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_negative = df_google[df_google['Overall Score'] < 3]\n",
        "df_trustpilot_negative = df_trustpilot[df_trustpilot['Review Stars'] < 3]"
      ],
      "metadata": {
        "id": "99s1v2ti9AML"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_top_words(df_google_negative, column_name='Comment', top_n=10, title='Google - Negative')\n",
        "plot_top_words(df_trustpilot_negative, column_name='Review Content', top_n=10, title='Trustpilot - Negative')"
      ],
      "metadata": {
        "id": "X8pWmjaqAAsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(df_google_negative, 'Comment', title='Word Cloud for Google Reviews - Negative')\n",
        "plot_wordcloud(df_trustpilot_negative, 'Review Content', title='Word Cloud for Trustpilot Reviews - Negative')"
      ],
      "metadata": {
        "id": "J86Q1nJV_3KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Conducting initial topic modelling:"
      ],
      "metadata": {
        "id": "LkZcg9CSBBf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1\n",
        "\n",
        "- Filtering out the reviews that are from the locations common to both data sets.\n",
        "Merging the reviews to form a new list\n",
        "\n",
        "- Preprocessing\n",
        "- Applying BERTopic\n",
        "- Top topics, words and frequencies\n",
        "- Visualising of the topics to identify clusters of topics and to understand the intertopic distance map\n",
        "- Barchart of the topics\n",
        "- Heatmap of similarity matrix."
      ],
      "metadata": {
        "id": "0j5_bewdKJ1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline"
      ],
      "metadata": {
        "id": "GlKcHFswh6sp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google_negative.head(2))\n",
        "display(df_trustpilot_negative.head(2))"
      ],
      "metadata": {
        "id": "t2sDgwhGCbTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the set to a list\n",
        "common_locations_list = list(common_locations)\n",
        "\n",
        "# Filter the DataFrame\n",
        "df_google_neg_common = df_google_negative[df_google_negative[\"Club's Name\"].isin(common_locations_list)]\n",
        "\n",
        "# Filter the DataFrame\n",
        "df_trustpilot_neg_common = df_trustpilot_negative[df_trustpilot_negative[\"Location Name\"].isin(common_locations_list)]\n",
        "\n",
        "# Filter the DataFrame for rows where 'Club\\'s Name' is not in the common_locations_list\n",
        "df_google_neg_uncommon = df_google_negative[~df_google_negative[\"Club's Name\"].isin(common_locations_list)]\n",
        "\n",
        "# Filter the DataFrame for rows where 'Club\\'s Name' is not in the common_locations_list\n",
        "df_trustpilot_neg_uncommon = df_trustpilot_negative[~df_trustpilot_negative[\"Location Name\"].isin(common_locations_list)]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4qTT_83_CxZB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google_neg_common.head(2))\n",
        "display(df_trustpilot_neg_common.head(2))\n",
        "\n",
        "display(df_google_neg_uncommon.head(2))\n",
        "display(df_trustpilot_neg_uncommon.head(2))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7gR13WYlCwJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_dataframes(df1, df2, df1_cols, df2_cols, rename_dict=None):\n",
        "    \"\"\"\n",
        "    Concatenates two DataFrames along rows, retaining specified columns from each DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df1: The first DataFrame.\n",
        "    - df2: The second DataFrame.\n",
        "    - df1_cols: List of columns to retain from df1.\n",
        "    - df2_cols: List of columns to retain from df2.\n",
        "    - rename_dict: Optional dictionary to rename columns in df2 to match df1.\n",
        "\n",
        "    Returns:\n",
        "    - Concatenated DataFrame.\n",
        "    \"\"\"\n",
        "    # Select the relevant columns from df1\n",
        "    df1_subset = df1[df1_cols]\n",
        "\n",
        "    # Select the relevant columns from df2\n",
        "    df2_subset = df2[df2_cols]\n",
        "\n",
        "    # Rename columns in df2 if rename_dict is provided\n",
        "    if rename_dict:\n",
        "        df2_subset = df2_subset.rename(columns=rename_dict)\n",
        "\n",
        "    # Add a column to identify the source DataFrame\n",
        "    df1_subset['Source'] = 'df1'\n",
        "    df2_subset['Source'] = 'df2'\n",
        "\n",
        "    # Concatenate the DataFrames\n",
        "    concatenated_df = pd.concat([df1_subset, df2_subset], ignore_index=True)\n",
        "\n",
        "    return concatenated_df"
      ],
      "metadata": {
        "id": "JZCxL25PRR3X"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_cols = [\"Club's Name\", \"Creation Date\", \"Overall Score\", \"Comment\"]\n",
        "df_trustpilot_cols = [\"Review Created (UTC)\", \"Review Title\", \"Review Content\", \"Review Stars\", \"Company Reply Date (UTC)\", \"Location Name\"]\n",
        "rename_dict = {\"Location Name\": \"Club's Name\"}\n",
        "\n",
        "merged_common = concatenate_dataframes(df_google_neg_common, df_trustpilot_neg_common, df_google_cols, df_trustpilot_cols, rename_dict)\n",
        "merged_uncommon = concatenate_dataframes(df_google_neg_uncommon, df_trustpilot_neg_uncommon, df_google_cols, df_trustpilot_cols, rename_dict)"
      ],
      "metadata": {
        "id": "jgBYRLtJJkNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(merged_common.head(2))\n",
        "display(merged_uncommon.head(2))"
      ],
      "metadata": {
        "id": "HabAkipuJqE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_common"
      ],
      "metadata": {
        "id": "68FfaZcAQqM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T95yFcoROCXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "lN0z0FdIOFeP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_common['Comment']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8z5AfDjQbAmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_common['Review Content']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SHwfFHLzbGOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns expected to contain lists that need NaN values filled with empty lists\n",
        "#list_columns = ['Comment', 'Review Content']\n",
        "\n",
        "# Fill NaN values with empty lists for specific columns\n",
        "#for col in list_columns:\n",
        "#    merged_common[col] = merged_common[col].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Fill NaN values with empty lists\n",
        "merged_common['Comment'] = merged_common['Comment'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "merged_common['Review Content'] = merged_common['Review Content'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Combine the tokens from both columns row-wise into single documents\n",
        "documents = [\n",
        "    \" \".join((comment if isinstance(comment, list) else []) +\n",
        "             (review_content if isinstance(review_content, list) else []))\n",
        "    for comment, review_content in zip(merged_common['Comment'], merged_common['Review Content'])\n",
        "]\n",
        "\n",
        "# Check the first few documents\n",
        "print(f\"First few documents:\\n{documents[:10]}\")\n",
        "\n",
        "# Check the total number of documents\n",
        "print(f\"Total Documents: {len(documents)}\")"
      ],
      "metadata": {
        "id": "KKpo0sP_XUVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "gNneW9-9Suej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_invalid_entries(documents):\n",
        "    empty_entries = [i for i, doc in enumerate(documents) if not doc.strip()]\n",
        "    nan_entries = [i for i, doc in enumerate(documents) if pd.isna(doc)]\n",
        "    valid_entries = [i for i, doc in enumerate(documents) if doc.strip() and not pd.isna(doc)]\n",
        "\n",
        "    print(f\"Total Documents: {len(documents)}\")\n",
        "    print(f\"Empty Entries: {len(empty_entries)}\")\n",
        "    print(f\"NaN Entries: {len(nan_entries)}\")\n",
        "    print(f\"Valid Entries: {len(valid_entries)}\")\n",
        "\n",
        "    return empty_entries, nan_entries, valid_entries\n",
        "\n",
        "check_invalid_entries(documents)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8CZoobcfZC16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the BERTopic model\n",
        "model = BERTopic(embedding_model=SentenceTransformer(\"paraphrase-MiniLM-L6-v2\"))\n",
        "\n",
        "# Fit the model to your documents\n",
        "topics, probabilities = model.fit_transform(documents)\n",
        "\n",
        "# Display topics\n",
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "SLEY2sGrOVHD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "e0OyprlSYBQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic_info()"
      ],
      "metadata": {
        "id": "GA4WwyuRlGOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sizeable number of outliers (topic -1) and should typically be ignored. Let's take a look at the most frequent topic that was generated, topic 0"
      ],
      "metadata": {
        "id": "gHulytallYng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(0)"
      ],
      "metadata": {
        "id": "Vr1s6bejlVQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(1)"
      ],
      "metadata": {
        "id": "jiu0Ja2nljbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract topic-document mappings\n",
        "topic_docs = model.get_representative_docs()\n",
        "\n",
        "# Create a list of dictionaries to convert to a DataFrame\n",
        "data = []\n",
        "for topic, docs in topic_docs.items():\n",
        "    for doc in docs:\n",
        "        data.append({'Topic': topic, 'Document': doc, 'Probability': 1.0})  # Assuming probability is not available, use a placeholder\n",
        "\n",
        "# Convert to DataFrame\n",
        "topics_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame to understand its structure\n",
        "print(topics_df.head())\n",
        "\n",
        "# For each topic, get the top 10 documents\n",
        "top_docs_per_topic = {}\n",
        "for topic in topics_df['Topic'].unique():\n",
        "    topic_docs_df = topics_df[topics_df['Topic'] == topic]\n",
        "    # Sort documents based on their probabilities\n",
        "    sorted_docs = topic_docs_df.sort_values(by='Probability', ascending=False)\n",
        "    top_docs_per_topic[topic] = sorted_docs.head(10)  # Get top 10 documents\n",
        "\n",
        "# Print the top 10 documents for each topic\n",
        "for topic, top_docs in top_docs_per_topic.items():\n",
        "    print(f\"Topic {topic}:\")\n",
        "    for _, row in top_docs.iterrows():\n",
        "        print(f\"Document: {row['Document']}\\nProbability: {row['Probability']}\\n\")"
      ],
      "metadata": {
        "id": "Bderw9vchOpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic information\n",
        "topic_info = model.get_topic_info()\n",
        "\n",
        "# Identify the top 2 topics based on frequency\n",
        "top_topics = topic_info.head(3)['Topic'].tolist()\n",
        "\n",
        "# Get top words for these topics\n",
        "for topic in top_topics:\n",
        "    top_words = model.get_topic(topic)\n",
        "    print(f\"Top words for Topic {topic}:\")\n",
        "    for word, weight in top_words:\n",
        "        print(f\"{word}: {weight}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "hvqTmn_lirmI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of topic_docs\n",
        "print(topic_docs)"
      ],
      "metadata": {
        "id": "-gsboZCshxoD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "d_N2Vs6nl-EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "Y70YDgsmmQ04",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Exploring the data further:\n",
        "\n",
        "- Top 20 locations with the highest number of negative reviews across the data sets\n",
        "\n",
        "- Merging the 2 data sets using Location Name and Club's Name\n",
        "\n",
        "    Sorting based on the total number of reviews.\n",
        "\n",
        "    Reviewing the locations and the total number of reviews for each location on Trustpilot and Google, as well as the combined total of reviews from both platforms\n",
        "\n",
        "- Redoing the word frequency and word cloud for the top 30 locations\n",
        "\n",
        "- For the top 30 locations, combine the reviews for both platforms and running them through BERTopic."
      ],
      "metadata": {
        "id": "TXMEFq-fe1nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that merged_common is only capturing the negative scores, so I should just be able to count by freq of location \"Club's Name\" to find the top 20 locations with the highest number of negative reviews, sorted in descending order separately for the 2 review sites\n",
        "\n",
        "Recall that df1 is for google and df2 is for trustpilot reviews\n",
        "\n",
        "May be worth renaming merged_common to merged_common_neg"
      ],
      "metadata": {
        "id": "rvDMTUKnpN-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_common"
      ],
      "metadata": {
        "id": "wK7sqKiiod8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create counts for Google reviews\n",
        "google_location_counts = (\n",
        "    merged_common[merged_common['Source'] == 'df1'][\"Club's Name\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "google_location_counts.columns = [\"Club's Name\", \"Negative Review Count\"]\n",
        "google_top_20_locations = google_location_counts.sort_values(by=\"Negative Review Count\", ascending=False).head(20)\n",
        "\n",
        "# Create counts for Trustpilot reviews\n",
        "trustpilot_location_counts = (\n",
        "    merged_common[merged_common['Source'] == 'df2'][\"Club's Name\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "trustpilot_location_counts.columns = [\"Club's Name\", \"Negative Review Count\"]\n",
        "trustpilot_top_20_locations = trustpilot_location_counts.sort_values(by=\"Negative Review Count\", ascending=False).head(20)\n",
        "\n",
        "# Display the results\n",
        "print(\"Top 20 locations with highest number of negative reviews for Google:\")\n",
        "print(google_top_20_locations)\n",
        "\n",
        "print(\"\\nTop 20 locations with highest number of negative reviews for Trustpilot:\")\n",
        "print(trustpilot_top_20_locations)"
      ],
      "metadata": {
        "id": "kjfSITfsq8vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 5 locations for Google and Trustpilot reviews\n",
        "google_top_5_locations = google_top_20_locations.head(5)\n",
        "trustpilot_top_5_locations = trustpilot_top_20_locations.head(5)\n",
        "\n",
        "# Find the intersection of the top 5 locations\n",
        "common_top_5_locations = pd.merge(google_top_5_locations, trustpilot_top_5_locations, on=\"Club's Name\", suffixes=('_google', '_trustpilot'))\n",
        "\n",
        "# Display the common top 5 locations\n",
        "print(\"Common clubs in the top 5 of both Google and Trustpilot negative reviews: \\n\")\n",
        "\n",
        "common_top_5_locations"
      ],
      "metadata": {
        "id": "oP2MFjtKrkpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_common.head(3)"
      ],
      "metadata": {
        "id": "KyX0w1X5sK7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Locations\n",
        "\n",
        "• Number of Trustpilot reviews for this location\n",
        "\n",
        "• Number of Google reviews for this location\n",
        "\n",
        "• Total number of reviews for this location (sum of Google reviews and Trustpilot reviews)\n",
        "\n",
        "Sorting based on the total number of reviews."
      ],
      "metadata": {
        "id": "9KimnW_HsH3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the negative reviews by location for Google and Trustpilot separately\n",
        "google_negative_review_counts = merged_common[merged_common['Source'] == 'df1']['Club\\'s Name'].value_counts().reset_index()\n",
        "trustpilot_negative_review_counts = merged_common[merged_common['Source'] == 'df2']['Club\\'s Name'].value_counts().reset_index()\n",
        "\n",
        "# Rename the columns for clarity\n",
        "google_negative_review_counts.columns = ['Club\\'s Name', 'negative_review_count_google']\n",
        "trustpilot_negative_review_counts.columns = ['Club\\'s Name', 'negative_review_count_trustpilot']\n",
        "\n",
        "# Merge the counts DataFrames\n",
        "merged_negative_review_counts = pd.merge(\n",
        "    google_negative_review_counts,\n",
        "    trustpilot_negative_review_counts,\n",
        "    on='Club\\'s Name',\n",
        "    how='outer'\n",
        ")\n",
        "\n",
        "# Fill NaN values with 0 for counts\n",
        "merged_negative_review_counts['negative_review_count_google'] = merged_negative_review_counts['negative_review_count_google'].fillna(0).astype(int)\n",
        "merged_negative_review_counts['negative_review_count_trustpilot'] = merged_negative_review_counts['negative_review_count_trustpilot'].fillna(0).astype(int)\n",
        "\n",
        "# Calculate the total number of reviews for each location\n",
        "merged_negative_review_counts['total_negative_reviews'] = merged_negative_review_counts['negative_review_count_google'] + merged_negative_review_counts['negative_review_count_trustpilot']\n",
        "\n",
        "# Sort the DataFrame in descending order based on the total number of reviews\n",
        "sorted_negative_review_counts = merged_negative_review_counts.sort_values(by='total_negative_reviews', ascending=False)\n",
        "\n",
        "# Display the final DataFrame\n",
        "sorted_negative_review_counts"
      ],
      "metadata": {
        "id": "-EjVZgmQtXtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Worth renaming to top 30 neg reviews"
      ],
      "metadata": {
        "id": "bTB8dVfb2T-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_30_locations = sorted_negative_review_counts.head(30)['Club\\'s Name']\n",
        "top_30_reviews = merged_common[merged_common['Club\\'s Name'].isin(top_30_locations)]\n",
        "top_30_reviews"
      ],
      "metadata": {
        "id": "DSDSDjKIvjgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Worth renaming combined_reviews_df to combined_reviews_top_30_neg"
      ],
      "metadata": {
        "id": "RywV2iXe2XQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrames from each column\n",
        "review_content_df = top_30_reviews[['Review Content']].copy() # This one is Trustpilot\n",
        "comment_df = top_30_reviews[['Comment']].copy() # This one is Google\n",
        "\n",
        "# Rename columns to facilitate concatenation\n",
        "review_content_df.columns = ['Review']\n",
        "comment_df.columns = ['Review']\n",
        "\n",
        "# Concatenate the DataFrames\n",
        "combined_reviews_df = pd.concat([review_content_df, comment_df], ignore_index=True)\n",
        "\n",
        "# Convert the combined reviews into individual documents\n",
        "documents_top_30_neg_locations = combined_reviews_df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).tolist()\n",
        "\n",
        "# Assuming documents_top_30_neg_locations is a list of strings\n",
        "# Remove empty strings from the list\n",
        "cleaned_documents_top_30_neg_locations = [doc for doc in documents_top_30_neg_locations if doc.strip() != '']\n",
        "\n",
        "# Check the cleaned list\n",
        "print(f\"First few cleaned documents:\\n{cleaned_documents_top_30_neg_locations[:10]}\")\n",
        "print(f\"Total Cleaned Documents: {len(cleaned_documents_top_30_neg_locations)}\")\n",
        "\n",
        "# Optionally, you can convert the cleaned list back into a single text if needed\n",
        "all_reviews_text_cleaned = ' '.join(cleaned_documents_top_30_neg_locations)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eEoo5p1jw3na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_documents_top_30_neg_locations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YLylkVqx594O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_reviews_df.head(20)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BonHufPhx1lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure no empty arrays or NaNs are present\n",
        "combined_reviews_df = combined_reviews_df[combined_reviews_df['Review'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
        "\n",
        "# Plot top words\n",
        "plot_top_words(combined_reviews_df, column_name='Review', top_n=10, title='Top 30 highest negative location reviews')"
      ],
      "metadata": {
        "id": "V6TngKonzcHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(combined_reviews_df, 'Review', title='Word Cloud for top 30 negative location reviews')"
      ],
      "metadata": {
        "id": "gsOI7v4h0r4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "collapsed": true,
        "id": "26UsZsS348K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_documents_top_30_neg_locations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Lxjnd8un45qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out empty or short documents\n",
        "cleaned_documents_top_30_neg_locations = [\n",
        "    doc for doc in cleaned_documents_top_30_neg_locations\n",
        "    if doc.strip() != '' and len(doc.split()) > 1\n",
        "]\n",
        "\n",
        "# Check the cleaned list\n",
        "print(f\"First few cleaned documents:\\n{cleaned_documents_top_30_neg_locations[:10]}\")\n",
        "print(f\"Total Cleaned Documents: {len(cleaned_documents_top_30_neg_locations)}\")"
      ],
      "metadata": {
        "id": "bumcLYRd7SA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the BERTopic model\n",
        "model = BERTopic(embedding_model=SentenceTransformer(\"paraphrase-MiniLM-L6-v2\"))\n",
        "\n",
        "# Fit the model to your documents\n",
        "topics, probabilities = model.fit_transform(cleaned_documents_top_30_neg_locations)\n",
        "\n",
        "# Display topics\n",
        "#model.visualize_topics()"
      ],
      "metadata": {
        "id": "_m0C_Fhm2NSi"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of topics\n",
        "print(f\"Number of topics found: {len(set(topics))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVD-nKwc7uAo",
        "outputId": "eaacbbc8-1f34-47b4-891d-dc1133c1fc04"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of topics found: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print topics and their distribution\n",
        "from collections import Counter\n",
        "\n",
        "topic_counts = Counter(topics)\n",
        "print(f\"Topic distribution: {topic_counts}\")\n",
        "\n",
        "# Display topics\n",
        "for i in set(topics):\n",
        "    print(f\"Topic {i}: {model.get_topic(i)}\")"
      ],
      "metadata": {
        "id": "rI5GO4YD817c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and print the top words for each topic\n",
        "for i in set(topics):\n",
        "    print(f\"Topic {i}:\")\n",
        "    print(model.get_topic(i))\n",
        "    print()"
      ],
      "metadata": {
        "id": "_zpbBMR58-L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic_info()"
      ],
      "metadata": {
        "id": "7kWRNLo72eFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic information\n",
        "topic_info = model.get_topic_info()\n",
        "\n",
        "# Identify the top 2 topics based on frequency\n",
        "top_topics = topic_info.head(3)['Topic'].tolist()\n",
        "\n",
        "# Get top words for these topics\n",
        "for topic in top_topics:\n",
        "    top_words = model.get_topic(topic)\n",
        "    print(f\"Top words for Topic {topic}:\")\n",
        "    for word, weight in top_words:\n",
        "        print(f\"{word}: {weight}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "55uRY5xk29BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract topic-document mappings\n",
        "topic_docs = model.get_representative_docs()\n",
        "\n",
        "# Create a list of dictionaries to convert to a DataFrame\n",
        "data = []\n",
        "for topic, docs in topic_docs.items():\n",
        "    for doc in docs:\n",
        "        data.append({'Topic': topic, 'Document': doc, 'Probability': 1.0})  # Assuming probability is not available, use a placeholder\n",
        "\n",
        "# Convert to DataFrame\n",
        "topics_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame to understand its structure\n",
        "print(topics_df.head())\n",
        "\n",
        "# For each topic, get the top 10 documents\n",
        "top_docs_per_topic = {}\n",
        "for topic in topics_df['Topic'].unique():\n",
        "    topic_docs_df = topics_df[topics_df['Topic'] == topic]\n",
        "    # Sort documents based on their probabilities\n",
        "    sorted_docs = topic_docs_df.sort_values(by='Probability', ascending=False)\n",
        "    top_docs_per_topic[topic] = sorted_docs.head(10)  # Get top 10 documents\n",
        "\n",
        "# Print the top 10 documents for each topic\n",
        "for topic, top_docs in top_docs_per_topic.items():\n",
        "    print(f\"Topic {topic}:\")\n",
        "    for _, row in top_docs.iterrows():\n",
        "        print(f\"Document: {row['Document']}\\nProbability: {row['Probability']}\\n\")"
      ],
      "metadata": {
        "id": "561uDKU03CQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "0tahKgoQ3VdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "DSL4YYH53blZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Conducting emotion analysis:\n",
        "\n",
        "- Using BERT model bhadresh-savani/bert-base-uncased-emotion, setting up a pipeline for text classification\n",
        "- Testing the model on a sample and displaying the different emotion classifications that the model outputs\n",
        "- Capturing the top emotion for each review\n",
        "- Visualising the top emotion distribution for all negative reviews in both data sets\n",
        "- Extract all the negative reviews (from both data sets) where anger is top emotion\n",
        "- Running BERTopic on the output of the previous step\n",
        "- Visualing the clusters from this run to see if it is possible to narrow down the primary issues that have led to an angry review"
      ],
      "metadata": {
        "id": "lKfpwmdr-9pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "df_neg df has only been filtered for where score <3"
      ],
      "metadata": {
        "id": "lEaLSmDXGAri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datetime\n",
        "df_google_negative['Creation Date'] = pd.to_datetime(df_google_negative['Creation Date'], format='%m/%d/%y %H:%M')\n",
        "df_trustpilot_negative['Review Created (UTC)'] = pd.to_datetime(df_trustpilot_negative['Review Created (UTC)'])"
      ],
      "metadata": {
        "id": "70BcSskkX91R"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "uikdt5W1YFXu"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find overlaps within a given time window\n",
        "def find_temporal_overlap(df1, df2, time_col1, time_col2, time_window=timedelta(minutes=10)):\n",
        "    overlaps = []\n",
        "    for idx, row in df1.iterrows():\n",
        "        time1 = row[time_col1]\n",
        "        mask = (df2[time_col2] >= time1 - time_window) & (df2[time_col2] <= time1 + time_window)\n",
        "        overlapping_rows = df2[mask]\n",
        "        for _, overlap_row in overlapping_rows.iterrows():\n",
        "            overlaps.append((row, overlap_row))\n",
        "    return overlaps\n",
        "\n",
        "# Find overlapping rows\n",
        "overlaps = find_temporal_overlap(df_google_negative, df_trustpilot_negative, 'Creation Date', 'Review Created (UTC)')\n",
        "\n",
        "# Print overlapping pairs\n",
        "for google_row, trustpilot_row in overlaps:\n",
        "    print(\"Google Row:\\n\", google_row)\n",
        "    print(\"Trustpilot Row:\\n\", trustpilot_row)\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "Krcti0SvYFkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract indices of overlapping rows\n",
        "google_indices = [google_row.name for google_row, _ in overlaps]\n",
        "trustpilot_indices = [trustpilot_row.name for _, trustpilot_row in overlaps]\n",
        "\n",
        "# Create new DataFrames excluding overlapping rows\n",
        "df_google_reduced = df_google_negative.drop(google_indices)\n",
        "df_trustpilot_reduced = df_trustpilot_negative.drop(trustpilot_indices)\n",
        "\n",
        "print(\"Reduced Google DataFrame:\\n\", df_google_reduced)\n",
        "print(\"Reduced Trustpilot DataFrame:\\n\", df_trustpilot_reduced)"
      ],
      "metadata": {
        "id": "GQDWURzfYRxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_reduced"
      ],
      "metadata": {
        "id": "W1dCSB3FYLkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot_reduced"
      ],
      "metadata": {
        "id": "-_Re9HAVYS7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "XZC7BlHZ_Iqm"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"text-classification\", model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)"
      ],
      "metadata": {
        "id": "t30z8fhbGGHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"I found NLP boring when studying the material but now putting things into practive via this topic project I found its actually quite interesting and I'm mclovin it!\""
      ],
      "metadata": {
        "id": "TRzFmAULGeBU"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels = classifier(example_sentence, )"
      ],
      "metadata": {
        "id": "LQzlpmuPGv3R"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels_sorted = sorted(emotion_labels[0], key=lambda x: x[\"score\"], reverse=True)"
      ],
      "metadata": {
        "id": "N-IaBCAkG5qy"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emotion_labels_sorted)"
      ],
      "metadata": {
        "id": "iU8_w6_JG7vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_google.head(2))\n",
        "display(df_trustpilot.head(2))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KBbqweN5If0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_emotion(text):\n",
        "    results = classifier(text)\n",
        "    # results is a list of dictionaries containing labels and scores\n",
        "    if results:\n",
        "        # Extracting the label with the highest score\n",
        "        return max(results[0], key=lambda x: x['score'])['label']\n",
        "    return None"
      ],
      "metadata": {
        "id": "9lZflzQQJYxT"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the classifier to each review\n",
        "#df_google['Top_Emotion'] = df_google[\"Comment\"].apply(get_top_emotion)\n",
        "#df_trustpilot['Top_Emotion'] = df_trustpilot[\"Review Content\"].apply(get_top_emotion)\n",
        "\n",
        "# Apply the classifier to each review\n",
        "#df_google_negative['Top_Emotion'] = df_google_negative[\"Comment\"].apply(get_top_emotion)\n",
        "#df_trustpilot_negative['Top_Emotion'] = df_trustpilot_negative[\"Review Content\"].apply(get_top_emotion)"
      ],
      "metadata": {
        "id": "5HyRZxDxMPMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_negative"
      ],
      "metadata": {
        "id": "XBdEgAAPXMQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot_negative"
      ],
      "metadata": {
        "id": "e_9pk3FGXars"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the classifier to each review\n",
        "df_google_reduced['Top_Emotion'] = df_google_reduced[\"Comment\"].apply(get_top_emotion)\n",
        "df_trustpilot_reduced['Top_Emotion'] = df_trustpilot_reduced[\"Review Content\"].apply(get_top_emotion)"
      ],
      "metadata": {
        "id": "cZS0nBNPY6Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use batch processing next time"
      ],
      "metadata": {
        "id": "SGuvbXI8fguT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the df_google DataFrame to a CSV file\n",
        "df_google_reduced.to_csv('/content/df_google_with_emotions.csv', index=False)\n",
        "\n",
        "# Save the df_trustpilot DataFrame to a CSV file\n",
        "df_trustpilot_reduced.to_csv('/content/df_trustpilot_with_emotions.csv', index=False)"
      ],
      "metadata": {
        "id": "itfcj897OKBT"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the df_google CSV file\n",
        "files.download('/content/df_google_with_emotions.csv')\n",
        "\n",
        "# Download the df_trustpilot CSV file\n",
        "files.download('/content/df_trustpilot_with_emotions.csv')"
      ],
      "metadata": {
        "id": "68DWfMjZ5XUC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "38af96ee-699e-4f44-936e-5e1a79d3454d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b590bcea-badd-4a07-bc5f-573fd4c75337\", \"df_google_with_emotions.csv\", 3879)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ff9c1fae-1120-40b3-b2fa-e0c0e2e5fc99\", \"df_trustpilot_with_emotions.csv\", 1932926)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## continuing 5."
      ],
      "metadata": {
        "id": "LOAsFyEh6xhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the df_google DataFrame from the CSV file\n",
        "df_google_with_emotions = pd.read_csv('/content/df_google_with_emotions.csv')\n",
        "\n",
        "# Load the df_trustpilot DataFrame from the CSV file\n",
        "df_trustpilot_with_emotions = pd.read_csv('/content/df_trustpilot_with_emotions.csv')"
      ],
      "metadata": {
        "id": "FD4iWlpVOMYb"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast"
      ],
      "metadata": {
        "id": "Ig8kCICDLGt-"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string representations of lists back to actual lists\n",
        "df_google_with_emotions['Comment'] = df_google_with_emotions['Comment'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
        "df_trustpilot_with_emotions['Review Content'] = df_trustpilot_with_emotions['Review Content'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])"
      ],
      "metadata": {
        "id": "Gi6f__sQLERj"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_with_emotions"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PIXuBrNA5ouz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot_with_emotions"
      ],
      "metadata": {
        "id": "n61JfI6i5qb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Add a 'Source' column to each DataFrame\n",
        "df_google_with_emotions['Source'] = 'Google Reviews'\n",
        "df_trustpilot_with_emotions['Source'] = 'Trustpilot'\n",
        "\n",
        "# Step 2: Combine the DataFrames\n",
        "df_combined_neg = pd.concat([\n",
        "    df_google_with_emotions[['Top_Emotion', 'Source']],\n",
        "    df_trustpilot_with_emotions[['Top_Emotion', 'Source']]\n",
        "])"
      ],
      "metadata": {
        "id": "KvlR9P9xA3Lp"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Plot with Seaborn\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df_combined_neg, x='Top_Emotion', hue='Source', palette='viridis')\n",
        "plt.title('Top Emotion Distribution for Negative Reviews')\n",
        "plt.xlabel('Top Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Source')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WX936hC7ZFTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot for Google Reviews\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.countplot(data=df_google_with_emotions, x='Top_Emotion', palette='viridis')\n",
        "plt.title('Top Emotion Distribution for Negative Google Reviews')\n",
        "plt.xlabel('Top Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot for Trustpilot Reviews\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.countplot(data=df_trustpilot_with_emotions, x='Top_Emotion', palette='viridis')\n",
        "plt.title('Top Emotion Distribution for Negative Trustpilot Reviews')\n",
        "plt.xlabel('Top Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bcO2BDhc-Iou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FacetGrid for side-by-side plots\n",
        "g = sns.FacetGrid(df_combined_neg, col='Source', col_wrap=2, height=6, aspect=1.5, sharey=False)\n",
        "\n",
        "# Map the countplot onto the grid\n",
        "g.map(sns.countplot, 'Top_Emotion', palette='viridis')\n",
        "\n",
        "# Adjust the titles and labels\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "g.set_axis_labels('Top Emotion', 'Count')\n",
        "g.set_xticklabels(rotation=45)\n",
        "\n",
        "# Add a main title\n",
        "plt.subplots_adjust(top=0.9)\n",
        "g.fig.suptitle('Top Emotion Distribution for Negative Reviews by Source', fontsize=16)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Plot for Google Reviews\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.countplot(data=df_google_with_emotions, x='Top_Emotion', palette='viridis')\n",
        "plt.title('Top Emotion Distribution for Negative Google Reviews')\n",
        "plt.xlabel('Top Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot for Trustpilot Reviews\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.countplot(data=df_trustpilot_with_emotions, x='Top_Emotion', palette='viridis')\n",
        "plt.title('Top Emotion Distribution for Negative Trustpilot Reviews')\n",
        "plt.xlabel('Top Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "i-CTi7NF-RSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Create a subplot with 2 columns\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=('Google Reviews', 'Trustpilot Reviews'))\n",
        "\n",
        "# Create bar plots for Google Reviews\n",
        "google_emotion_counts = df_google_with_emotions['Top_Emotion'].value_counts().reset_index()\n",
        "google_emotion_counts.columns = ['Top_Emotion', 'Count']\n",
        "fig.add_trace(\n",
        "    go.Bar(x=google_emotion_counts['Top_Emotion'], y=google_emotion_counts['Count'], name='Google Reviews', marker_color='blue'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Create bar plots for Trustpilot Reviews\n",
        "trustpilot_emotion_counts = df_trustpilot_with_emotions['Top_Emotion'].value_counts().reset_index()\n",
        "trustpilot_emotion_counts.columns = ['Top_Emotion', 'Count']\n",
        "fig.add_trace(\n",
        "    go.Bar(x=trustpilot_emotion_counts['Top_Emotion'], y=trustpilot_emotion_counts['Count'], name='Trustpilot Reviews', marker_color='orange'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Update layout for the subplots\n",
        "fig.update_layout(\n",
        "    title_text='Top Emotion Distribution for Negative Reviews by Source',\n",
        "    xaxis_title='Top Emotion',\n",
        "    yaxis_title='Count',\n",
        "    xaxis2_title='Top Emotion',\n",
        "    yaxis2_title='Count',\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4HhxXEcF_lz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_anger = df_google_with_emotions[df_google_with_emotions['Top_Emotion'] == 'anger']\n",
        "\n",
        "df_trustpilot_anger = df_trustpilot_with_emotions[df_trustpilot_with_emotions['Top_Emotion'] == 'anger']\n",
        "\n",
        "df_trustpilot_anger.head(5)"
      ],
      "metadata": {
        "id": "HW5OonkvAg-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot_anger['Review Language'].unique()"
      ],
      "metadata": {
        "id": "qpUVlU5zR9L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of languages to drop\n",
        "languages_to_drop = ['pl', 'da', 'de', 'ro']\n",
        "\n",
        "# Remove rows where 'Review Language' is in the list and update df_trustpilot_anger\n",
        "df_trustpilot_anger = df_trustpilot_anger[~df_trustpilot_anger['Review Language'].isin(languages_to_drop)]"
      ],
      "metadata": {
        "id": "3TC46fmzScIV"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_cols = [\"Club's Name\", \"Creation Date\", \"Overall Score\", \"Comment\"]\n",
        "df_trustpilot_cols = [\"Review Created (UTC)\", \"Review Title\", \"Review Content\", \"Review Stars\", \"Company Reply Date (UTC)\", \"Location Name\"]\n",
        "rename_dict = {\"Location Name\": \"Club's Name\"}\n",
        "\n",
        "merged_common_anger = concatenate_dataframes(df_google_anger, df_trustpilot_anger, df_google_cols, df_trustpilot_cols, rename_dict)\n",
        "\n",
        "merged_common_anger.head(8)"
      ],
      "metadata": {
        "id": "5CpX0FiyFE_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where 'Club\\'s Name' is '209 - Slagelse, Jernbanegade'\n",
        "merged_common_anger = merged_common_anger[merged_common_anger[\"Club's Name\"] != \"209 - Slagelse, Jernbanegade\"]"
      ],
      "metadata": {
        "id": "nA_jhNN4QHzb"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_common_anger[\"Club's Name\"].unique()"
      ],
      "metadata": {
        "id": "oYKY4TZYPdks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_common_anger.head(8)"
      ],
      "metadata": {
        "id": "QEgP3d9zQXkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_anger = []\n",
        "\n",
        "# Fill NaN values with empty lists\n",
        "merged_common_anger['Comment'] = merged_common_anger['Comment'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "merged_common_anger['Review Content'] = merged_common_anger['Review Content'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Combine the tokens from both columns row-wise into single documents\n",
        "documents_anger = [\n",
        "    \" \".join((comment if isinstance(comment, list) else []) +\n",
        "             (review_content if isinstance(review_content, list) else []))\n",
        "    for comment, review_content in zip(merged_common_anger['Comment'], merged_common_anger['Review Content'])\n",
        "]\n",
        "\n",
        "# Check the first few documents\n",
        "print(f\"First few documents:\\n{documents_anger[:10]}\")\n",
        "\n",
        "# Check the total number of documents\n",
        "print(f\"Total Documents: {len(documents_anger)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Fg9YH4oEFLsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data type of each element in 'Comment' column\n",
        "print(merged_common_anger['Comment'].apply(type).value_counts())\n",
        "\n",
        "# Check the data type of each element in 'Review Content' column\n",
        "print(merged_common_anger['Review Content'].apply(type).value_counts())"
      ],
      "metadata": {
        "id": "LWyNp5BFUCNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_anger"
      ],
      "metadata": {
        "id": "pzASn1mBHYTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_invalid_entries(documents_anger)"
      ],
      "metadata": {
        "id": "vDGK1yqOIrxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code to check if the model is fitted properly\n",
        "print(hasattr(model, 'components_'))  # Check if the model has components\n",
        "print(f\"Number of topics: {model.n_topics if hasattr(model, 'n_topics') else 'N/A'}\")"
      ],
      "metadata": {
        "id": "Jsy41iuJNPnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize the BERTopic model\n",
        "model = BERTopic(embedding_model=SentenceTransformer(\"paraphrase-MiniLM-L6-v2\"))\n",
        "\n",
        "# Fit the model to your documents\n",
        "topics, probabilities = model.fit_transform(documents_anger)"
      ],
      "metadata": {
        "id": "kWJtW_XHODyF"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the model has topics and other attributes\n",
        "print(f\"Topics: {model.get_topic_info()}\")\n",
        "print(f\"Number of topics: {len(model.get_topic_info())}\")"
      ],
      "metadata": {
        "id": "UYHiaWY3OK1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if any document is empty or has unexpected content\n",
        "print(f\"Number of documents: {len(documents_anger)}\")\n",
        "print(f\"Number of empty documents: {sum(not doc.strip() for doc in documents_anger)}\")\n",
        "\n",
        "print(f\"Topics found: {model.get_topic_info()}\")\n",
        "print(f\"Number of topics: {len(model.get_topic_info())}\")\n",
        "\n",
        "# Check for valid topic assignments\n",
        "print(f\"Topic assignments: {set(topics)}\")"
      ],
      "metadata": {
        "id": "Uv0HPAjtPOrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display topics\n",
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "5g9MDpX3FPYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic_info()\n",
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "s4ouvnESU8Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Using a large language model from Hugging Face\n",
        "\n",
        "- Loading: tiiuae/falcon-7b-instruct. Using a pipeline for text generation, max length of 1,000 for each review for efficiency.\n",
        "\n",
        "- Before passing to the model will use this prompt for every review:\n",
        "\n",
        "**In the following customer review, pick out the main 3 topics. Return them in a numbered list format, with each one on a new line.**\n",
        "\n",
        "Output should be top 3 topics from each review"
      ],
      "metadata": {
        "id": "_Q1MhjyTkcB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install -U safetensors"
      ],
      "metadata": {
        "id": "-tPksAtYAM87",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "foAifzOVAg8j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer"
      ],
      "metadata": {
        "id": "1UuTmmZPqqdt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and set up the text generation pipeline\n",
        "#model_name = \"distilgpt2\"\n",
        "model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "text_generator = pipeline(\"text-generation\", model=model_name, max_length=1000, truncation=True) #max_length=1000, max_new_tokens=1000\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4PZ_Ux3qWLu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_name,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "Q3LDMe2gBNly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_trustpilot_unprocessed"
      ],
      "metadata": {
        "collapsed": true,
        "id": "um3nFo96slqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_unprocessed"
      ],
      "metadata": {
        "id": "uejRnjr-smPY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Overall Score' directly to integer\n",
        "df_trustpilot_unprocessed['Review Stars'] = df_trustpilot_unprocessed['Review Stars'].astype(int)\n",
        "\n",
        "# Convert 'Overall Score' directly to integer\n",
        "df_google_unprocessed['Overall Score'] = df_google_unprocessed['Overall Score'].astype(int)"
      ],
      "metadata": {
        "id": "OZRWLKTXz3b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_negative_unprocessed = df_google_unprocessed[df_google_unprocessed['Overall Score'] < 3]\n",
        "df_trustpilot_negative_unprocessed = df_trustpilot_unprocessed[df_trustpilot_unprocessed['Review Stars'] < 3]"
      ],
      "metadata": {
        "id": "kglJgGdjtDRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_google_negative_unprocessed.info()\n",
        "print(\"\\n\")\n",
        "df_trustpilot_negative_unprocessed.info()"
      ],
      "metadata": {
        "id": "Q_Att60RtJSJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset_google_negative = df_google_negative_unprocessed.sample(frac=0.1, random_state=1)\n",
        "\n",
        "df_subset_trustpilot_negative = df_trustpilot_negative_unprocessed.sample(frac=0.1, random_state=1)\n",
        "df_subset_trustpilot_negative.head(5)"
      ],
      "metadata": {
        "id": "NH1YhsLntOjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Un comment if want this to be super quick\n",
        "#df_subset_trustpilot_negative = df_subset_trustpilot_negative[:3]"
      ],
      "metadata": {
        "id": "PYjHRdFO5pFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset_google_negative.head(5)"
      ],
      "metadata": {
        "id": "PiJX7WnV0lDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"In the following customer review, pick out the main 3 topics. Return them in a numbered list format, with each one on a new line\""
      ],
      "metadata": {
        "id": "FEUaDuqfiAxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Had an issue with batch processing so opting for this"
      ],
      "metadata": {
        "id": "2TGe6vgLyzfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate topics for each review\n",
        "def generate_topics_for_df(df, review_column, prefix=\"\", max_length=1000):\n",
        "    # Initialize a list to store the results\n",
        "    results = []\n",
        "\n",
        "    # Loop through each review in the DataFrame\n",
        "    for idx, review_text in enumerate(df[review_column]):\n",
        "        # Add the prefix to the review text\n",
        "        input_text = prefix + review_text\n",
        "\n",
        "        # Generate the text using the model\n",
        "        sequences = pipeline(\n",
        "            input_text,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=10,\n",
        "            temperature=0.5,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        # Extract the generated text\n",
        "        generated_text = sequences[0]['generated_text']\n",
        "        results.append(generated_text)\n",
        "\n",
        "        print(f\"Processed review {idx + 1}/{len(df)}: {generated_text}\")\n",
        "\n",
        "    # Add the results as a new column to the DataFrame\n",
        "    df['Generated Topics'] = results\n",
        "    return df"
      ],
      "metadata": {
        "id": "0_VJwGZkrJO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function below lets me choose various parameters as currently unsure of the format I want things to be in....but above one works too but less complex, modify above once decided"
      ],
      "metadata": {
        "id": "pK4or7fg5yF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_topics_for_df(df, review_column, prefix=\"\", max_length=200, clean_topics=False, store_individually=False, store_as_list=False, store_entire_text=False):\n",
        "    # Initialize lists to store results\n",
        "    results = []\n",
        "    individual_topics = []\n",
        "    topics_as_list = []\n",
        "\n",
        "    # Loop through each review in the DataFrame\n",
        "    for idx, review_text in enumerate(df[review_column]):\n",
        "        # Add the prefix to the review text\n",
        "        input_text = prefix + review_text\n",
        "\n",
        "        # Generate the text using the model\n",
        "        sequences = pipeline(\n",
        "            input_text,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=10,\n",
        "            temperature=0.5,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        # Extract the generated text\n",
        "        generated_text = sequences[0]['generated_text']\n",
        "\n",
        "        if clean_topics:\n",
        "            # Clean the generated topics\n",
        "            topic_splits = generated_text.splitlines()\n",
        "            if len(topic_splits) > 1:\n",
        "                topic_splits.pop(0)  # Remove the first line if it's not useful\n",
        "\n",
        "            cleaned_topics = []\n",
        "            for topic_split in topic_splits:\n",
        "                # Remove list numbering, hyphens, and extra spaces\n",
        "                parts = topic_split.split('.', 1)\n",
        "                if len(parts) > 1:\n",
        "                    cleaned_topic = parts[1].strip()\n",
        "                else:\n",
        "                    cleaned_topic = topic_split.strip()\n",
        "\n",
        "                # Additional cleaning\n",
        "                cleaned_topic = cleaned_topic.replace('-', '').strip()\n",
        "\n",
        "                if cleaned_topic:\n",
        "                    cleaned_topics.append(cleaned_topic)\n",
        "\n",
        "            if store_individually:\n",
        "                # Store topics in separate columns\n",
        "                individual_topics.append(cleaned_topics)\n",
        "            if store_as_list:\n",
        "                # Store topics as a list\n",
        "                topics_as_list.append(cleaned_topics)\n",
        "            if store_entire_text:\n",
        "                # Store entire cleaned/generated text if requested\n",
        "                results.append(\"\\n\".join(cleaned_topics))\n",
        "        else:\n",
        "            # Handle uncleaned text\n",
        "            topic_splits = generated_text.splitlines()\n",
        "            cleaned_topics = [line.strip() for line in topic_splits if line.strip()]\n",
        "\n",
        "            if store_individually:\n",
        "                # Store topics in separate columns\n",
        "                individual_topics.append(cleaned_topics)\n",
        "            if store_as_list:\n",
        "                # Store topics as a list\n",
        "                topics_as_list.append(cleaned_topics)\n",
        "            if store_entire_text:\n",
        "                # Store entire generated text if requested\n",
        "                results.append(generated_text)\n",
        "\n",
        "        print(f\"Processed review {idx + 1}/{len(df)}: {generated_text}\")\n",
        "\n",
        "    # Add the results to the DataFrame\n",
        "    if store_individually:\n",
        "        # Create columns for each topic\n",
        "        max_topics = max(len(topics) for topics in individual_topics)\n",
        "        for i in range(max_topics):\n",
        "            df[f'Topic {i + 1}'] = [topics[i] if i < len(topics) else '' for topics in individual_topics]\n",
        "\n",
        "    if store_as_list:\n",
        "        df['Topics List'] = topics_as_list\n",
        "\n",
        "    if store_entire_text:\n",
        "        df['Generated Topics'] = results\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "AQcAxXMQ56gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to your DataFrame\n",
        "df_subset_google_negative = generate_topics_for_df(df_subset_google_negative, review_column='Comment', prefix=prefix)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df_subset_google_negative[['Comment', 'Generated Topics']])"
      ],
      "metadata": {
        "id": "429duBGlt1lw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to your DataFrame\n",
        "df_subset_trustpilot_negative = generate_topics_for_df(df_subset_trustpilot_negative, review_column='Review Content', prefix=prefix)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df_subset_trustpilot_negative[['Review Content', 'Generated Topics']])"
      ],
      "metadata": {
        "id": "49JuKEHxt2f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "df_subset_trustpilot_negative.to_csv('df_subset_trustpilot_negative.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Convert column to a NumPy array and save\n",
        "np.save('np_topics_llm.npy', df_subset_trustpilot_negative['Generated Topics'].to_numpy())\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Uy3-v5CZ7QGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DataFrame from the CSV file\n",
        "df_loaded = pd.read_csv('df_subset_trustpilot_negative.csv')\n",
        "\n",
        "\"\"\"\n",
        "# Load the NumPy array and convert back to list\n",
        "loaded_arr = np.load('np_topics_llm.npy', allow_pickle=True)  # allow_pickle=True is needed for objects like lists\n",
        "topics_llm = loaded_arr.tolist()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4eFOSsu-8VQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the CSV file\n",
        "files.download('df_subset_trustpilot_negative.csv')"
      ],
      "metadata": {
        "id": "rau_l8JB7nMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DataFrame from the uploaded file\n",
        "df_subset_trustpilot_negative = pd.read_csv('df_subset_trustpilot_negative.csv')"
      ],
      "metadata": {
        "id": "wVAoL6EI7tdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset_trustpilot_negative['Generated Topics']"
      ],
      "metadata": {
        "id": "LQsd7u1r6vmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_subset_trustpilot_negative.loc[349, 'Generated Topics'])"
      ],
      "metadata": {
        "id": "smgend709Yhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_clean_topics(df, column_name):\n",
        "    all_topics = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        generated_text = row[column_name]\n",
        "\n",
        "        if pd.notna(generated_text):\n",
        "            # Split the text by lines\n",
        "            lines = generated_text.splitlines()\n",
        "\n",
        "            # Identify where the topics start\n",
        "            topic_start_index = 0\n",
        "            for i, line in enumerate(lines):\n",
        "                if line.strip().startswith(\"1.\"):\n",
        "                    topic_start_index = i\n",
        "                    break\n",
        "\n",
        "            # Extract topics\n",
        "            if topic_start_index > 0:\n",
        "                for line in lines[topic_start_index:]:\n",
        "                    # Remove list numbering and extra spaces\n",
        "                    if line.strip():\n",
        "                        # Remove list numbers, clean up extra spaces and other characters\n",
        "                        parts = line.split('.', 1)\n",
        "                        if len(parts) > 1:\n",
        "                            cleaned_topic = parts[1].strip()\n",
        "                        else:\n",
        "                            cleaned_topic = line.strip()\n",
        "\n",
        "                        # Additional cleaning for any unexpected formatting\n",
        "                        cleaned_topic = cleaned_topic.replace('-', '').strip()\n",
        "\n",
        "                        if cleaned_topic:\n",
        "                            all_topics.append(cleaned_topic)\n",
        "\n",
        "    return all_topics\n",
        "\n",
        "# Extract topics from DataFrame\n",
        "topics_llm = extract_and_clean_topics(df_subset_trustpilot_negative, 'Generated Topics')\n",
        "\n",
        "# Create a comprehensive list of topics\n",
        "topic_string = ', '.join(topics_llm)\n",
        "\n",
        "# Display the first 2000 characters (or adjust as needed)\n",
        "print(topic_string[:2000])  # Display a snippet if the list is very long\n",
        "\n",
        "# Optional: Save the topics to a file for later use\n",
        "with open('topics_list.txt', 'w') as file:\n",
        "    file.write(topic_string)\n"
      ],
      "metadata": {
        "id": "m-mQn_wu_7j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Redoing BERTopic with output from prev step"
      ],
      "metadata": {
        "id": "mXaDrV5NE8kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the BERTopic model\n",
        "model = BERTopic(embedding_model=SentenceTransformer(\"paraphrase-MiniLM-L6-v2\"))\n",
        "\n",
        "# Fit the model to your documents\n",
        "topics, probabilities = model.fit_transform(documents)\n",
        "\n",
        "# Display topics\n",
        "model.visualize_topics()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nDzI30K7KObz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "zLu2AVKWKOb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic_info()"
      ],
      "metadata": {
        "id": "HOZniJ8aKOb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sizeable number of outliers (topic -1) and should typically be ignored. Let's take a look at the most frequent topic that was generated, topic 0"
      ],
      "metadata": {
        "id": "zs4_s4tZKOb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(0)"
      ],
      "metadata": {
        "id": "tkvMQjRUKOb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(1)"
      ],
      "metadata": {
        "id": "PPbu9zanKOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract topic-document mappings\n",
        "topic_docs = model.get_representative_docs()\n",
        "\n",
        "# Create a list of dictionaries to convert to a DataFrame\n",
        "data = []\n",
        "for topic, docs in topic_docs.items():\n",
        "    for doc in docs:\n",
        "        data.append({'Topic': topic, 'Document': doc, 'Probability': 1.0})  # Assuming probability is not available, use a placeholder\n",
        "\n",
        "# Convert to DataFrame\n",
        "topics_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame to understand its structure\n",
        "print(topics_df.head())\n",
        "\n",
        "# For each topic, get the top 10 documents\n",
        "top_docs_per_topic = {}\n",
        "for topic in topics_df['Topic'].unique():\n",
        "    topic_docs_df = topics_df[topics_df['Topic'] == topic]\n",
        "    # Sort documents based on their probabilities\n",
        "    sorted_docs = topic_docs_df.sort_values(by='Probability', ascending=False)\n",
        "    top_docs_per_topic[topic] = sorted_docs.head(10)  # Get top 10 documents\n",
        "\n",
        "# Print the top 10 documents for each topic\n",
        "for topic, top_docs in top_docs_per_topic.items():\n",
        "    print(f\"Topic {topic}:\")\n",
        "    for _, row in top_docs.iterrows():\n",
        "        print(f\"Document: {row['Document']}\\nProbability: {row['Probability']}\\n\")"
      ],
      "metadata": {
        "id": "nLajNczHKOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic information\n",
        "topic_info = model.get_topic_info()\n",
        "\n",
        "# Identify the top 2 topics based on frequency\n",
        "top_topics = topic_info.head(3)['Topic'].tolist()\n",
        "\n",
        "# Get top words for these topics\n",
        "for topic in top_topics:\n",
        "    top_words = model.get_topic(topic)\n",
        "    print(f\"Top words for Topic {topic}:\")\n",
        "    for word, weight in top_words:\n",
        "        print(f\"{word}: {weight}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6efU3D3_KOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of topic_docs\n",
        "print(topic_docs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "55-IrRLEKOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "SLCfQCNRKOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_heatmap()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WKJ7HWpDKOb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hide next step for now"
      ],
      "metadata": {
        "id": "qfLkZR6se7Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the list from the prev step running Falcon-7b-Instruct model\n",
        "\n",
        "  This time using this pre-fix as the prompt for each review: **For the following text topics obtained from negative customer reviews, can you give some actionable insights that would help this gym company?**\n",
        "\n",
        "Listing the output in the form of suggestions, that the company can employ to address customer concerns."
      ],
      "metadata": {
        "id": "EjR11RA0fiSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the file\n",
        "file_path = 'topics_list.txt'\n",
        "\n",
        "# Read the contents of the file\n",
        "try:\n",
        "    with open(file_path, 'r') as file:\n",
        "        topic_string = file.read()\n",
        "        print(\"Data loaded from file successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {file_path} does not exist.\")\n",
        "except IOError as e:\n",
        "    print(f\"Error reading the file: {e}\")\n",
        "\n",
        "# Display the contents or process them as needed\n",
        "print(topic_string[:2000])  # Display a snippet if the data is very long"
      ],
      "metadata": {
        "id": "WhvWDNh1Q-9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_string"
      ],
      "metadata": {
        "id": "ObXWfii_hhUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_prefix = \"For the following text topics obtained from negative customer reviews, can you give some actionable insights that would help this gym company?\""
      ],
      "metadata": {
        "id": "3CxAyPCyKnfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply falcon on model again with topic list string, make sure this function is correct before running the code below"
      ],
      "metadata": {
        "id": "zat2cBdXK2lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input text\n",
        "tokens = tokenizer(topic_string, return_tensors='pt')\n",
        "\n",
        "# Count the number of tokens\n",
        "num_tokens = tokens['input_ids'].size(1)\n",
        "print(f\"Number of tokens: {num_tokens}\")\n"
      ],
      "metadata": {
        "id": "2-o01rZIpRTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to generate actionable insights\n",
        "def generate_actionable_insights(topic_string, prefix=\"\", max_length=6500): #max_new_tokens=500\n",
        "    # Combine prefix with topic string\n",
        "    input_text = f\"{prefix}\\n\\n{topic_string}\"\n",
        "\n",
        "    # Generate actionable insights using the Falcon model\n",
        "    try:\n",
        "        # Generate text with Falcon\n",
        "        sequences = pipeline(\n",
        "            input_text,\n",
        "            #max_new_tokens=max_new_tokens,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=10,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        # Extract the generated text\n",
        "        generated_text = sequences[0]['generated_text']\n",
        "\n",
        "        # Optionally: Process generated text to improve readability\n",
        "        actionable_insights = [line.strip() for line in generated_text.split('\\n') if line.strip()]\n",
        "\n",
        "        return actionable_insights\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating insights: {e}\")\n",
        "        return [\"Error generating insights\"]"
      ],
      "metadata": {
        "id": "pBq2BTBaKuqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the provided prefix and topic string\n",
        "actionable_insights = generate_actionable_insights(topic_string, BERT_prefix)\n",
        "print(\"Actionable Insights:\")\n",
        "for insight in actionable_insights:\n",
        "    print(insight)"
      ],
      "metadata": {
        "id": "xI2MP3VmNISI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "hZd-FcdMPiRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "1i9BdNG6Pf6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text generation function\n",
        "def generate_actionable_insights(topic_string, prefix=\"\", max_new_tokens=300, max_length=2048):\n",
        "    input_text = f\"{prefix}\\n\\n{topic_string}\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_length)\n",
        "\n",
        "    # Generate text\n",
        "    try:\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_k=10,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        # Decode and process the generated text\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        actionable_insights = [line.strip() for line in generated_text.split('\\n') if line.strip()]\n",
        "\n",
        "        return actionable_insights\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if 'CUDA out of memory' in str(e):\n",
        "            print(\"CUDA out of memory. Consider reducing the number of new tokens.\")\n",
        "        else:\n",
        "            print(f\"Error generating insights: {e}\")\n",
        "        return [\"Error generating insights\"]"
      ],
      "metadata": {
        "id": "fwVPauOgOcy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the provided prefix and topic string\n",
        "actionable_insights = generate_actionable_insights(topic_string, BERT_prefix)\n",
        "print(\"Actionable Insights:\")\n",
        "for insight in actionable_insights:\n",
        "    print(insight)"
      ],
      "metadata": {
        "id": "g7Uatlh0OehP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}