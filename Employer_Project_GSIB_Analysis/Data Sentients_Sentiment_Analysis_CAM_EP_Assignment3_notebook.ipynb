{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "fwI69OtX6qNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SOwSjec6dkXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "rAO40DsiO3Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l52crFLShGIc",
        "outputId": "d69f97a8-ba0f-4f84-ca43-db345a940ccd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import pdfplumber\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from statistics import mean\n",
        "from heapq import nlargest\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "xbgbVxl1PFzl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "jdxVFm-2Phfq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise the bank that is being processed\n",
        "# 'credit suisse' / 'svb' / 'nomura'\n",
        "bank = 'svb'"
      ],
      "metadata": {
        "id": "7VICv-fPPvXE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear existing handlers\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# Configure logging with both console and file output\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"pdf_processing_errors.log\"),\n",
        "        logging.StreamHandler()  # Output to console\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "RgfxmjspO8c7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_header_text(text=None):\n",
        "    \"\"\"Remove header text from the extracted text.\"\"\"\n",
        "    cleaned_text = []\n",
        "    for line in text.splitlines():\n",
        "        if len(line) < 5 or re.search(r'\\b(Results|Earnings Call Transcript|Transcript|Earnings Transcript|cautionary statement|GAAP|adjusted|risk|pre|year|on|quarter|revenue|expenses|credit|market|clients)\\b', line, re.IGNORECASE):\n",
        "            continue\n",
        "        cleaned_text.append(line)\n",
        "    return \"\\n\".join(cleaned_text)\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = []\n",
        "    for line in text.splitlines():\n",
        "        if len(line) < 5 or re.search(r'\\b(Results|Earnings Call Transcript|earnings-call-transcript|Earnings Transcript|cautionary statement|GAAP|adjusted|risk|pre|year|on|quarter|revenue|expenses|market|clients)\\b', line, re.IGNORECASE):\n",
        "            #print(line)\n",
        "            continue\n",
        "        cleaned_text.append(line)\n",
        "    return \"\\n\".join(cleaned_text)\n",
        "\n",
        "# PDF Text Extraction Functions\n",
        "def extract_first_page_text(file_path=None):\n",
        "    \"\"\"Extracts text from the first page of a PDF file to find year and quarter information.\"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            first_page_text = pdf.pages[0].extract_text()\n",
        "        return first_page_text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in extract_first_page_text, file: {file_path}, error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_year_quarter_from_text(text=None):\n",
        "\n",
        "    \"\"\"Extracts year and quarter information from the text.\"\"\"\n",
        "\n",
        "    match = re.search(r\"(Q[1-4]\\s*\\d{4}|\\d{4}\\s*Q[1-4])\", text)\n",
        "    if match:\n",
        "        found = match.group(0)\n",
        "        if found.startswith(\"Q\"):\n",
        "            quarter, year = found.split()\n",
        "        else:\n",
        "            year, quarter = found.split()\n",
        "        return quarter.strip(), year.strip()\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def extract_company_participants(text):\n",
        "    \"\"\"Extracts company participants from the 'Company Participants' section of the text.\"\"\"\n",
        "    company_participants = []\n",
        "    in_company_section = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        if \"Company Participants\" in line:\n",
        "            in_company_section = True\n",
        "            continue\n",
        "\n",
        "        if \"Operator\" in line or \"Question-and-Answer\" in line or \"Conference Call Participants\" in line:\n",
        "            break\n",
        "\n",
        "        if in_company_section:\n",
        "            match = re.match(r'(?P<name>[\\w\\s\\.\\-\\'\\u00C0-\\u017F]+?)\\s*[-–]\\s*(?P<designation>[\\w\\s,&\\.\\-\\'\\u00C0-\\u017F]+)', line)\n",
        "            if match:\n",
        "                name = match.group(\"name\").strip()\n",
        "                designation = match.group(\"designation\").strip()\n",
        "                designation = re.sub(r'\\s*&\\s*', ' and ', designation)\n",
        "                designation = re.sub(r'\\s*,\\s*', ', ', designation)\n",
        "                company_participants.append((name, designation))\n",
        "\n",
        "    return company_participants\n",
        "\n",
        "def extract_conference_participants(text):\n",
        "    \"\"\"Extracts conference participants from the 'Conference Call Participants' section of the text.\"\"\"\n",
        "    conference_participants = []\n",
        "    in_conference_section = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        if \"Conference Call Participants\" in line:\n",
        "            in_conference_section = True\n",
        "            continue\n",
        "\n",
        "        if \"Operator\" in line or \"Question-and-Answer\" in line or \"Disclaimer\" in line:\n",
        "            break\n",
        "\n",
        "        if in_conference_section:\n",
        "            match = re.match(r'(?P<name>[\\w\\s\\.\\-\\'\\u00C0-\\u017F]+?)\\s*[-–]\\s*(?P<bank>[\\w\\s,&\\.\\-\\'\\u00C0-\\u017F]+)', line)\n",
        "            if match:\n",
        "                name = match.group(\"name\").strip()\n",
        "                bank = match.group(\"bank\").strip()\n",
        "                conference_participants.append((name, bank))\n",
        "\n",
        "    return conference_participants\n",
        "\n",
        "def extract_participants_from_text(text):\n",
        "    \"\"\"Extracts both company and conference participants from the text.\"\"\"\n",
        "    company_participants = extract_company_participants(text)\n",
        "    conference_participants = extract_conference_participants(text)\n",
        "\n",
        "    return company_participants, conference_participants\n",
        "\n",
        "\n",
        "def extract_qa_section(text):\n",
        "    # Locate the start of the Q&A section and isolate that portion of the text\n",
        "    qa_section_start = text.find(\"Question-and-Answer Session\")\n",
        "    if qa_section_start == -1:\n",
        "        return \"\"\n",
        "    return text[qa_section_start:]\n",
        "\n",
        "def extract_interview_details(text, company_df, conference_df):\n",
        "    # Define regex patterns to capture Q&A pairs sequentially\n",
        "    qa_pattern = r\"\\n([A-Z][a-z]+ [A-Z][a-z]+)\\n(.*?)(?=\\n[A-Z][a-z]+ [A-Z][a-z]+|\\nOperator|\\Z)\"\n",
        "\n",
        "    # Extract all Q&A pairs in sequence\n",
        "    qa_matches = re.findall(qa_pattern, text, re.DOTALL)\n",
        "\n",
        "    # Initialize list to hold Q&A pairs\n",
        "    qa_data = []\n",
        "\n",
        "    # Process each pair as a (question, answer) sequentially\n",
        "    for i in range(0, len(qa_matches) - 1, 2):\n",
        "        interviewer, question = qa_matches[i]\n",
        "        interviewee, answer = qa_matches[i + 1]\n",
        "\n",
        "        # Lookup bank for interviewer from conference_df\n",
        "        interviewer_bank = conference_df.loc[conference_df['Name'] == interviewer, 'Bank'].values\n",
        "        interviewer_bank = interviewer_bank[0] if interviewer_bank.size > 0 else None\n",
        "\n",
        "        # Lookup designation for interviewee from company_df\n",
        "        interviewee_designation = company_df.loc[company_df['Name'] == interviewee, 'Designation'].values\n",
        "        interviewee_designation = interviewee_designation[0] if interviewee_designation.size > 0 else None\n",
        "\n",
        "        # Append structured Q&A data\n",
        "        qa_data.append({\n",
        "            \"Interviewer\": interviewer,\n",
        "            \"Interviewer Bank\": interviewer_bank,\n",
        "            \"Question\": question.strip(),\n",
        "            \"Interviewee\": interviewee,\n",
        "            \"Interviewee Designation\": interviewee_designation,\n",
        "            \"Answer\": answer.strip()\n",
        "        })\n",
        "\n",
        "    # Convert the collected data into a DataFrame\n",
        "    qa_df = pd.DataFrame(qa_data)\n",
        "\n",
        "    # Fill missing values with specified text\n",
        "    qa_df['Interviewer Bank'] = qa_df['Interviewer Bank'].fillna(\"Interviewee\")\n",
        "    qa_df['Interviewee Designation'] = qa_df['Interviewee Designation'].fillna(\"Interviewer\")\n",
        "\n",
        "    return qa_df\n",
        "\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    base_name = os.path.basename(file_path)\n",
        "    text = extract_first_page_text(file_path)\n",
        "    quarter, year = extract_year_quarter_from_text(text)\n",
        "\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Extract company and conference participants\n",
        "    company_participants, conference_participants = extract_participants_from_text(text)\n",
        "\n",
        "    return base_name, year, quarter, company_participants, conference_participants\n",
        "\n",
        "def process_folder(folder_path):\n",
        "    company_data = []\n",
        "    conference_data = []\n",
        "    document_data = []  # For storing bank and cleaned_text data\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        # Process only PDF files\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            try:\n",
        "                # Process the PDF and unpack returned values\n",
        "                base_name, year, quarter, bank_name, cleaned_text, company_participants, conference_participants = process_pdf(file_path)\n",
        "\n",
        "                # Populate Company Participants Data\n",
        "                for name, designation in company_participants:\n",
        "                    company_data.append({\n",
        "                        \"Year\": year,\n",
        "                        \"Quarter\": quarter,\n",
        "                        \"Name\": name,\n",
        "                        \"Designation\": designation,\n",
        "                        \"FileName\": base_name,\n",
        "                        \"Bank\": bank_name\n",
        "                    })\n",
        "\n",
        "                # Populate Conference Participants Data\n",
        "                for name, bank in conference_participants:\n",
        "                    conference_data.append({\n",
        "                        \"Year\": year,\n",
        "                        \"Quarter\": quarter,\n",
        "                        \"Name\": name,\n",
        "                        \"Bank\": bank,\n",
        "                        \"FileName\": base_name\n",
        "                    })\n",
        "\n",
        "                # Populate Document Data\n",
        "                document_data.append({\n",
        "                    \"Year\": year,\n",
        "                    \"Quarter\": quarter,\n",
        "                    \"Bank\": bank_name,\n",
        "                    \"FileName\": base_name,\n",
        "                    \"Text\": cleaned_text\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                # Log any error that occurs during processing of a specific PDF file\n",
        "                logging.error(f\"Error processing file {file_name} in {folder_path}: {e}\")\n",
        "                continue  # Skip the file with issues and move to the next one\n",
        "\n",
        "    # Convert lists to DataFrames\n",
        "    try:\n",
        "        company_df = pd.DataFrame(company_data, columns=[\"Year\", \"Quarter\", \"Name\", \"Designation\", \"FileName\", \"Bank\"])\n",
        "        conference_df = pd.DataFrame(conference_data, columns=[\"Year\", \"Quarter\", \"Name\", \"Bank\", \"FileName\"])\n",
        "        document_df = pd.DataFrame(document_data, columns=[\"Year\", \"Quarter\", \"Bank\", \"FileName\", \"Text\"])\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating DataFrames from processed data: {e}\")\n",
        "        return None, None, None  # Return None if DataFrame creation fails\n",
        "\n",
        "    return company_df, conference_df, document_df\n",
        "\n",
        "\n",
        "def process_pdf_q_and_a(file_path, company_df, conference_df):\n",
        "    base_name = os.path.basename(file_path)\n",
        "    text = extract_first_page_text(file_path)\n",
        "    quarter, year = extract_year_quarter_from_text(text)\n",
        "\n",
        "    is_new_line = is_noise = qa_section = False\n",
        "    text_by = text_type = text_to_add = \"\"\n",
        "    qa_data = []\n",
        "    noises = [\"call-transcript\", \"Call Transcript\"]\n",
        "    key_words = [\"Operator\"]\n",
        "    section = \"Presentation\"\n",
        "\n",
        "    # Open the PDF file\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "            # Extract text line-by-line to identify speakers\n",
        "            lines = page.extract_text().split('\\n')\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                is_noise = any(noise in line for noise in noises)\n",
        "\n",
        "                if not is_noise:\n",
        "                    is_new_line, text_by, text_type, text_to_add, section = add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, company_df.Name, line, \"Answer\", section, qa_section)\n",
        "                    is_new_line, text_by, text_type, text_to_add, section = add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, conference_df.Name, line, \"Question\", section, qa_section)\n",
        "                    is_new_line, text_by, text_type, text_to_add, section = add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, key_words, line, \"Operator\", section, qa_section)\n",
        "\n",
        "                    if is_new_line == True:\n",
        "                        is_new_line = False\n",
        "                        text_to_add = \"\"\n",
        "                    else:\n",
        "                        text_to_add += \" \" + line\n",
        "\n",
        "                if \"Question-and-Answer\" in line:\n",
        "                    qa_section = True\n",
        "\n",
        "    qa_data.append({\n",
        "        \"Text Type\": text_type,\n",
        "        \"Name\": text_by,\n",
        "        \"Dialogue\": text_to_add.replace(text_by, '').strip(),\n",
        "        \"Section\":section\n",
        "    })\n",
        "\n",
        "    qa_df = pd.DataFrame(qa_data)\n",
        "\n",
        "    qa_df[\"Year\"] = year\n",
        "    qa_df[\"Quarter\"] = quarter\n",
        "    qa_df[\"FileName\"] = base_name\n",
        "\n",
        "    return qa_df\n",
        "\n",
        "def add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, key_words, line, new_text_type, section, qa_section):\n",
        "    for key_word in key_words:\n",
        "        if line in key_word:\n",
        "            is_new_line = True\n",
        "            if text_to_add.strip():\n",
        "                if text_by:\n",
        "                    qa_data.append({\n",
        "                        \"Text Type\": text_type,\n",
        "                        \"Name\": text_by,\n",
        "                        \"Dialogue\": text_to_add.replace(text_by, '').strip(),\n",
        "                        \"Section\":section\n",
        "                    })\n",
        "                    if qa_section:\n",
        "                        section = \"Question-and-Answer\"\n",
        "                text_to_add = \"\"\n",
        "                is_new_line = False\n",
        "            text_by = line\n",
        "            text_type = new_text_type\n",
        "\n",
        "    return is_new_line, text_by, text_type, text_to_add, section\n",
        "\n",
        "def process_folder_q_and_a(folder_path, company_df, conference_df):\n",
        "    all_qa_data = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            qa_df = process_pdf_q_and_a(file_path, company_df, conference_df)\n",
        "            all_qa_data.append(qa_df)\n",
        "\n",
        "    final_df = pd.concat(all_qa_data, ignore_index=True)\n",
        "\n",
        "    final_df[\"Credential\"] = \"Operator\"\n",
        "\n",
        "    # Apply the lookup function to each row in final_df\n",
        "    final_df['Credential'] = final_df.apply(lambda row: lookup_credential(row, company_df, conference_df), axis=1)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def lookup_credential(row, company_df, conference_df):\n",
        "    match_designation = company_df.loc[(company_df['Name'] == row['Name']) & (company_df['Year'] == row['Year']) & (company_df['Quarter'] == row['Quarter'])]\n",
        "    match_bank = conference_df.loc[(conference_df['Name'] == row['Name']) & (conference_df['Year'] == row['Year']) & (conference_df['Quarter'] == row['Quarter'])]\n",
        "\n",
        "    if not match_designation.empty:\n",
        "        return match_designation['Designation'].values[0]\n",
        "    elif not match_bank.empty:\n",
        "        return match_bank['Bank'].values[0]\n",
        "    else:\n",
        "        return row['Credential']\n"
      ],
      "metadata": {
        "id": "H1qLniHUPAyM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf(file_path):\n",
        "    try:\n",
        "        # Extract file and bank names\n",
        "        base_name = os.path.basename(file_path)\n",
        "        bank_name = os.path.basename(os.path.dirname(file_path))  # Assuming bank name is in the parent directory name\n",
        "\n",
        "        # Extract first page text to determine year and quarter\n",
        "        text = extract_first_page_text(file_path)\n",
        "        quarter, year = extract_year_quarter_from_text(text)\n",
        "\n",
        "        # Initialize variable for full text extraction\n",
        "        text = \"\"\n",
        "\n",
        "        # Open and read PDF, handling errors in extraction\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                try:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                except Exception as page_error:\n",
        "                    logging.error(f\"Error extracting text from page in {file_path}: {page_error}\")\n",
        "                    continue  # Skip the problematic page\n",
        "\n",
        "        # Clean the extracted text\n",
        "        cleaned_text = clean_text(text)\n",
        "\n",
        "        # Extract company and conference participants from cleaned text\n",
        "        company_participants, conference_participants = extract_participants_from_text(cleaned_text)\n",
        "\n",
        "        # Return the gathered data\n",
        "        return base_name, year, quarter, bank_name, cleaned_text, company_participants, conference_participants\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing PDF {file_path}: {e}\")\n",
        "        return base_name, None, None, bank_name, \"\", [], []\n",
        "\n",
        "\n",
        "def discover_files(folder_path, sample=None, sample_size=3):\n",
        "    \"\"\"Discovers all PDF files in a folder and its subfolders, optionally sampling them.\"\"\"\n",
        "    logging.info(f\"Starting to discover files in folder: {folder_path}\")\n",
        "    folder_files = defaultdict(list)\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file_name in files:\n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                folder_files[os.path.basename(root)].append(os.path.join(root, file_name))\n",
        "\n",
        "    if sample:\n",
        "        file_paths = [files[:sample_size] for folder, files in folder_files.items()]\n",
        "    else:\n",
        "        file_paths = [file for files in folder_files.values() for file in files]\n",
        "\n",
        "    return file_paths\n",
        "\n",
        "def process_files(file_paths):\n",
        "\n",
        "    \"\"\"Processes PDF files to extract year, quarter, bank, full document text, and company participants.\"\"\"\n",
        "    document_data = []  # To store document-level data\n",
        "    participant_data = []  # To store participant-level data\n",
        "\n",
        "\n",
        "    logging.info(f\"Starting to process {len(file_paths)} PDF files.\")\n",
        "    logging.info(f\"Sample file: {file_paths[0]}\")\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            base_name, year, quarter, bank, text, company_participants, _ = process_pdf(file_path)\n",
        "            if year and quarter:\n",
        "                # Store document-level data\n",
        "                document_data.append({\n",
        "                    \"Year\": year,\n",
        "                    \"Quarter\": quarter,\n",
        "                    \"Text\": text,\n",
        "                    \"Bank\": bank,\n",
        "                    \"File\": base_name\n",
        "                })\n",
        "\n",
        "                # Store participant-level data\n",
        "                for name, designation in company_participants:\n",
        "                    participant_data.append({\n",
        "                        \"Year\": year,\n",
        "                        \"Quarter\": quarter,\n",
        "                        \"Name\": name,\n",
        "                        \"Designation\": designation,\n",
        "                        \"File\": base_name,\n",
        "                        \"Bank\": bank\n",
        "                    })\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Create DataFrames to store document-level and participant-level information\n",
        "    document_df = pd.DataFrame(document_data, columns=[\"Year\", \"Quarter\", \"Text\", \"Bank\", \"File\"])\n",
        "    participant_df = pd.DataFrame(participant_data, columns=[\"Year\", \"Quarter\", \"Name\", \"Designation\", \"File\", \"Bank\"])\n",
        "    logging.info(f\"Successfully processed {len(document_data)} out of {len(file_paths)} PDF files.\")\n",
        "\n",
        "    return document_df, participant_df\n",
        "\n",
        "def save_to_csv(dataframe, save_folder, filename):\n",
        "    \"\"\"Saves the DataFrame to a CSV file.\"\"\"\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "    csv_path = os.path.join(save_folder, filename)\n",
        "    logging.info(f\"Saving data to CSV at: {csv_path}\")\n",
        "    dataframe.to_csv(csv_path, index=False)\n",
        "    logging.info(\"Data successfully saved to CSV.\")\n",
        "\n",
        "def process_all_documents(raw_folder, processed_folder, metadata_folder, sample=False, sample_size=1):\n",
        "    \"\"\"\n",
        "    Main function to process all PDF files and save both document and participant data.\n",
        "\n",
        "    Args:\n",
        "        raw_folder (str): Path to the folder containing raw PDF files.\n",
        "        processed_folder (str): Path to the folder where processed CSVs will be saved.\n",
        "        sample (bool): Whether to sample files for testing.\n",
        "        sample_size (int): Number of files to sample if `sample=True`.\n",
        "\n",
        "    Returns:\n",
        "        tuple: DataFrames for document-level and participant-level data.\n",
        "    \"\"\"\n",
        "    # Step 1: Discover files, with sampling option\n",
        "    file_paths = discover_files(raw_folder, sample, sample_size)\n",
        "    logging.info(f\"Discovered {len(file_paths)} PDF files.\")\n",
        "\n",
        "    # Step 2: Process files to extract document and participant data\n",
        "    document_df, participant_df = process_files(file_paths)\n",
        "\n",
        "    # Step 3: Save to CSV\n",
        "    save_to_csv(document_df, save_folder=processed_folder, filename=\"pdf_summarytext_data.csv\")\n",
        "    save_to_csv(participant_df, save_folder=metadata_folder, filename=\"company_participants.csv\")\n",
        "\n",
        "    # Return DataFrames\n",
        "    return document_df, participant_df"
      ],
      "metadata": {
        "id": "631wDbAyPOWv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    raw_folder = \"/content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw\"  # Path to raw PDF files\n",
        "    processed_folder = \"/content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed\"  # Path to save processed CSVs\n",
        "    metadata_folder = \"/content/drive/MyDrive/Colab Notebooks/Employer Project/data/metadata\"  # Path to save metadata CSV\n",
        "\n",
        "    logging.info(\"Processing begins for sentiment analysis.\")\n",
        "    company_df, conference_df, document_df = process_folder(f'{raw_folder}/{bank}')\n",
        "    final_qa_df = process_folder_q_and_a(f'{raw_folder}/{bank}', company_df, conference_df)\n",
        "\n",
        "    save_to_csv(company_df, save_folder=f'{processed_folder}/{bank}', filename='company_df.csv')\n",
        "    save_to_csv(conference_df, save_folder=f'{processed_folder}/{bank}', filename='conference_df.csv')\n",
        "    save_to_csv(final_qa_df, save_folder=f'{processed_folder}/{bank}', filename='final_qa_df.csv')\n",
        "\n",
        "    logging.info(\"Processing complete for sentiment analysis.\")\n",
        "\n",
        "    logging.info(\"Processing begins for metadata\")\n",
        "\n",
        "    # Run the document processing function\n",
        "    document_df, participant_df = process_all_documents(raw_folder, processed_folder, metadata_folder, sample=False)\n",
        "    logging.info(\"Processing complete for metadata\")\n",
        "\n",
        "    logging.info(\"Processing complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYf1TowcPoqn",
        "outputId": "271088cd-8608-4059-a08c-9453a84372a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-26 14:55:27,112 - INFO - Processing begins for sentiment analysis.\n",
            "2024-11-26 14:58:28,154 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/svb/company_df.csv\n",
            "2024-11-26 14:58:28,554 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 14:58:28,561 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/svb/conference_df.csv\n",
            "2024-11-26 14:58:28,763 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 14:58:28,769 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/svb/final_qa_df.csv\n",
            "2024-11-26 14:58:29,162 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 14:58:29,169 - INFO - Processing complete for sentiment analysis.\n",
            "2024-11-26 14:58:29,172 - INFO - Processing begins for metadata\n",
            "2024-11-26 14:58:29,182 - INFO - Starting to discover files in folder: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw\n",
            "2024-11-26 14:58:29,308 - INFO - Discovered 55 PDF files.\n",
            "2024-11-26 14:58:29,310 - INFO - Starting to process 55 PDF files.\n",
            "2024-11-26 14:58:29,317 - INFO - Sample file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q1 2021 Results - Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 14:58:29,324 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q1 2021 Results - Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 14:58:42,330 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q4 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:58:51,672 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q1 2022 Results - Earnings Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 14:58:58,477 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q1 2022 Results - Earnings Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:59:06,872 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q2 2022 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:59:13,437 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q1 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:59:23,585 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q2 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:59:30,845 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q4 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:59:40,840 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) Q3 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:59:43,955 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q3 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 14:59:53,184 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) Q4 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:00,142 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q2 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:10,314 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q3 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:14,666 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Greg Becker on Q3 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:19,715 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial_s (SIVB) CEO Greg Becker on Q4 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:23,871 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Greg Becker on Q1 2020 Results - Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:00:27,127 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Greg Becker on Q1 2022 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:32,106 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Gregory Becker on Q2 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:37,517 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) Q4 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:42,136 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Greg Becker on Q1 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:45,688 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Gregory Becker on Q2 2020 Results - Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:00:50,085 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Gregory Becker on Q3 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:54,959 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Gregory Becker on Q2 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:00:58,444 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) Q3 2022 Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:01:03,650 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Greg Becker on Q4 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:08,228 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) Q3 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:12,886 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Gregory Becker on Q1 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:16,774 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/svb/SVB Financial Group (SIVB) CEO Greg Becker on Q2 2022 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:22,909 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:25,977 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2020 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:29,475 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2022 FY2022 (1).pdf\n",
            "2024-11-26 15:01:32,886 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2023 Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:01:36,835 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2022 FY2022.pdf\n",
            "2024-11-26 15:01:39,752 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:42,487 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2023 Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:01:45,909 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2021 Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:01:50,522 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:53,981 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:56,633 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q2 2020 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:01:59,399 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2024 Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:02:03,773 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:06,877 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2022 Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:02:10,103 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2021 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:13,576 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:18,636 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q2 2021 FY2022.pdf\n",
            "2024-11-26 15:02:22,005 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:24,873 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q2 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:28,151 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2020 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:32,909 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:36,159 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2021 Earnings Call Transcript _ Seeking Alpha (1).pdf\n",
            "2024-11-26 15:02:39,183 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q3 2021 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:42,469 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q1 2020 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:46,975 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q1 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:49,812 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Nomura Holdings, Inc. (NMR) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
            "2024-11-26 15:02:52,630 - INFO - Processing file: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/raw/nomura/Sample/Q1 2021 FY2022.pdf\n",
            "2024-11-26 15:02:54,381 - INFO - Successfully processed 54 out of 55 PDF files.\n",
            "2024-11-26 15:02:54,385 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/pdf_summarytext_data.csv\n",
            "2024-11-26 15:02:54,722 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 15:02:54,728 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/metadata/company_participants.csv\n",
            "2024-11-26 15:02:55,149 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 15:02:55,152 - INFO - Processing complete for metadata\n",
            "2024-11-26 15:02:55,155 - INFO - Processing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcript Summarisation with NLTK"
      ],
      "metadata": {
        "id": "HBwKKzsxQG3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du3y-a9EmxQm",
        "outputId": "f21e5c9b-e500-4f32-a014-984b90aff74a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = punctuation + '\\n' + '—' + '“' + ',' + '”' + '‘' + '-' + '’'\n",
        "\n",
        "contractions_dict = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"doesn’t\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\"ain’t\": \"am not\",\n",
        "\"aren’t\": \"are not\",\n",
        "\"can’t\": \"cannot\",\n",
        "\"can’t’ve\": \"cannot have\",\n",
        "\"’cause\": \"because\",\n",
        "\"could’ve\": \"could have\",\n",
        "\"couldn’t\": \"could not\",\n",
        "\"couldn’t’ve\": \"could not have\",\n",
        "\"didn’t\": \"did not\",\n",
        "\"doesn’t\": \"does not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"hadn’t\": \"had not\",\n",
        "\"hadn’t’ve\": \"had not have\",\n",
        "\"hasn’t\": \"has not\",\n",
        "\"haven’t\": \"have not\",\n",
        "\"he’d\": \"he had\",\n",
        "\"he’d’ve\": \"he would have\",\n",
        "\"he’ll\": \"he will\",\n",
        "\"he’ll’ve\": \"he will have\",\n",
        "\"he’s\": \"he is\",\n",
        "\"how’d\": \"how did\",\n",
        "\"how’d’y\": \"how do you\",\n",
        "\"how’ll\": \"how will\",\n",
        "\"how’s\": \"how is\",\n",
        "\"i’d\": \"i would\",\n",
        "\"i’d’ve\": \"i would have\",\n",
        "\"i’ll\": \"i will\",\n",
        "\"i’ll’ve\": \"i will have\",\n",
        "\"i’m\": \"i am\",\n",
        "\"i’ve\": \"i have\",\n",
        "\"isn’t\": \"is not\",\n",
        "\"it’d\": \"it would\",\n",
        "\"it’d’ve\": \"it would have\",\n",
        "\"it’ll\": \"it will\",\n",
        "\"it’ll’ve\": \"it will have\",\n",
        "\"it’s\": \"it is\",\n",
        "\"let’s\": \"let us\",\n",
        "\"ma’am\": \"madam\",\n",
        "\"mayn’t\": \"may not\",\n",
        "\"might’ve\": \"might have\",\n",
        "\"mightn’t\": \"might not\",\n",
        "\"mightn’t’ve\": \"might not have\",\n",
        "\"must’ve\": \"must have\",\n",
        "\"mustn’t\": \"must not\",\n",
        "\"mustn’t’ve\": \"must not have\",\n",
        "\"needn’t\": \"need not\",\n",
        "\"needn’t’ve\": \"need not have\",\n",
        "\"o’clock\": \"of the clock\",\n",
        "\"oughtn’t\": \"ought not\",\n",
        "\"oughtn’t’ve\": \"ought not have\",\n",
        "\"shan’t\": \"shall not\",\n",
        "\"sha’n’t\": \"shall not\",\n",
        "\"shan’t’ve\": \"shall not have\",\n",
        "\"she’d\": \"she would\",\n",
        "\"she’d’ve\": \"she would have\",\n",
        "\"she’ll\": \"she will\",\n",
        "\"she’ll’ve\": \"she will have\",\n",
        "\"she’s\": \"she is\",\n",
        "\"should’ve\": \"should have\",\n",
        "\"shouldn’t\": \"should not\",\n",
        "\"shouldn’t’ve\": \"should not have\",\n",
        "\"so’ve\": \"so have\",\n",
        "\"so’s\": \"so is\",\n",
        "\"that’d\": \"that would\",\n",
        "\"that’d’ve\": \"that would have\",\n",
        "\"that’s\": \"that is\",\n",
        "\"there’d\": \"there would\",\n",
        "\"there’d’ve\": \"there would have\",\n",
        "\"there’s\": \"there is\",\n",
        "\"they’d\": \"they would\",\n",
        "\"they’d’ve\": \"they would have\",\n",
        "\"they’ll\": \"they will\",\n",
        "\"they’ll’ve\": \"they will have\",\n",
        "\"they’re\": \"they are\",\n",
        "\"they’ve\": \"they have\",\n",
        "\"to’ve\": \"to have\",\n",
        "\"wasn’t\": \"was not\",\n",
        "\"we’d\": \"we would\",\n",
        "\"we’d’ve\": \"we would have\",\n",
        "\"we’ll\": \"we will\",\n",
        "\"we’ll’ve\": \"we will have\",\n",
        "\"we’re\": \"we are\",\n",
        "\"we’ve\": \"we have\",\n",
        "\"weren’t\": \"were not\",\n",
        "\"what’ll\": \"what will\",\n",
        "\"what’ll’ve\": \"what will have\",\n",
        "\"what’re\": \"what are\",\n",
        "\"what’s\": \"what is\",\n",
        "\"what’ve\": \"what have\",\n",
        "\"when’s\": \"when is\",\n",
        "\"when’ve\": \"when have\",\n",
        "\"where’d\": \"where did\",\n",
        "\"where’s\": \"where is\",\n",
        "\"where’ve\": \"where have\",\n",
        "\"who’ll\": \"who will\",\n",
        "\"who’ll’ve\": \"who will have\",\n",
        "\"who’s\": \"who is\",\n",
        "\"who’ve\": \"who have\",\n",
        "\"why’s\": \"why is\",\n",
        "\"why’ve\": \"why have\",\n",
        "\"will’ve\": \"will have\",\n",
        "\"won’t\": \"will not\",\n",
        "\"won’t’ve\": \"will not have\",\n",
        "\"would’ve\": \"would have\",\n",
        "\"wouldn’t\": \"would not\",\n",
        "\"wouldn’t’ve\": \"would not have\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y’all’d\": \"you all would\",\n",
        "\"y’all’d’ve\": \"you all would have\",\n",
        "\"y’all’re\": \"you all are\",\n",
        "\"y’all’ve\": \"you all have\",\n",
        "\"you’d\": \"you would\",\n",
        "\"you’d’ve\": \"you would have\",\n",
        "\"you’ll\": \"you will\",\n",
        "\"you’ll’ve\": \"you will have\",\n",
        "\"you’re\": \"you are\",\n",
        "\"you’re\": \"you are\",\n",
        "\"you’ve\": \"you have\",\n",
        "}\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "# Function to clean the html from the article\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "# Function expand the contractions if there's any\n",
        "def expand_contractions(s, contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)\n",
        "\n",
        "# Function which generates the summary of the articles (This uses the 20% of the sentences with the highest score)\n",
        "def summary(sentence_score_OwO):\n",
        "    summary_list = []\n",
        "    for summ in sentence_score_OwO:\n",
        "        select_length = int(len(summ)*0.25)\n",
        "        summary_ = nlargest(select_length, summ, key = summ.get)\n",
        "        summary_list.append(\".\".join(summary_))\n",
        "    return summary_list\n",
        "\n",
        "# Function to normalize the word frequency which is used in the function word_frequency\n",
        "def normalize(li_word):\n",
        "    global normalized_freq\n",
        "    normalized_freq = []\n",
        "    for dictionary in li_word:\n",
        "        max_frequency = max(dictionary.values())\n",
        "        for word in dictionary.keys():\n",
        "            dictionary[word] = dictionary[word]/max_frequency\n",
        "        normalized_freq.append(dictionary)\n",
        "    return normalized_freq\n",
        "\n",
        "# Function to calculate the word frequency\n",
        "def word_frequency(article_word):\n",
        "    word_frequency = {}\n",
        "    li_word = []\n",
        "    for sentence in article_word:\n",
        "        for word in word_tokenize(sentence):\n",
        "            if word not in word_frequency.keys():\n",
        "                word_frequency[word] = 1\n",
        "            else:\n",
        "                word_frequency[word] += 1\n",
        "        li_word.append(word_frequency)\n",
        "        word_frequency = {}\n",
        "    normalize(li_word)\n",
        "    return normalized_freq\n",
        "\n",
        "# Function to Score the sentence which is called in the function sent_token\n",
        "def sentence_score(li):\n",
        "    global sentence_score_list\n",
        "    sentence_score = {}\n",
        "    sentence_score_list = []\n",
        "    for list_, dictionary in zip(li, normalized_freq):\n",
        "        for sent in list_:\n",
        "            for word in word_tokenize(sent):\n",
        "                if word in dictionary.keys():\n",
        "                    if sent not in sentence_score.keys():\n",
        "                        sentence_score[sent] = dictionary[word]\n",
        "                    else:\n",
        "                        sentence_score[sent] += dictionary[word]\n",
        "        sentence_score_list.append(sentence_score)\n",
        "        sentence_score = {}\n",
        "    return sentence_score_list\n",
        "\n",
        "# Function to tokenize the sentence\n",
        "def sent_token(article_sent):\n",
        "    sentence_list = []\n",
        "    sent_token = []\n",
        "    for sent in article_sent:\n",
        "        token = sent_tokenize(sent)\n",
        "        for sentence in token:\n",
        "            token_2 = ''.join(word for word in sentence if word not in punctuation)\n",
        "            token_2 = re.sub(' +', ' ',token_2)\n",
        "            sent_token.append(token_2)\n",
        "        sentence_list.append(sent_token)\n",
        "        sent_token = []\n",
        "    sentence_score(sentence_list)\n",
        "    return sentence_score_list\n",
        "\n",
        "# Function to preprocess the articles\n",
        "def preprocessing(article):\n",
        "    global article_sent\n",
        "\n",
        "    # Converting to lowercase\n",
        "    article = article.str.lower()\n",
        "\n",
        "    # Removing the HTML\n",
        "    article = article.apply(lambda x: cleanhtml(x))\n",
        "\n",
        "    # Removing the email ids\n",
        "    article = article.apply(lambda x: re.sub('\\S+@\\S+','', x))\n",
        "\n",
        "    # Removing The URLS\n",
        "    article = article.apply(lambda x: re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\",'', x))\n",
        "\n",
        "    # Removing the '\\xa0'\n",
        "    article = article.apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
        "\n",
        "    # Removing the contractions\n",
        "    article = article.apply(lambda x: expand_contractions(x))\n",
        "\n",
        "    # Stripping the possessives\n",
        "    article = article.apply(lambda x: x.replace(\"'s\", ''))\n",
        "    article = article.apply(lambda x: x.replace('’s', ''))\n",
        "    article = article.apply(lambda x: x.replace(\"\\'s\", ''))\n",
        "    article = article.apply(lambda x: x.replace(\"\\’s\", ''))\n",
        "\n",
        "    # Removing the Trailing and leading whitespace and double spaces\n",
        "    article = article.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "    # Copying the article for the sentence tokenization\n",
        "    article_sent = article.copy()\n",
        "\n",
        "    # Removing punctuations from the article\n",
        "    article = article.apply(lambda x: ''.join(word for word in x if word not in punctuation))\n",
        "\n",
        "    # Removing the Trailing and leading whitespace and double spaces again as removing punctuation might\n",
        "    # Lead to a white space\n",
        "    article = article.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "    # Removing the Stopwords\n",
        "    article = article.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
        "\n",
        "    return article\n",
        "\n",
        "# Functions to change the article string (if passed) to change it to generate a pandas series\n",
        "def make_series(art):\n",
        "    global dataframe\n",
        "    data_dict = {'article' : [art]}\n",
        "    dataframe = pd.DataFrame(data_dict)['article']\n",
        "    return dataframe\n",
        "\n",
        "# Function which is to be called to generate the summary which in further calls other functions alltogether\n",
        "def article_summarize(artefact):\n",
        "\n",
        "    if type(artefact) != pd.Series:\n",
        "        artefact = make_series(artefact)\n",
        "\n",
        "    df = preprocessing(artefact)\n",
        "\n",
        "    word_normalization = word_frequency(df)\n",
        "\n",
        "    sentence_score_OwO = sent_token(article_sent)\n",
        "\n",
        "    summarized_article = summary(sentence_score_OwO)\n",
        "\n",
        "    return summarized_article"
      ],
      "metadata": {
        "id": "t2yNB-OtP_Lp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  processed_folder = \"/content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed\"  # Path to save processed CSVs\n",
        "\n",
        "  csv_path = f'{processed_folder}/{bank}/final_qa_df.csv'\n",
        "  transcript_df = pd.read_csv(csv_path)\n",
        "\n",
        "  summaries = article_summarize(transcript_df['Dialogue'])\n",
        "\n",
        "  transcript_df[\"Summarised_dialogue\"] = \"\"\n",
        "  for i, row in transcript_df.iterrows():\n",
        "      transcript_df.loc[transcript_df.index == i, 'Summarised_dialogue'] = summaries[i]\n",
        "\n",
        "  save_to_csv(transcript_df, save_folder=f'{processed_folder}/{bank}', filename='final_qa_df.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSZVOgVfRCfx",
        "outputId": "03fbeb81-94ab-4701-e1f7-62a55030debb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-26 13:23:33,861 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/final_qa_df.csv\n",
            "2024-11-26 13:23:33,924 - INFO - Data successfully saved to CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis - using yiyanghkust/finbert-tone"
      ],
      "metadata": {
        "id": "zF5cT4aIRZ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFKDbS7RRjYa",
        "outputId": "8c313b73-ea24-48a0-812a-8a37a4146572"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface langchain chromadb pypdf sentence-transformers accelerate langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vELhKvTRRvGx",
        "outputId": "7355e501-75a1-42dd-ed38-c2d7b1248fce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.26.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.3.19)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.20.3)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.46.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.3-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.11)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain-huggingface)\n",
            "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.1.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
            "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading chromadb-0.5.20-py3-none-any.whl (617 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.9/617.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.8-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.3-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=e1a135f123d6dd6f2c475e946feb4837da64b12c678725aa6029c287c3123062\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, SQLAlchemy, python-dotenv, pyproject_hooks, pypdf, protobuf, overrides, opentelemetry-util-http, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, starlette, posthog, opentelemetry-proto, coloredlogs, build, pydantic-settings, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, dataclasses-json, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, langchain-huggingface, langchain, langchain-community, chromadb\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-2.0.35 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.20 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 fastapi-0.115.5 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-31.0.0 langchain-0.3.8 langchain-community-0.3.8 langchain-core-0.3.21 langchain-huggingface-0.1.2 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-util-http-0.49b2 overrides-7.7.0 posthog-3.7.3 protobuf-5.28.3 pydantic-settings-2.6.1 pypdf-5.1.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 typing-inspect-0.9.0 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.0 websockets-14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "OcXUS7LtR99u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "iC-pEAGxSBNR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_result(results):\n",
        "  positive_count = 0\n",
        "  neutral_count = 0\n",
        "  negative_count = 0\n",
        "\n",
        "  for result in results:\n",
        "    if result[\"label\"] == \"Positive\":\n",
        "      positive_count += 1\n",
        "    elif result[\"label\"] == \"Neutral\":\n",
        "      neutral_count += 1\n",
        "    else:\n",
        "      negative_count += 1\n",
        "\n",
        "  total_count = len(results)\n",
        "  negative_pct = (negative_count*100)/total_count\n",
        "  positive_pct = (positive_count*100)/total_count\n",
        "  neutral_pct = (neutral_count*100)/total_count\n",
        "\n",
        "  sentiment_pct = {\n",
        "    \"Positive\": positive_pct,\n",
        "    \"Neutral\": neutral_pct,\n",
        "    \"Negative\": negative_pct,\n",
        "  }\n",
        "\n",
        "  # Get the dominant sentiment\n",
        "  dominant_sentiment = max(sentiment_pct, key=sentiment_pct.get)\n",
        "\n",
        "  return dominant_sentiment, sentiment_pct\n",
        "\n",
        "# Initialize the RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "   chunk_size=512,\n",
        "   chunk_overlap=20,\n",
        "   length_function=len,\n",
        "   add_start_index=True,\n",
        ")\n",
        "\n",
        "def analyse_detail_sentiment(transcript_df, bank, dialogue_col):\n",
        "  detail_sentiment_data = []\n",
        "  transcript_grouped_df = transcript_df.groupby(by=[\"Year\", \"Quarter\"])\n",
        "  for name, groups in transcript_grouped_df:\n",
        "      for i, row in groups.iterrows():\n",
        "        text = row[dialogue_col]\n",
        "        if text:\n",
        "          #sentences = [text[i:i+512] for i in range(0, len(text), 512)]\n",
        "          documents = [Document(\n",
        "            page_content=text,\n",
        "            metadata=row.to_dict()\n",
        "          )]\n",
        "          chunks = text_splitter.split_documents(documents)\n",
        "          sentences = [chunk.page_content for chunk in chunks]\n",
        "          results = nlp(sentences)\n",
        "\n",
        "          sentiment, scores = get_result(results)\n",
        "\n",
        "          detail_sentiment_data.append({\n",
        "              \"Year\": row['Year'],\n",
        "              \"Quarter\": row['Quarter'],\n",
        "              \"Sentiment\": sentiment,\n",
        "              \"Sentiment_Score\": round(scores[sentiment], 2),\n",
        "              \"Bank\": bank,\n",
        "              \"Section\": row['Section']\n",
        "          })\n",
        "\n",
        "  return pd.DataFrame(detail_sentiment_data)"
      ],
      "metadata": {
        "id": "O_JG8vFWSDJM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_percentage(groups, total_count):\n",
        "    negative_count = len(groups[(groups[\"Sentiment\"]==\"Negative\")])\n",
        "    negative_pct = (negative_count*100)/total_count\n",
        "\n",
        "    positive_count = len(groups[(groups[\"Sentiment\"]==\"Positive\")])\n",
        "    positive_pct = (positive_count*100)/total_count\n",
        "\n",
        "    neutral_count = len(groups[(groups[\"Sentiment\"]==\"Neutral\")])\n",
        "    neutral_pct = (neutral_count*100)/total_count\n",
        "\n",
        "    sentiment_pct = {\n",
        "      \"Positive\": positive_pct,\n",
        "      \"Neutral\": neutral_pct,\n",
        "      \"Negative\": negative_pct,\n",
        "    }\n",
        "\n",
        "    # Get the dominant sentiment\n",
        "    dominant_sentiment = max(sentiment_pct, key=sentiment_pct.get)\n",
        "\n",
        "    return dominant_sentiment, sentiment_pct\n",
        "\n",
        "def summarise_sentiments(sentiment_df, section, dialogue_col):\n",
        "    sentiment_detail_grouped_df = sentiment_df.groupby(by=[\"Year\", \"Quarter\"])\n",
        "\n",
        "    quaterly_sentiment_data = []\n",
        "    for name, groups in sentiment_detail_grouped_df:\n",
        "        for i, row in groups.iterrows():\n",
        "            year = row[\"Year\"]\n",
        "            quarter = row[\"Quarter\"]\n",
        "\n",
        "        total_count = groups.Bank.count()\n",
        "\n",
        "        sentiment, sentiment_pct = get_sentiment_percentage(groups, total_count)\n",
        "\n",
        "        quaterly_sentiment_data.append({\n",
        "            \"Year\": row['Year'],\n",
        "            \"Quarter\": row['Quarter'],\n",
        "            \"Bank\": row['Bank'],\n",
        "            \"Section\": section,\n",
        "            \"Sentiment\": sentiment,\n",
        "            \"Positivity\": round(sentiment_pct[\"Positive\"], 2),\n",
        "            \"Neutrality\": round(sentiment_pct[\"Neutral\"], 2),\n",
        "            \"Negativity\": round(sentiment_pct[\"Negative\"], 2)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(quaterly_sentiment_data)\n",
        "\n",
        "def get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, sentiment_col):\n",
        "    qa_positivity = q_qa_sentiment_df.loc[(q_qa_sentiment_df['Year'] == row['Year']) & (q_qa_sentiment_df['Quarter'] == row['Quarter'])][sentiment_col]\n",
        "    presentation_positivity = q_presentation_sentiment_df.loc[(q_presentation_sentiment_df['Year'] == row['Year']) & (q_presentation_sentiment_df['Quarter'] == row['Quarter'])][sentiment_col]\n",
        "    combined_positivity = presentation_positivity.values[0] + qa_positivity.values[0]\n",
        "    if combined_positivity > 0:\n",
        "        combined_positivity = combined_positivity / 2\n",
        "    return combined_positivity\n",
        "\n",
        "def get_combined_sentiment(q_qa_sentiment_df, q_presentation_sentiment_df):\n",
        "    quaterly_sentiment_data = []\n",
        "    for i, row in q_presentation_sentiment_df.iterrows():\n",
        "        combined_positivity = get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, \"Positivity\")\n",
        "        combined_neutrality = get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, \"Neutrality\")\n",
        "        combined_negativity = get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, \"Negativity\")\n",
        "\n",
        "        if combined_negativity >= 7.5:\n",
        "            sentiment = \"Negative\"\n",
        "        elif combined_positivity > combined_neutrality:\n",
        "            sentiment = \"Positive\"\n",
        "        else:\n",
        "            sentiment = \"Neutral\"\n",
        "\n",
        "        quaterly_sentiment_data.append({\n",
        "            \"Year\": row['Year'],\n",
        "            \"Quarter\": row['Quarter'],\n",
        "            \"Bank\": row['Bank'],\n",
        "            \"Section\": 'Combined',\n",
        "            \"Sentiment\": sentiment,\n",
        "            \"Positivity\": round(combined_positivity, 2),\n",
        "            \"Neutrality\": round(combined_neutrality, 2),\n",
        "            \"Negativity\": round(combined_negativity, 2)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(quaterly_sentiment_data)"
      ],
      "metadata": {
        "id": "KD5xdOHZSGkT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  processed_folder = \"/content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed\"  # Path to save processed CSVs\n",
        "\n",
        "  csv_path = f'{processed_folder}/{bank}/final_qa_df.csv'\n",
        "  transcript_df = pd.read_csv(csv_path)\n",
        "  transcript_df = transcript_df[transcript_df['Text Type'] != \"Operator\"]\n",
        "\n",
        "  dialogue_cols = [\"Summarised_dialogue\", \"Dialogue\"]\n",
        "  for dialogue_col in dialogue_cols:\n",
        "\n",
        "      dialog_df = transcript_df.dropna(subset=[dialogue_col])\n",
        "\n",
        "      # Sentiment analysis for dialog\n",
        "      detail_sentiment_df = analyse_detail_sentiment(dialog_df, bank, dialogue_col)\n",
        "      save_to_csv(detail_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'detail_sentiment_df_{dialogue_col}.csv')\n",
        "\n",
        "      # Summarise quaterly sentiments - Presentation section\n",
        "      presentation_sentiment_df = detail_sentiment_df[detail_sentiment_df['Section'] == \"Presentation\"]\n",
        "      quaterly_presentation_sentiment_df = summarise_sentiments(presentation_sentiment_df, \"Presentation\", dialogue_col)\n",
        "      save_to_csv(quaterly_presentation_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'quaterly_presentation_sentiment_df_{dialogue_col}.csv')\n",
        "\n",
        "      # Summarise quaterly sentiments - Question-and-Answer section\n",
        "      qa_sentiment_df = detail_sentiment_df[detail_sentiment_df['Section'] == \"Question-and-Answer\"]\n",
        "      quaterly_qa_sentiment_df = summarise_sentiments(qa_sentiment_df, \"Question-and-Answer\", dialogue_col)\n",
        "      save_to_csv(quaterly_qa_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'quaterly_qa_sentiment_df_{dialogue_col}.csv')\n",
        "\n",
        "      # Summarise quaterly sentiments - combined\n",
        "      quaterly_combined_sentiment_df = get_combined_sentiment(quaterly_qa_sentiment_df, quaterly_presentation_sentiment_df)\n",
        "      save_to_csv(quaterly_combined_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'quaterly_combined_sentiment_df_{dialogue_col}.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwopFD3KSPuX",
        "outputId": "5870192a-896d-4058-bf6b-03017143aafb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-26 13:39:03,029 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/detail_sentiment_df_Summarised_dialogue.csv\n",
            "2024-11-26 13:39:03,343 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 13:39:03,376 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/quaterly_presentation_sentiment_df_Summarised_dialogue.csv\n",
            "2024-11-26 13:39:03,695 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 13:39:03,763 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/quaterly_qa_sentiment_df_Summarised_dialogue.csv\n",
            "2024-11-26 13:39:04,082 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 13:39:04,144 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/quaterly_combined_sentiment_df_Summarised_dialogue.csv\n",
            "2024-11-26 13:39:04,453 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 13:51:04,023 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/detail_sentiment_df_Dialogue.csv\n",
            "2024-11-26 13:51:04,217 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 13:51:04,256 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/quaterly_presentation_sentiment_df_Dialogue.csv\n",
            "2024-11-26 13:51:04,469 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 13:51:04,612 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/quaterly_qa_sentiment_df_Dialogue.csv\n",
            "2024-11-26 13:51:04,920 - INFO - Data successfully saved to CSV.\n",
            "2024-11-26 13:51:05,005 - INFO - Saving data to CSV at: /content/drive/MyDrive/Colab Notebooks/Employer Project/data/processed/bank_svb/quaterly_combined_sentiment_df_Dialogue.csv\n",
            "2024-11-26 13:51:05,226 - INFO - Data successfully saved to CSV.\n"
          ]
        }
      ]
    }
  ]
}