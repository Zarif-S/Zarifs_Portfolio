{"cells":[{"cell_type":"markdown","metadata":{"id":"NmB9-jDAY0zz"},"source":["# RAG Based Implementation"]},{"cell_type":"markdown","source":["## Pip Installs and Import Statements"],"metadata":{"id":"qjtoeCRRDDA0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DN3du5hU9X45","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1f034fc-8d16-46f0-d17e-0dab949e8007"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-huggingface\n","  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n","Collecting chromadb\n","  Downloading chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\n","Collecting pypdf\n","  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n","Collecting pypdf2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.26.2)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.3.19)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.20.3)\n","Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.46.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n","Collecting build>=1.0.3 (from chromadb)\n","  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n","Collecting chroma-hnswlib==0.7.6 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n","Collecting fastapi>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.7.2-py2.py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.0)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.0)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.11)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n","Collecting SQLAlchemy<3,>=1.4 (from langchain)\n","  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain\n","  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.15 (from langchain-huggingface)\n","  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n","Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n","  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.1.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n","  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n","Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n","Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n","  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n","Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n","Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n","Downloading chromadb-0.5.20-py3-none-any.whl (617 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.9/617.9 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain-0.3.8-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n","Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n","Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n","Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n","Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n","Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-3.7.2-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n","Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n","Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n","Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Building wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=d2a84c77790e5fee639509eee1e23d48296f1c202497da1f8cbcb60da65d80a6\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, SQLAlchemy, python-dotenv, pyproject_hooks, pypdf2, pypdf, protobuf, overrides, opentelemetry-util-http, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, starlette, posthog, opentelemetry-proto, coloredlogs, build, pydantic-settings, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, dataclasses-json, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, langchain-huggingface, langchain, langchain-community, chromadb\n","  Attempting uninstall: SQLAlchemy\n","    Found existing installation: SQLAlchemy 2.0.36\n","    Uninstalling SQLAlchemy-2.0.36:\n","      Successfully uninstalled SQLAlchemy-2.0.36\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.25.5\n","    Uninstalling protobuf-4.25.5:\n","      Successfully uninstalled protobuf-4.25.5\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.19\n","    Uninstalling langchain-core-0.3.19:\n","      Successfully uninstalled langchain-core-0.3.19\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.7\n","    Uninstalling langchain-0.3.7:\n","      Successfully uninstalled langchain-0.3.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed SQLAlchemy-2.0.35 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.20 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 fastapi-0.115.5 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-31.0.0 langchain-0.3.8 langchain-community-0.3.8 langchain-core-0.3.21 langchain-huggingface-0.1.2 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-util-http-0.49b2 overrides-7.7.0 posthog-3.7.2 protobuf-5.28.3 pydantic-settings-2.6.1 pypdf-5.1.0 pypdf2-3.0.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 typing-inspect-0.9.0 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.0 websockets-14.1\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.8)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Collecting transformers\n","  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Collecting sentence-transformers\n","  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.20)\n","Collecting langchain_chroma\n","  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.21)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\n","Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n","Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.5)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.1)\n","Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.7.2)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.20.1)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n","Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.49b2)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.0)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.1)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.0)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (31.0.0)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.0.1)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.11)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.1.0)\n","Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n","Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n","Requirement already satisfied: opentelemetry-proto==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n","Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n","Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n","Requirement already satisfied: opentelemetry-util-http==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n","Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n","Installing collected packages: transformers, sentence-transformers, langchain_chroma\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.46.2\n","    Uninstalling transformers-4.46.2:\n","      Successfully uninstalled transformers-4.46.2\n","  Attempting uninstall: sentence-transformers\n","    Found existing installation: sentence-transformers 3.2.1\n","    Uninstalling sentence-transformers-3.2.1:\n","      Successfully uninstalled sentence-transformers-3.2.1\n","Successfully installed langchain_chroma-0.1.4 sentence-transformers-3.3.1 transformers-4.46.3\n","Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n","Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.8.0\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: langchain_chroma in /usr/local/lib/python3.10/dist-packages (0.1.4)\n","Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (0.5.20)\n","Requirement already satisfied: fastapi<1,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (0.115.5)\n","Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (0.3.21)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (1.26.4)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2.post1)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.9.2)\n","Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.6)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.32.1)\n","Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.7.2)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.12.2)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.20.1)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.20.3)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.66.6)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.68.0)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.2.1)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.13.0)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (31.0.0)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (9.0.0)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.0.2)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.0.1)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.11)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.27.2)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (13.9.4)\n","Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.41.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (0.1.143)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (24.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.1.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain_chroma) (3.0.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.3)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.9)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain_chroma) (1.0.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.28.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.15)\n","Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.5.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.66.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-proto==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-util-http==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n","Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.8.1)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.6)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.23.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.26.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.5.4)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.21.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (14.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.10.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.4.0)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n","Requirement already satisfied: langchain_huggingface in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: langchain_chroma in /usr/local/lib/python3.10/dist-packages (0.1.4)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.26.2)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.3.21)\n","Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (3.3.1)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.20.3)\n","Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.46.3)\n","Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (0.5.20)\n","Requirement already satisfied: fastapi<1,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (0.115.5)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (1.26.4)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2.post1)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.9.2)\n","Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.6)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.32.1)\n","Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.7.2)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.12.2)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.20.1)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.66.6)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.68.0)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.2.1)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.13.0)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (31.0.0)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (9.0.0)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.0.2)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.0.1)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.11)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.27.2)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (13.9.4)\n","Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.41.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.1.143)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.1.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.3)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.9)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.28.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.15)\n","Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.5.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.66.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-proto==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.28.2)\n","Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: opentelemetry-util-http==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.49b2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n","Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.8.1)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.6)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.5.4)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.21.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.1.2)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n","Collecting bertopic\n","  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n","Collecting hdbscan>=0.8.29 (from bertopic)\n","  Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.26.4)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n","Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n","Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.2)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.3.1)\n","Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.6)\n","Collecting umap-learn>=0.5.0 (from bertopic)\n","  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.46.3)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.1+cu121)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.26.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.0.0)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n","Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n","  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n","Downloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pynndescent, hdbscan, umap-learn, bertopic\n","Successfully installed bertopic-0.16.4 hdbscan-0.8.40 pynndescent-0.5.13 umap-learn-0.5.7\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n","Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.40)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.26.4)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n","Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n","Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.2)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.3.1)\n","Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.7)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.46.3)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.1+cu121)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.26.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.0.0)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n","Collecting pyLDAvis\n","  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n","Collecting funcy (from pyLDAvis)\n","  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n","Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-2.0 pyLDAvis-3.4.1\n","Collecting en-core-web-md==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.13.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["# Install and upgrade necessary libraries for RAG Pipeline and Topic Modeling\n","\n","# Install core libraries for RAG pipeline\n","!pip install langchain-huggingface langchain chromadb pypdf sentence-transformers accelerate langchain-community pypdf2 torch\n","\n","# Upgrade libraries to the latest versions\n","!pip install --upgrade langchain transformers sentence-transformers langchain-huggingface chromadb langchain_chroma\n","\n","# Install and upgrade additional libraries\n","!pip install tiktoken\n","!pip install --upgrade torch torchvision torchaudio transformers sentence-transformers\n","!pip install -U langchain_chroma\n","!pip install -U langchain_huggingface langchain_chroma\n","\n","# Install libraries for topic modeling\n","!pip install bertopic\n","!pip install nltk bertopic\n","!pip install pyLDAvis\n","\n","# Download Spacy English model for NER\n","!python -m spacy download en_core_web_md\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zw0Ja11P9X48"},"outputs":[],"source":["# =====================================================================\n","# 1. STANDARD LIBRARIES\n","# =====================================================================\n","import logging                        # For logging system setup and debugging\n","import re                             # For regular expressions and text processing\n","from typing import List, Dict, Set    # For type hinting\n","import os                             # For operating system interactions\n","import sys                            # For system-specific parameters and functions\n","\n","# =====================================================================\n","# 2. THIRD-PARTY LIBRARIES\n","# =====================================================================\n","# 2.1 Data Analysis\n","import pandas as pd                   # For handling structured data\n","\n","# 2.2 Machine Learning and Deep Learning\n","import torch                          # For PyTorch models and tensors\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # For Hugging Face transformers\n","\n","# 2.3 Natural Language Processing (NLP)\n","from nltk.corpus import stopwords     # For stopword filtering\n","from nltk import download             # For downloading NLTK resources\n","import spacy                          # For natural language processing and named entity recognition\n","\n","# 2.4 Visualization\n","import pyLDAvis                             # For LDA topic visualization\n","import pyLDAvis.gensim_models as gensimvis  # For integrating Gensim models with pyLDAvis\n","\n","# 2.5 Topic Modeling\n","from bertopic import BERTopic               # For topic modeling using BERTopic\n","from sklearn.feature_extraction.text import CountVectorizer   # For feature extraction in BERTopic\n","from sentence_transformers import SentenceTransformer         # For sentence embeddings\n","\n","# 2.6 Gensim Topic Modeling\n","from gensim.models import LdaModel                            # For Gensim LDA topic modeling\n","from gensim.models.phrases import Phrases, Phraser            # For creating bigrams and trigrams\n","from gensim.corpora import Dictionary                         # For creating a dictionary for Gensim models\n","\n","# =====================================================================\n","# 3. LANGCHAIN MODULES\n","# =====================================================================\n","## 3.1 Document Management\n","from langchain.document_loaders import PyPDFLoader                  # For loading PDF documents\n","from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into chunks\n","from langchain.schema import Document                               # For representing document objects\n","\n","## 3.2 Prompts and Chains\n","from langchain.prompts import PromptTemplate                        # For creating custom prompts\n","from langchain.chains.question_answering import load_qa_chain       # For setting up QA chains\n","from langchain.chains import LLMChain                               # For generic language model chains\n","\n","## 3.3 Embeddings and Vector Store\n","from langchain.embeddings import HuggingFaceEmbeddings              # For generating document embeddings\n","from langchain_chroma import Chroma                                 # For Chroma vector store integration\n","\n","# =====================================================================\n","# 4. HUGGING FACE INTEGRATION\n","# =====================================================================\n","from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings  # For Hugging Face integration\n","from transformers import pipeline as hf_pipeline                              # For Hugging Face pipeline utilities\n"]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"],"metadata":{"id":"PFg3vSs1M5Um","colab":{"base_uri":"https://localhost:8080/"},"outputId":"989967c9-f598-47fa-fe7e-c4205f76b315"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["### Log configurations"],"metadata":{"id":"ZdJhqKzYDKGX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGFaYWUv9X48","colab":{"base_uri":"https://localhost:8080/"},"outputId":"feef63aa-6dcf-4009-f475-6a60c378e651"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-25 19:00:30,544 - INFO - Logging system is configured.\n"]}],"source":["# Create or get logger\n","logger = logging.getLogger(\"RAGRetriever\")\n","\n","# Clear existing handlers\n","if logger.hasHandlers():\n","    logger.handlers.clear()\n","\n","# Disable propagation to ancestor loggers\n","logger.propagate = False\n","\n","# Set logger level\n","logger.setLevel(logging.INFO)\n","\n","# Formatter\n","formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n","\n","# File handler\n","file_handler = logging.FileHandler(\"pdf_processing_errors.log\")\n","file_handler.setFormatter(formatter)\n","logger.addHandler(file_handler)\n","\n","# Console handler\n","console_handler = logging.StreamHandler(sys.stdout)\n","console_handler.setFormatter(formatter)\n","logger.addHandler(console_handler)\n","\n","# Example usage\n","logger.info(\"Logging system is configured.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nY0LbO8i9X49","colab":{"base_uri":"https://localhost:8080/"},"outputId":"023b1063-feb0-4e5e-ae05-dfb3f8183d79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"XnAjr00sHe3t"},"source":["### Preprocessing Utilities\n","\n","Purpose: These functions handle text preprocessing for cleaning, tokenizing, and preparing text for embedding, summarization, or topic modeling.\n","\n","**Functions:**\n","\n","- normalize_metadata\n","- normalize_metadata_values\n","- preprocess_text\n","- preprocess_text_gensim\n","- preprocess_corpus_gensim\n","- preprocess_corpus_bertopic\n","- remove_repetitions\n","- get_page_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3Jn7SIsHe3t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2fe9a8b6-8495-485c-8570-3942f90febc2"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Download NLTK stopwords\n","download(\"stopwords\")\n","DEFAULT_STOPWORDS = set(stopwords.words(\"english\"))\n","\n","# Load Spacy NLP model for NER\n","nlp_model = spacy.load(\"en_core_web_md\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqFy1nLSHe3t"},"outputs":[],"source":["def normalize_metadata_values(metadata: Dict, required_fields: Set[str], case: str = \"lower\") -> Dict:\n","    \"\"\"\n","    Normalizes metadata fields and ensures required fields are present.\n","    Args:\n","        metadata (dict): Metadata dictionary.\n","        required_fields (set): Set of required metadata fields.\n","        case (str): Case normalization ('lower', 'upper', or 'title').\n","    Returns:\n","        dict: Normalized metadata.\n","    \"\"\"\n","    normalized_metadata = {}\n","\n","    for key in required_fields:\n","        value = metadata.get(key)\n","        normalized_metadata[key] = \"N/A\" if value is None else str(value).strip().lower() if case == \"lower\" else str(value).strip().upper()\n","\n","    return normalized_metadata\n","\n","def normalize_metadata(metadata_df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Normalizes the metadata DataFrame by standardizing text fields and handling missing values.\n","    Args:\n","        metadata_df (pd.DataFrame): The metadata DataFrame to normalize.\n","    Returns:\n","        pd.DataFrame: The normalized metadata DataFrame.\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting metadata normalization...\")\n","\n","        required_fields = {\"Year\", \"Quarter\", \"Bank\", \"Designation\", \"Name\", \"Source\"}\n","\n","        # Check for required columns\n","        missing_columns = required_fields - set(metadata_df.columns)\n","        if missing_columns:\n","            raise KeyError(f\"Metadata DataFrame is missing required columns: {missing_columns}\")\n","\n","        # Normalize all fields using `normalize_metadata_values`\n","        normalized_metadata = metadata_df.apply(\n","            lambda row: normalize_metadata_values(row.to_dict(), required_fields, case=\"lower\"), axis=1\n","        )\n","        normalized_df = pd.DataFrame(list(normalized_metadata))\n","\n","        logger.info(f\"Metadata normalization completed successfully for {len(metadata_df)} rows.\")\n","        return normalized_df\n","\n","    except KeyError as ke:\n","        logger.error(f\"Metadata normalization error: {ke}\")\n","        raise\n","    except Exception as e:\n","        logger.error(f\"Unexpected error during metadata normalization: {e}\")\n","        raise\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmDS3c75He3u"},"outputs":[],"source":["def preprocess_text(\n","    text: str,\n","    nlp_model=None,\n","    custom_stopwords=None,\n","    redundant_terms=None\n",") -> list:\n","    \"\"\"\n","    Preprocess text for BERTopic.\n","\n","    Args:\n","        text (str): Raw text input.\n","        nlp_model (spacy.Language): SpaCy model for NER.\n","        custom_stopwords (list): Additional stopwords.\n","        redundant_terms (list): Terms to explicitly remove.\n","\n","    Returns:\n","        list: Tokenized and cleaned words.\n","    \"\"\"\n","    try:\n","        # Normalize text\n","        text = text.lower()\n","\n","        # Remove URLs\n","        text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n","\n","        # Remove emails\n","        text = re.sub(r\"\\S+@\\S+\", \"\", text)\n","\n","        # Use NER to remove names\n","        if nlp_model:\n","            doc = nlp_model(text)\n","            filtered_tokens = [token.text for token in doc if token.ent_type_ != \"PERSON\"]\n","            text = \" \".join(filtered_tokens)\n","\n","        # Default redundant terms\n","        default_redundant_terms = [\n","            \"thank you\", \"good morning\", \"good afternoon\", \"earnings call\",\n","            \"conference call\", \"slide\", \"question\", \"operator\"\n","        ]\n","        redundant_terms = set(redundant_terms or []).union(default_redundant_terms)\n","\n","        # Remove redundant terms\n","        for term in redundant_terms:\n","            text = re.sub(rf\"\\b{re.escape(term)}\\b\", \"\", text)\n","\n","        # Remove special characters and normalize whitespace\n","        text = re.sub(r\"[^a-z\\s]\", \"\", text).strip()\n","        text = re.sub(r\"\\s+\", \" \", text)\n","\n","        # Tokenize and remove stopwords\n","        stop_words = DEFAULT_STOPWORDS.union(custom_stopwords or [])\n","        tokens = [word for word in text.split() if word not in stop_words]\n","\n","        return tokens\n","    except Exception as e:\n","        logger.error(f\"Error during text preprocessing: {e}\")\n","        return []\n","\n","def preprocess_corpus_bertopic(\n","    corpus, nlp_model=None, custom_stopwords=None, redundant_terms=None, min_count=2, threshold=5\n","):\n","    \"\"\"\n","    Preprocess a corpus for BERTopic and optionally generate bigrams and trigrams.\n","\n","    Args:\n","        corpus (list of str): List of raw text documents.\n","        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n","        custom_stopwords (list): Additional stopwords to remove.\n","        redundant_terms (list): Terms to explicitly remove from text.\n","        min_count (int): Minimum count for bigram detection.\n","        threshold (int): Phrase scoring threshold for bigram detection.\n","\n","    Returns:\n","        list of str: Preprocessed text documents (joined tokens).\n","    \"\"\"\n","    logger.info(\"Starting corpus preprocessing for BERTopic...\")\n","\n","    def remove_urls(text):\n","        \"\"\"Remove URLs from the text.\"\"\"\n","        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n","\n","    # Preprocess each document in the corpus\n","    logger.info(\"Preprocessing individual documents...\")\n","    tokenized_corpus = []\n","    for doc in corpus:\n","        # Normalize text and remove URLs\n","        doc = remove_urls(doc)\n","\n","        # Tokenize, filter stopwords, and remove redundant terms\n","        tokens = preprocess_text(doc, nlp_model, custom_stopwords, redundant_terms)\n","        tokenized_corpus.append(tokens)\n","\n","    # Log tokenized sample\n","    logger.info(\"Sample preprocessed tokens:\")\n","    logger.info(tokenized_corpus[:2])\n","\n","    # Build bigram and trigram models with adjusted parameters\n","    logger.info(\"Building bigram and trigram models...\")\n","    bigram_model = Phrases(tokenized_corpus, min_count=min_count, threshold=threshold)\n","    trigram_model = Phrases(bigram_model[tokenized_corpus], threshold=threshold)\n","    bigram_phraser = Phraser(bigram_model)\n","    trigram_phraser = Phraser(trigram_model)\n","\n","    # Apply bigram and trigram models to the corpus\n","    logger.info(\"Applying bigram and trigram models...\")\n","    processed_corpus = []\n","    for doc in tokenized_corpus:\n","        phrases = trigram_phraser[bigram_phraser[doc]]\n","        # Join tokens back into strings for BERTopic\n","        processed_corpus.append(\" \".join(phrases))\n","\n","    # Log sample processed corpus\n","    logger.info(\"Sample processed documents for BERTopic:\")\n","    logger.info(processed_corpus[:2])\n","\n","    return processed_corpus\n","\n","\n","def preprocess_text_gensim(text, nlp_model=None, custom_stopwords=None, redundant_terms=None):\n","    \"\"\"\n","    Preprocess text for LDA topic modeling, focusing on bigrams and trigrams.\n","\n","    Args:\n","        text (str): Raw text to preprocess.\n","        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n","        custom_stopwords (list): Additional stopwords to remove.\n","        redundant_terms (list): Terms to explicitly remove from text.\n","\n","    Returns:\n","        list: Tokenized and preprocessed words.\n","    \"\"\"\n","    # Normalize text to lowercase\n","    text = text.lower()\n","\n","    # Remove URLs\n","    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n","\n","    # Remove names using Spacy NER if model is providedz\n","    if nlp_model:\n","        doc = nlp_model(text)\n","        filtered_tokens = [token.text for token in doc if token.ent_type_ != \"PERSON\"]\n","        text = \" \".join(filtered_tokens)\n","\n","    # Default redundant terms to remove\n","    default_redundant_terms = [\n","        \"seeking_alpha\", \"thank_much\", \"group_ag_cs_results\", \"transcript_seeking_alpha\",\n","        \"good_morning\", \"good_afternoon\", \"earnings_call\", \"company\", \"presentation\",\n","        \"analyst\", \"operator\", \"thomas\", \"gottstein\"\n","    ]\n","\n","    # Merge default and custom redundant terms\n","    if redundant_terms:\n","        redundant_terms.extend(default_redundant_terms)\n","    else:\n","        redundant_terms = default_redundant_terms\n","\n","    # Remove redundant terms using regex\n","    for term in redundant_terms:\n","        text = re.sub(rf\"\\b{re.escape(term.lower())}\\b\", \"\", text)\n","\n","    # Remove special characters, digits, and extra whitespace\n","    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove non-alphabetic characters\n","    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n","\n","    # Tokenize and remove stopwords\n","    stop_words = DEFAULT_STOPWORDS\n","    if custom_stopwords:\n","        stop_words.update(custom_stopwords)\n","\n","    tokens = [word for word in text.split() if word not in stop_words]\n","\n","    return tokens\n","\n","def preprocess_corpus_gensim(\n","    corpus, nlp_model=None, custom_stopwords=None, redundant_terms=None, extra_redundant_terms=None, min_count=2, threshold=5, keep_unigrams=False\n","):\n","    \"\"\"\n","    Preprocess a corpus for LDA topic modeling and generate bigrams and trigrams.\n","\n","    Args:\n","        corpus (list of str): List of raw text documents.\n","        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n","        custom_stopwords (list): Additional stopwords to remove.\n","        redundant_terms (list): Terms to explicitly remove from text.\n","        min_count (int): Minimum count for bigram/trigram detection.\n","        threshold (int): Phrase scoring threshold for bigram/trigram detection.\n","        keep_unigrams (bool): If True, include unigrams alongside bigrams/trigrams.\n","\n","    Returns:\n","        list of list: Preprocessed tokenized documents with bigrams/trigrams (and optionally unigrams).\n","    \"\"\"\n","    logger.info(\"Starting corpus preprocessing for bigrams and trigrams...\")\n","\n","    def remove_urls(text):\n","        \"\"\"Remove URLs from the text.\"\"\"\n","        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n","\n","    # Preprocess each document in the corpus\n","    logger.info(\"Preprocessing individual documents...\")\n","    tokenized_corpus = []\n","    for doc in corpus:\n","        # Normalize text and remove URLs\n","        doc = remove_urls(doc)\n","\n","        # Tokenize, filter stopwords, and remove redundant terms\n","        tokens = preprocess_text_gensim(doc, nlp_model, custom_stopwords, redundant_terms)\n","        tokenized_corpus.append(tokens)\n","\n","    # Log tokenized sample\n","    logger.info(\"Sample preprocessed tokens:\")\n","    logger.info(tokenized_corpus[:2])\n","\n","    # Build bigram and trigram models with adjusted parameters\n","    logger.info(\"Building bigram and trigram models...\")\n","    bigram_model = Phrases(tokenized_corpus, min_count=min_count, threshold=threshold)\n","    trigram_model = Phrases(bigram_model[tokenized_corpus], threshold=threshold)\n","    bigram_phraser = Phraser(bigram_model)\n","    trigram_phraser = Phraser(trigram_model)\n","\n","    # Apply bigram and trigram models to the corpus\n","    logger.info(\"Applying bigram and trigram models...\")\n","    processed_corpus = []\n","    for doc in tokenized_corpus:\n","        phrases = trigram_phraser[bigram_phraser[doc]]\n","        if keep_unigrams:\n","            # Include unigrams alongside bigrams/trigrams\n","            processed_corpus.append(list(phrases))\n","        else:\n","            # Keep only bigrams/trigrams\n","            processed_corpus.append([token for token in phrases if \"_\" in token])\n","\n","    # Log sample processed corpus\n","    logger.info(\"Sample processed documents with bigrams/trigrams:\")\n","    logger.info(processed_corpus[:2])\n","\n","    return processed_corpus\n"]},{"cell_type":"markdown","metadata":{"id":"41zcyOPOHe3u"},"source":["### Pipeline Configuration\n","\n","Purpose: These functions set up the overall Retrieval-Augmented Generation (RAG) pipeline, including metadata handling, document loading, embedding generation, and retriever initialization.\n","\n","**Functions:**\n","- load_metadata\n","- load_pdf_file\n","- load_documents\n","- attach_metadata_to_documents\n","- propagate_metadata_to_chunks\n","- validate_metadata\n","- initialize_embeddings_and_vector_store\n","- setup_retriever\n","- load_model\n","- configure_rag_pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgVhadqSHe3v"},"outputs":[],"source":["def load_metadata(metadata_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Loads and normalizes metadata from the specified CSV file.\n","    Args:\n","        metadata_path (str): Path to the metadata CSV file.\n","    Returns:\n","        pd.DataFrame: The normalized metadata DataFrame.\n","    \"\"\"\n","    try:\n","        logger.info(f\"Loading metadata from: {metadata_path}\")\n","\n","        metadata_df = pd.read_csv(metadata_path)\n","        logger.info(f\"Loaded {len(metadata_df)} rows of metadata.\")\n","\n","        # Rename 'File' column to 'Source' if necessary\n","        if \"File\" in metadata_df.columns:\n","            metadata_df.rename(columns={\"File\": \"Source\"}, inplace=True)\n","\n","        # Normalize the metadata\n","        metadata_df = normalize_metadata(metadata_df)\n","\n","        logger.info(f\"Loaded data with meta data completed successfully for {len(metadata_df)} rows.\")\n","        return metadata_df\n","\n","    except pd.errors.EmptyDataError:\n","        logger.error(\"The metadata file is empty or improperly formatted.\")\n","        raise ValueError(\"The metadata file is empty or improperly formatted.\")\n","    except KeyError as ke:\n","        logger.error(f\"Metadata normalization error: {ke}\")\n","        raise\n","    except Exception as e:\n","        logger.error(f\"Unexpected error during metadata loading: {e}\")\n","        raise\n","\n","\n","def load_pdf_file(file_path: str) -> List[Document]:\n","    \"\"\"\n","    Loads and splits a single PDF file into document objects.\n","\n","    Args:\n","        file_path (str): Path to the PDF file.\n","\n","    Returns:\n","        List[Document]: A list of document objects with metadata.\n","    \"\"\"\n","    try:\n","        loader = PyPDFLoader(file_path)\n","        pdf_documents = loader.load_and_split()\n","        logger.info(f\"Loaded {len(pdf_documents)} pages from {file_path}\")\n","\n","        # Assign normalized source metadata to each document\n","        for doc in pdf_documents:\n","            doc.metadata = {\"source\": os.path.basename(file_path).strip().lower()}\n","\n","        return pdf_documents\n","    except Exception as e:\n","        logger.error(f\"Error loading or processing PDF '{file_path}': {e}\")\n","        return []\n","\n","def load_documents(folder_path: str) -> List[Document]:\n","    \"\"\"\n","    Loads PDF documents from the specified folder.\n","    Args:\n","        folder_path (str): Path to the folder containing PDF files.\n","    Returns:\n","        List[Document]: A list of document objects with metadata.\n","    \"\"\"\n","    try:\n","        if not os.path.exists(folder_path):\n","            raise FileNotFoundError(f\"Specified folder path does not exist: {folder_path}\")\n","\n","        logger.info(f\"Loading PDF files from folder: {folder_path}\")\n","\n","        documents = [\n","            doc\n","            for filename in os.listdir(folder_path)\n","            if filename.lower().endswith(\".pdf\")\n","            for doc in load_pdf_file(os.path.join(folder_path, filename))\n","        ]\n","\n","        if not documents:\n","            logger.warning(\"No valid PDF documents were loaded from the folder.\")\n","        else:\n","            logger.info(f\"Successfully loaded {len(documents)} documents from {folder_path}.\")\n","\n","        return documents\n","    except Exception as e:\n","        logger.error(f\"An error occurred while loading documents: {e}\")\n","        return []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Utr_IIwwHe3v"},"outputs":[],"source":["def attach_metadata_to_documents(documents: List[Document], metadata_df: pd.DataFrame) -> List[Document]:\n","    \"\"\"\n","    Attaches metadata to documents based on their source.\n","    Args:\n","        documents (List[Document]): List of document objects.\n","        metadata_df (pd.DataFrame): Normalized metadata DataFrame.\n","    Returns:\n","        List[Document]: List of documents with enriched metadata.\n","    \"\"\"\n","    try:\n","        if \"Source\" not in metadata_df.columns:\n","            raise KeyError(\"The metadata DataFrame is missing the 'Source' column.\")\n","\n","        enriched_documents = []\n","\n","        for doc in documents:\n","            source_file = os.path.basename(doc.metadata.get(\"source\", \"\")).lower().strip()\n","            logger.debug(f\"Processing document with source: {source_file}\")\n","\n","            matched_metadata = metadata_df[metadata_df[\"Source\"] == source_file]\n","\n","            if not matched_metadata.empty:\n","                metadata_dict = matched_metadata.iloc[0].to_dict()\n","                doc.metadata = {\n","                    key.lower(): str(value).strip() if pd.notna(value) else \"n/a\"\n","                    for key, value in metadata_dict.items()\n","                }\n","            else:\n","                logger.warning(f\"No metadata match found for source: {source_file}\")\n","\n","            enriched_documents.append(doc)\n","\n","        return enriched_documents\n","    except Exception as e:\n","        logger.error(f\"Error attaching metadata to documents: {e}\")\n","        raise\n","\n","def propagate_metadata_to_chunks(documents: List[Document], chunk_size=512, chunk_overlap=50) -> List[Document]:\n","    \"\"\"\n","    Splits documents into token-based chunks and propagates normalized metadata to each chunk.\n","\n","    Args:\n","        documents (list): List of documents with metadata.\n","        chunk_size (int): Maximum number of tokens per chunk.\n","        chunk_overlap (int): Number of overlapping tokens between chunks.\n","\n","    Returns:\n","        List[Document]: A list of document chunks with normalized metadata.\n","    \"\"\"\n","    # Token-based text splitter\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","\n","    # Load tokenizer for token validation\n","    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n","\n","    enriched_chunks = []\n","\n","    for doc_idx, doc in enumerate(documents):\n","        # Split document into token-based chunks\n","        chunks = text_splitter.split_documents([doc])\n","\n","        for chunk_idx, chunk in enumerate(chunks):\n","            # Propagate metadata to the chunk\n","            chunk.metadata.update(doc.metadata)\n","            chunk.metadata[\"chunk_id\"] = f\"{doc.metadata.get('source', 'unknown')}_{doc_idx}_{chunk_idx}\"\n","\n","            # Estimate token count and log\n","            token_count = len(tokenizer.encode(chunk.page_content, truncation=False))\n","            logger.debug(f\"Chunk {chunk_idx} from Document {doc_idx} has {token_count} tokens.\")\n","            if token_count > 1024:\n","                logger.warning(f\"Chunk {chunk_idx} exceeds token limit with {token_count} tokens.\")\n","\n","            enriched_chunks.append(chunk)\n","\n","    logger.info(f\"Generated {len(enriched_chunks)} enriched chunks.\")\n","    return enriched_chunks\n","\n","def validate_metadata(documents: List[Document], required_fields: Set[str] = None) -> List[Document]:\n","    \"\"\"\n","    Validates and standardizes metadata for a list of documents.\n","    \"\"\"\n","    if required_fields is None:\n","        required_fields = {\"year\", \"quarter\", \"bank\", \"designation\", \"name\", \"source\"}\n","\n","    validated_documents = []\n","    try:\n","        for doc in documents:\n","            if hasattr(doc, \"metadata\") and isinstance(doc.metadata, dict):\n","                for field in required_fields:\n","                    if field not in doc.metadata or not doc.metadata[field]:\n","                        doc.metadata[field] = \"Unknown\"\n","                validated_documents.append(doc)\n","        return validated_documents\n","    except Exception as e:\n","        logger.error(f\"Error in validate_metadata: {e}\")\n","        return documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Be5gcPgUHe3v"},"outputs":[],"source":["def initialize_embeddings_and_vector_store(\n","    chunks,\n","    embeddings_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n","    persist_directory=\"qa_index\",\n","    use_existing=True,\n","):\n","    \"\"\"\n","    Initializes embeddings and vector store.\n","\n","    Args:\n","        chunks (list): List of document chunks for embedding.\n","        embeddings_model_name (str): Hugging Face model to generate embeddings.\n","        persist_directory (str): Path to persist or load the vector store.\n","        use_existing (bool): If True, load an existing vector store if available.\n","\n","    Returns:\n","        Chroma: Initialized or loaded vector store instance.\n","    \"\"\"\n","    try:\n","        if not chunks:\n","            raise ValueError(\"No chunks provided for vector store initialization.\")\n","\n","        # Ensure the persist directory exists\n","        os.makedirs(persist_directory, exist_ok=True)\n","        logger.info(f\"Using embedding model: {embeddings_model_name}\")\n","\n","        # Initialize embeddings using the specified model\n","        embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n","\n","        # Check if an existing vector store should be used\n","        vector_store = None\n","        if use_existing and os.path.exists(persist_directory) and os.listdir(persist_directory):\n","            try:\n","                logger.info(f\"Loading existing vector store from: {persist_directory}\")\n","                vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n","\n","                # Check if the vector store contains any embeddings\n","                if vector_store._collection.count() == 0:\n","                    logger.warning(\"Existing vector store is empty. Rebuilding with provided chunks...\")\n","                    vector_store = Chroma.from_documents(\n","                        documents=chunks, embedding=embeddings, persist_directory=persist_directory\n","                    )\n","                    logger.info(f\"Rebuilt vector store with {len(chunks)} document chunks.\")\n","                else:\n","                    logger.info(f\"Loaded vector store with {vector_store._collection.count()} embeddings.\")\n","            except Exception as e:\n","                logger.warning(f\"Failed to load existing vector store: {e}. Rebuilding...\")\n","\n","        # If no vector store exists or is empty, create a new one\n","        if not vector_store:\n","            logger.info(f\"Creating a new vector store with {len(chunks)} document chunks.\")\n","            vector_store = Chroma.from_documents(\n","                documents=chunks, embedding=embeddings, persist_directory=persist_directory\n","            )\n","\n","        logger.info(\"Vector store initialized successfully.\")\n","        return vector_store\n","\n","    except Exception as e:\n","        logger.exception(\"Error initializing vector store.\")\n","        raise\n","\n","\n","def setup_retriever(vector_store: Chroma, top_k=3):\n","    \"\"\"\n","    Configures a retriever for the vector store with an embedding function.\n","\n","    Args:\n","        vector_store (Chroma): The vector database instance.\n","        top_k (int): Number of top results to retrieve.\n","\n","    Returns:\n","        Retriever: A retriever configured for the vector store.\n","    \"\"\"\n","    try:\n","        if not vector_store:\n","            raise ValueError(\"Vector store is empty or not initialized.\")\n","\n","        logger.info(f\"Setting up retriever with top_k={top_k}...\")\n","        retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n","\n","        logger.info(\"Retriever configured successfully.\")\n","        return retriever\n","\n","    except Exception as e:\n","        logger.exception(\"Error setting up retriever.\")\n","        raise\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsdAoVj8He3w"},"outputs":[],"source":["def load_model(\n","    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n","    max_new_tokens=300,\n","    device=\"cuda\",\n","    precision=\"float16\",\n","    trust_remote_code=True\n","):\n","    \"\"\"\n","    Loads a language model and sets up a text-generation pipeline.\n","\n","    Args:\n","        model_name (str): Name of the pre-trained model to load.\n","        max_new_tokens (int): Maximum number of tokens to generate per response.\n","        device (str): Device to load the model on (\"cuda\" or \"cpu\").\n","        precision (str): Precision type (\"float16\", \"float32\").\n","        trust_remote_code (bool): Trust remote code for custom models.\n","\n","    Returns:\n","        transformers.pipelines.Pipeline: A configured pipeline ready for text generation.\n","    \"\"\"\n","    # Set random seed for reproducibility\n","    torch.manual_seed(0)\n","\n","    try:\n","        # Validate max_new_tokens\n","        if not isinstance(max_new_tokens, int) or max_new_tokens <= 0 or max_new_tokens > 1024:\n","            raise ValueError(\"max_new_tokens must be a positive integer and less than or equal to 1024.\")\n","\n","        # Check device compatibility\n","        if device == \"cuda\" and not torch.cuda.is_available():\n","            logger.warning(\"CUDA is not available. Falling back to CPU.\")\n","            device = \"cpu\"\n","            precision = \"float32\"\n","\n","        # Determine the dtype based on precision and device\n","        dtype = torch.float16 if precision == \"float16\" and device == \"cuda\" else torch.float32\n","\n","        # Log initialization parameters\n","        logger.info(f\"Loading model '{model_name}' on {device} with precision '{precision}' and max_new_tokens={max_new_tokens}.\")\n","\n","        # Load the model and tokenizer\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name, torch_dtype=dtype, trust_remote_code=trust_remote_code\n","        ).to(device)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","        # Initialize the pipeline\n","        pipe = pipeline(\n","            \"text-generation\",\n","            model=model,\n","            tokenizer=tokenizer,\n","            device=0 if device == \"cuda\" else -1,\n","            max_new_tokens=max_new_tokens,\n","        )\n","\n","        # Log model details\n","        model_params = sum(p.numel() for p in model.parameters())\n","        logger.info(f\"Model '{model_name}' loaded successfully with {model_params:,} parameters.\")\n","\n","        return pipe\n","\n","    except Exception as e:\n","        logger.exception(f\"Failed to load model '{model_name}': {e}\")\n","        raise RuntimeError(f\"Failed to load model: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65dzuox5He3w"},"outputs":[],"source":["def load_and_preprocess_data(\n","    folder_path: str,\n","    metadata_path: str,\n","    chunk_size: int,\n","    chunk_overlap: int,\n","    required_metadata_fields: Set[str]\n",") -> List[Document]:\n","    \"\"\"\n","    Streamlines the loading, metadata attachment, validation, and chunking process.\n","\n","    Args:\n","        folder_path (str): Path to the folder containing documents.\n","        metadata_path (str): Path to the metadata CSV file.\n","        chunk_size (int): Maximum number of characters per chunk.\n","        chunk_overlap (int): Number of overlapping characters between chunks.\n","        required_metadata_fields (Set[str]): Required metadata fields for validation.\n","\n","    Returns:\n","        Tuple[List[Document], List[Document]]: Enriched documents and their chunks.\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting the preprocessing pipeline...\")\n","\n","        # Step 1: Load metadata from the provided CSV file\n","        logger.info(\"Step 1: Loading metadata...\")\n","        metadata_df = load_metadata(metadata_path)\n","        logger.info(f\"Metadata loaded successfully with {len(metadata_df)} rows.\")\n","\n","        # Step 2: Load PDF documents from the specified folder\n","        logger.info(\"Step 2: Loading documents from the folder...\")\n","        documents = load_documents(folder_path)\n","        if not documents:\n","            raise ValueError(\"No documents found in the specified folder.\")\n","        logger.info(f\"Loaded {len(documents)} documents from {folder_path}.\")\n","\n","        # Step 3: Attach metadata to each document\n","        logger.info(\"Step 3: Attaching metadata to documents...\")\n","        enriched_documents = attach_metadata_to_documents(documents, metadata_df)\n","        logger.info(f\"Metadata successfully attached to {len(enriched_documents)} documents.\")\n","\n","        # Step 4: Validate the metadata against required fields\n","        logger.info(\"Step 4: Validating metadata for enriched documents...\")\n","        validated_documents = validate_metadata(enriched_documents, required_fields=required_metadata_fields)\n","        logger.info(f\"Metadata validation completed for {len(validated_documents)} documents.\")\n","\n","        # Step 5: Split documents into token-based chunks and propagate metadata\n","        logger.info(\"Step 5: Splitting documents into chunks...\")\n","        document_chunks = propagate_metadata_to_chunks(\n","            validated_documents,\n","            chunk_size=chunk_size,\n","            chunk_overlap=chunk_overlap\n","        )\n","        logger.info(f\"Generated {len(document_chunks)} document chunks.\")\n","\n","        # Final Step: Log completion and return results\n","        logger.info(\"Preprocessing pipeline completed successfully.\")\n","        return enriched_documents, document_chunks\n","\n","    except FileNotFoundError as fnf_error:\n","        logger.error(f\"File not found: {fnf_error}\")\n","        raise  # Specific actionable error, such as missing folder or file.\n","    except Exception as e:\n","        logger.error(f\"An error occurred during preprocessing: {e}\")\n","        raise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_mJSQgNHe3x"},"outputs":[],"source":["def get_model_token_limit(model_name: str) -> int:\n","    \"\"\"\n","    Dynamically retrieves the token limit of a model based on its configuration.\n","\n","    Args:\n","        model_name (str): The name or path of the pre-trained model.\n","\n","    Returns:\n","        int: The maximum token limit of the model.\n","    \"\"\"\n","    try:\n","        # Load the model configuration\n","        model = AutoModelForCausalLM.from_pretrained(model_name)\n","        if hasattr(model.config, \"max_position_embeddings\"):\n","            return model.config.max_position_embeddings\n","        else:\n","            raise ValueError(f\"Model {model_name} does not specify 'max_position_embeddings'.\")\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to retrieve token limit for model '{model_name}': {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"4w4vpHnNHe3x"},"source":["### RAG Pipeline Implementation\n","\n","The Retrieval-Augmented Generation (RAG) pipeline is an advanced framework designed to enhance natural language processing by combining retrieval-based document search with generative language models. This approach provides highly accurate and context-aware responses to user queries by integrating the following components:\n","\n","- Document Retrieval\n","- Generative Language Model\n","- Metadata and Context Management\n","- Sentiment Analysis\n","- Summarization\n","- Topic Modeling\n","\n","**configure_rag_pipeline**: this function serves as the backbone for applications requiring accurate document retrieval, summarization, sentiment classification, and topic analysis, making it an all-in-one solution for retrieval-augmented generation tasks.\n","\n","Workflow\n","- Document Preprocessing: Load and enrich documents, then split them into manageable chunks.\n","- Embedding Generation: Convert document chunks into vector embeddings and store them in a ChromaDB vector store.\n","- Retriever Setup: Configure a retriever to fetch the most relevant chunks for user queries.\n","- Generative Responses: Use retrieved chunks to generate responses via a language model.\n","- Auxiliary Insights: Apply summarization, sentiment analysis, and topic modeling for deeper understanding of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poWR6iLtHe3x"},"outputs":[],"source":["def configure_rag_pipeline(\n","    folder_path: str,\n","    metadata_path: str,\n","    model_name: str = \"microsoft/Phi-3-mini-4k-instruct\",\n","    embeddings_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n","    model_token_limit: int = 4096,\n","    top_k: int = 3,\n","    max_new_tokens: int = 500,\n","    chunk_size: int = None,  # Dynamically calculated based on model token limit\n","    chunk_overlap: int = 200,\n","    required_metadata_fields: Set[str] = None,\n","    persist_directory: str = \"qa_index\",\n","    device: str = \"cuda\",\n",") -> dict:\n","    \"\"\"\n","    Configures a Retrieval-Augmented Generation (RAG) pipeline.\n","\n","    Args:\n","        folder_path (str): Path to the folder containing documents.\n","        metadata_path (str): Path to the metadata CSV file.\n","        model_name (str): Default language model to load.\n","        embeddings_model_name (str): The embeddings model to use for vector store creation.\n","        top_k (int): Number of top results to retrieve.\n","        max_new_tokens (int): Maximum number of tokens to generate.\n","        chunk_size (int): Maximum number of characters in each document chunk.\n","        chunk_overlap (int): Number of overlapping characters between chunks.\n","        required_metadata_fields (Set[str]): Set of required metadata fields for validation.\n","        persist_directory (str): Directory for the vector store.\n","        device (str): Device for model loading (e.g., \"cuda\" or \"cpu\").\n","\n","    Returns:\n","        dict: A dictionary containing configured pipeline components.\n","    \"\"\"\n","    try:\n","        # Dynamically fetch the model's token limit\n","        model_token_limit = get_model_token_limit(model_name)\n","        logger.info(f\"Model token limit for '{model_name}': {model_token_limit}\")\n","\n","        # Set default required metadata fields if not provided\n","        if required_metadata_fields is None:\n","            required_metadata_fields = {\"year\", \"quarter\", \"bank\", \"source\", \"designation\", \"name\"}\n","\n","        # Calculate dynamic chunk size if not specified\n","        if chunk_size is None:\n","            prompt_tokens = 200  # Reserve tokens for the prompt\n","            token_budget = model_token_limit - prompt_tokens - max_new_tokens\n","            chunk_size = min(2000, token_budget // 2)  # Use half the remaining tokens per chunk\n","            logger.info(\n","                f\"Dynamic chunk size calculated based on model_token_limit={model_token_limit}, \"\n","                f\"prompt_tokens={prompt_tokens}, max_new_tokens={max_new_tokens}: {chunk_size}\"\n","            )\n","\n","        # Step 1: Load and preprocess documents\n","        logger.info(\"Starting document preprocessing...\")\n","        enriched_documents, document_chunks = load_and_preprocess_data(\n","            folder_path=folder_path,\n","            metadata_path=metadata_path,\n","            chunk_size=chunk_size,\n","            chunk_overlap=chunk_overlap,\n","            required_metadata_fields=required_metadata_fields,\n","        )\n","        if not enriched_documents:\n","            logger.error(\"No documents were enriched. Check metadata and folder path.\")\n","            return None\n","\n","        if not document_chunks:\n","            logger.error(\"No document chunks generated. Check text splitting or preprocessing logic.\")\n","            return None\n","\n","        # Log enriched document metadata for verification\n","        logger.info(\"Enriched document metadata (preview):\")\n","        for i, doc in enumerate(enriched_documents[:3]):  # Preview the first 3 documents\n","            logger.info(f\"Document {i + 1}: {doc.metadata}\")\n","\n","        # Step 2: Initialize embeddings and vector store\n","        logger.info(\"Initializing embeddings and vector store...\")\n","        vector_store = initialize_embeddings_and_vector_store(\n","            chunks=document_chunks,\n","            embeddings_model_name=embeddings_model_name,\n","            persist_directory=persist_directory,\n","            use_existing=True,\n","        )\n","\n","        if not vector_store or vector_store._collection.count() == 0:\n","            logger.warning(\"Vector store is empty. Consider rebuilding with valid document chunks.\")\n","            return None\n","\n","        # Step 3: Set up retriever\n","        logger.info(\"Setting up retriever...\")\n","        retriever = setup_retriever(vector_store, top_k=top_k)\n","        if not retriever:\n","            logger.error(\"Failed to configure retriever. Exiting pipeline configuration.\")\n","            return None\n","\n","        # Step 4: Load the language model\n","        logger.info(\"Loading the language model...\")\n","        llm_pipeline = load_model(\n","            model_name=model_name,\n","            max_new_tokens=max_new_tokens,\n","            device=device,\n","        )\n","\n","        if not llm_pipeline:\n","            logger.error(\"Failed to load the language model. Exiting pipeline configuration.\")\n","            return None\n","\n","        # Step 5: Initialize QA chains\n","        logger.info(\"Initializing QA chains...\")\n","        qa_chains = initialize_chains(llm_pipeline, \"stuff\")\n","\n","        if not qa_chains:\n","            logger.error(\"Failed to initialize QA chains. Exiting pipeline configuration.\")\n","            return None\n","\n","        # Step 6: Load metadata for display and options\n","        logger.info(\"Loading metadata...\")\n","        metadata_df = pd.read_csv(metadata_path)\n","        if metadata_df.empty:\n","            logger.warning(\"Metadata file is empty. Ensure the metadata CSV is correctly populated.\")\n","        else:\n","            logger.info(f\"Loaded metadata with {len(metadata_df)} rows.\")\n","\n","        # Step 7: Set up the summarization pipeline\n","        logger.info(\"Initializing summarization pipeline...\")\n","        summarization_pipeline = hf_pipeline(\n","            \"summarization\",\n","            model=\"sshleifer/distilbart-cnn-6-6\",  # Or any suitable summarization model\n","            device=0 if device == \"cuda\" else -1  # Use GPU if available\n","        )\n","\n","        # Step 8: Set up the sentiment analysis pipeline\n","        logger.info(\"Initializing sentiment analysis pipeline...\")\n","        sentiment_pipeline = hf_pipeline(\n","            \"sentiment-analysis\",\n","            model=\"yiyanghkust/finbert-tone\",  # Example sentiment analysis model\n","            device=0 if device == \"cuda\" else -1  # Use GPU if available\n","        )\n","\n","        # Step 9: Initialize BERTopic for topic modeling\n","        logger.info(\"Initializing BERTopic model...\")\n","        topic_model = BERTopic(language=\"english\", verbose=True)\n","        logger.info(\"BERTopic model initialized successfully.\")\n","\n","        # Return all components in a dictionary\n","        logger.info(\"RAG pipeline configured successfully.\")\n","        return {\n","            \"retriever\": retriever,\n","            \"vector_store\": vector_store,\n","            \"llm_pipeline\": llm_pipeline,\n","            \"qa_chains\": qa_chains,\n","            \"metadata_df\": metadata_df,\n","            \"documents\": enriched_documents,\n","            \"summarization_pipeline\": summarization_pipeline,\n","            \"sentiment_pipeline\": sentiment_pipeline,\n","            \"topic_model\": topic_model,\n","        }\n","\n","    except Exception as e:\n","        logger.exception(f\"An unexpected error occurred during pipeline configuration: {e}\")\n","        return None\n"]},{"cell_type":"markdown","source":["This pipeline can be enhanced by integrating topics and sentiments extracted via sentiment-modelling pipeline and topic-modelling pipeline. The same data can be added to the context for better and accurate results."],"metadata":{"id":"DFcxxWjqDZr9"}},{"cell_type":"markdown","metadata":{"id":"Ghas6njjHe3y"},"source":["### RAG Pipeline and Other Helper Functions\n","\n","Purpose: General-purpose functions to support RAG workflows, such as extracting responses, displaying text, and dynamic user interactions.\n","**Functions:**\n","- select_model\n","- extract_answer\n","- initialize_chains\n","- display_wrapped_output\n","- display_title\n","- display_query_menu\n","- get_filters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaPPPassHe3y"},"outputs":[],"source":["def select_model(model_name=\"microsoft/Phi-3-mini-4k-instruct\"):\n","    \"\"\"\n","    Allows the user to select a language model dynamically.\n","\n","    Args:\n","        model_name (str): Default model name to use if the user skips selection.\n","\n","    Returns:\n","        str: Name of the selected model or the default model.\n","    \"\"\"\n","\n","    # Define supported models\n","    supported_models = {\n","        \"1\": \"microsoft/Phi-3-mini-4k-instruct\",\n","        \"2\": \"microsoft/Phi-3.5-MoE-instruct\",\n","        \"3\": \"gemini-1.5-flash-8b\",\n","        \"4\": \"openai/gpt-4\"\n","    }\n","\n","    # Display available models\n","    print(\"\\nAvailable Language Models:\")\n","    for key, model_name in supported_models.items():\n","        print(f\"{key}. {model_name}\")\n","\n","    default_model = \"microsoft/Phi-3-mini-4k-instruct\"\n","\n","    # Prompt user for selection\n","    while True:\n","        try:\n","            choice = input(\n","                f\"\\nSelect a model (1-{len(supported_models)}) or press Enter for default [{default_model}]: \"\n","            ).strip()\n","\n","            # Use default model if input is empty\n","            if not choice:\n","                logger.info(f\"Default model selected: {model_name}\")\n","                print(f\"[INFO] Using default model: {model_name}\")\n","                return model_name\n","\n","            # Validate and return the selected model\n","            if choice in supported_models:\n","                selected_model = supported_models[choice]\n","                logger.info(f\"User selected model: {selected_model}\")\n","                print(f\"[INFO] Selected model: {selected_model}\")\n","\n","                return selected_model\n","\n","            # Handle invalid input\n","            logger.warning(f\"Invalid choice entered: {choice}\")\n","            print(\"[ERROR] Invalid choice. Please select a valid option.\")\n","        except Exception as e:\n","            logger.error(f\"An error occurred during model selection: {e}\")\n","            print(\"[ERROR] An unexpected error occurred. Please try again.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KnTvZ13He3z"},"outputs":[],"source":["def extract_answer(response: dict, tag: str = \"<|assistant|>\") -> str:\n","    \"\"\"\n","    Extracts and formats the answer from the response.\n","\n","    Args:\n","        response (dict | str): The response dictionary or string from the QA chain.\n","        tag (str): The delimiter tag to locate the assistant's response.\n","\n","    Returns:\n","        str: The extracted answer, or a default message if not found.\n","    \"\"\"\n","    try:\n","        # Check if response is a string\n","        if isinstance(response, str):\n","            response_text = response\n","        elif isinstance(response, dict):\n","            response_text = response.get(\"output_text\", \"No response generated.\")\n","        else:\n","            raise ValueError(\"Response must be a dictionary or a string.\")\n","\n","        if tag in response_text:\n","            # Extract the text after the assistant tag\n","            answer = response_text.split(tag)[-1].strip()\n","        else:\n","            # Default to the entire response or an error message\n","            answer = response_text.strip()\n","        return answer\n","    except Exception as e:\n","        logger.error(f\"Error extracting answer: {e}\")\n","        return \"Error extracting the answer.\"\n"]},{"cell_type":"markdown","source":["## Prompt Template\n","\n","This function contains a dictionary of prompt templates that are integrated to the handlers to action the menu options."],"metadata":{"id":"EUxV-Ea-HuNY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0w9dNnDjHe3z"},"outputs":[],"source":["def initialize_chains(llm_pipeline, chain_type=\"stuff\"):\n","    \"\"\"\n","    Initializes QA chains using predefined prompt templates for different menu options.\n","\n","    Args:\n","        llm_pipeline: A Hugging Face pipeline wrapped for LangChain.\n","        chain_type (str): The chain type to use for initializing the QA chains.\n","\n","    Returns:\n","        dict: Dictionary of QA chains, keyed by chain purpose.\n","\n","    Raises:\n","        RuntimeError: If the chain initialization fails.\n","    \"\"\"\n","    try:\n","        # Wrap the LLM pipeline for LangChain\n","        llm = HuggingFacePipeline(pipeline=llm_pipeline)\n","\n","        # Define prompt templates\n","        prompt_templates = {\n","            \"generic_question\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Your expertise lies in analyzing financial institutions with a focus on European banks.\n","                You have 20+ years of experience in financial modeling and due diligence.\n","                Only use the provided context and metadata to answer the question. Avoid any assumptions or unsupported claims.\n","                If information is missing or incomplete, state it clearly and suggest what additional data would help.\n","\n","                Metadata:\n","                {metadata}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"metadata\", \"context\", \"question\"]\n","            ),\n","            \"compare_two_quarters_same_year\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Your goal is to identify key highlights, challenges, and strategic shifts between two quarters in same year.\n","                Use only the given context and metadata. If data is unavailable, state this explicitly.\n","\n","\n","                Based on the provided data:\n","                1. Summarize the key highlights of the company’s performance.\n","                2. Identify challenges or risks mentioned.\n","                3. Provide insights into future strategies or guidance.\n","\n","                Provide the output as:\n","                1. Key Highlights:\n","                  - [Performance overview]\n","                  - [Growth areas]\n","                2. Challenges/Risks:\n","                  - [Key risks]\n","                3. Future Strategies:\n","                  - [Summary of plans or initiatives]\n","\n","                Metadata:\n","                - Year: {year}\n","                - Quarter 1: {quarter1}\n","                - Quarter 2: {quarter2}\n","                - Bank: {bank}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"context\", \"question\", \"year\", \"quarter1\", \"quarter2\", \"bank\"]\n","            ),\n","            \"compare_two_quarters_diff_years\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Your goal is to identify key highlights, challenges, and strategic shifts across two quarters in different years.\n","                Use only the given context and metadata. If data is unavailable, state this explicitly.\n","\n","\n","                Based on the provided data:\n","                1. Summarize the key highlights of the company’s performance.\n","                2. Identify challenges or risks mentioned.\n","                3. Provide insights into future strategies or guidance.\n","\n","                Provide the output as:\n","                1. Key Highlights:\n","                  - [Performance overview]\n","                  - [Growth areas]\n","                2. Challenges/Risks:\n","                  - [Key risks]\n","                3. Future Strategies:\n","                  - [Summary of plans or initiatives]\n","\n","                Metadata:\n","                - Quarter 1: {quarter1} ({year1})\n","                - Quarter 2: {quarter2} ({year2})\n","                - Bank: {bank}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"context\", \"question\", \"year1\", \"year2\", \"quarter1\", \"quarter2\", \"bank\"]\n","            ),\n","            \"year_comparison\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Your goal is to identify key highlights, challenges, and strategic shifts across years.\n","                Use only the given context and metadata. If data is unavailable, state this explicitly.\n","\n","\n","                Based on the provided data:\n","                1. Summarize the key highlights of the company’s performance.\n","                2. Identify challenges or risks mentioned.\n","                3. Provide insights into future strategies or guidance.\n","\n","                Provide the output as:\n","                1. Key Highlights:\n","                  - [Performance overview]\n","                  - [Growth areas]\n","                2. Challenges/Risks:\n","                  - [Key risks]\n","                3. Future Strategies:\n","                  - [Summary of plans or initiatives]\n","\n","                Metadata:\n","                - Year 1: {year1}\n","                - Year 2: {year2}\n","                - Bank: {bank}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"context\", \"question\", \"year1\", \"year2\", \"bank\"]\n","            ),\n","            \"all_quarters_same_year\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Your goal is to identify key highlights, challenges, and strategic shifts across all quarters in a year.\n","                Use only the given context and metadata. If data is unavailable, state this explicitly.\n","\n","\n","                Based on the provided data:\n","                1. Summarize the key highlights of the company’s performance.\n","                2. Identify challenges or risks mentioned.\n","                3. Provide insights into future strategies or guidance.\n","\n","                Provide the output as:\n","                1. Key Highlights:\n","                  - [Performance overview]\n","                  - [Growth areas]\n","                2. Challenges/Risks:\n","                  - [Key risks]\n","                3. Future Strategies:\n","                  - [Summary of plans or initiatives]\n","\n","                Metadata:\n","                - Year: {year}\n","                - Bank: {bank}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"context\", \"question\", \"year\", \"bank\"]\n","            ),\n","            \"single_quarter_sentiment\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Determine and analyze sentiment for a single quarter.\n","                Provide insights into the sentiment, including whether it is positive, negative, or neutral, and why.\n","\n","                Metadata:\n","                - Year: {year}\n","                - Quarter: {quarter}\n","                - Bank: {bank}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"context\", \"question\", \"year\", \"quarter\", \"bank\"]\n","            ),\n","            \"year_sentiment_trends\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Determine and analyze sentiment changes over a year.\n","                Provide insights into the sentiment, including whether it is positive, negative, or neutral, and why.\n","                Identify shifts in sentiment across quarters and provide possible reasons.\n","\n","                Metadata:\n","                - Year: {year}\n","                - Bank: {bank}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"context\", \"question\", \"year\", \"bank\"]\n","            ),\n","            \"summarize_single_quarter\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Summarize the main points of a single quarter's earnings call data.\n","                Include highlights and key performance indicators.\n","\n","                Metadata:\n","                - Year: {year}\n","                - Quarter: {quarter}\n","                - Bank: {bank}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"context\", \"question\", \"year\", \"quarter\", \"bank\"]\n","            ),\n","            \"aggregated_summaries\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Summarize the financial performance and sentiment.\n","                Include key trends, highlights, and areas of concern.\n","\n","                Metadata:\n","                {metadata}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"metadata\", \"context\", \"question\"]\n","            ),\n","            \"trend_analysis\": PromptTemplate(\n","                template=\"\"\"<|system|>\n","                You are a senior investment analyst at a major hedge fund.\n","                Analyze trends in financial metrics over time.\n","                Highlight significant changes and provide actionable insights.\n","\n","                Metadata:\n","                {metadata}\n","\n","                Context:\n","                {context}\n","\n","                Question:\n","                {question}\n","\n","                <|assistant|>\"\"\",\n","                input_variables=[\"metadata\", \"context\", \"question\"]\n","            ),\n","        }\n","\n","        # Validate templates\n","        for key, template in prompt_templates.items():\n","            missing_vars = [var for var in template.input_variables if f\"{{{var}}}\" not in template.template]\n","            unused_vars = [\n","                var.split(\"}\")[0]\n","                for var in template.template.split(\"{\")\n","                if \"}\" in var and var.split(\"}\")[0] not in template.input_variables\n","            ]\n","\n","            if missing_vars:\n","                logger.warning(f\"Template '{key}' is missing variables: {missing_vars}\")\n","            if unused_vars:\n","                logger.warning(f\"Template '{key}' contains unused variables: {unused_vars}\")\n","            if not missing_vars and not unused_vars:\n","                logger.info(f\"Template '{key}' validated successfully.\")\n","\n","        # Validate chain_type\n","        valid_chain_types = [\"stuff\", \"map_reduce\", \"refine\"]\n","        if chain_type not in valid_chain_types:\n","            raise ValueError(f\"Invalid chain_type '{chain_type}'. Valid options are: {valid_chain_types}\")\n","\n","        # Create chains dynamically\n","        chains = {\n","            key: load_qa_chain(llm, chain_type=chain_type, prompt=prompt)\n","            for key, prompt in prompt_templates.items()\n","        }\n","\n","        logger.info(\"QA chains initialized successfully.\")\n","        return chains\n","\n","    except ValueError as ve:\n","        logger.error(f\"Validation error: {ve}\")\n","        raise RuntimeError(f\"Failed to initialize QA chains: {ve}\")\n","    except Exception as e:\n","        logger.exception(\"An unexpected error occurred while initializing QA chains.\")\n","        raise RuntimeError(f\"Failed to initialize QA chains: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAkW8I3bHe30"},"outputs":[],"source":["def get_filters():\n","    filters = {}\n","    filter_bank = input(\"Do you want to filter by a specific bank? (yes/no): \").strip().lower()\n","    if filter_bank == \"yes\":\n","        filters[\"bank\"] = input(\"Enter the bank name: \").strip().lower()\n","    year = input(\"Enter the year to filter (or press Enter to skip): \").strip()\n","    if year:\n","        filters[\"year\"] = year\n","    quarter = input(\"Enter the quarter to filter (e.g., Q1, or press Enter to skip): \").strip()\n","    if quarter:\n","        filters[\"quarter\"] = quarter\n","    return filters\n"]},{"cell_type":"markdown","metadata":{"id":"fAxYdyz-He30"},"source":["## Context Preparation\n","\n","Purpose: Functions to prepare text and metadata context dynamically based on token limits and requirements for answering queries.\n","\n","**Functions:**\n","- prepare_context_with_dynamic_chunking\n","- prepare_context_with_metadata\n","- prepare_context_with_limit\n","- filter_documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1cT2fHxHe31"},"outputs":[],"source":["def prepare_context_with_dynamic_chunking(\n","    documents: List[Document],\n","    summarization_pipeline: pipeline,\n","    tokenizer: AutoTokenizer,\n","    model_token_limit: int,\n","    prompt_tokens: int,\n","    max_new_tokens: int,\n","    bank: str\n",") -> str:\n","    \"\"\"\n","    Prepares the context by dynamically chunking documents and summarizing if necessary.\n","\n","    Args:\n","        documents (List[Document]): List of relevant documents.\n","        summarization_pipeline (pipeline): Summarization model pipeline.\n","        tokenizer (AutoTokenizer): Tokenizer for token estimation.\n","        model_token_limit (int): Maximum token limit for the model.\n","        prompt_tokens (int): Number of tokens reserved for the prompt.\n","        max_new_tokens (int): Number of tokens reserved for the model's response.\n","        bank (str): The bank name to include in the context.\n","\n","    Returns:\n","        str: The prepared context string.\n","    \"\"\"\n","    # Calculate the remaining token budget for context\n","    context_token_budget = model_token_limit - prompt_tokens - max_new_tokens\n","    logger.info(f\"Context token budget: {context_token_budget}\")\n","\n","    # Initialize context\n","    context = \"\"\n","    for doc in documents:\n","        try:\n","            # Tokenize the document content\n","            tokens = tokenizer.encode(doc.page_content, truncation=False)\n","            token_count = len(tokens)\n","\n","            # Check if the document fits within the budget\n","            if token_count <= context_token_budget:\n","                context += doc.page_content + \"\\n\"\n","                context_token_budget -= token_count\n","            else:\n","                # Summarize if the document exceeds the token budget\n","                logger.info(f\"Summarizing document with {token_count} tokens...\")\n","                summary = summarization_pipeline(\n","                    doc.page_content, max_length=200, min_length=50, do_sample=False\n","                )\n","                summary_text = summary[0][\"summary_text\"]\n","                summary_tokens = tokenizer.encode(summary_text, truncation=False)\n","\n","                if len(summary_tokens) <= context_token_budget:\n","                    context += summary_text + \"\\n\"\n","                    context_token_budget -= len(summary_tokens)\n","                else:\n","                    logger.warning(\"Summarized content still exceeds token budget. Skipping this document.\")\n","        except Exception as e:\n","            logger.error(f\"Error processing document for context: {e}\")\n","            continue\n","\n","    return context.strip()\n","\n","def prepare_context_with_metadata(documents, metadata_df, filters):\n","    filtered_metadata = metadata_df[\n","        (metadata_df[\"Bank\"].str.lower() == filters[\"bank\"].lower()) &\n","        (metadata_df[\"Year\"].str.lower() == filters[\"year\"].lower()) &\n","        (metadata_df[\"Quarter\"].str.lower() == filters[\"quarter\"].lower())\n","    ]\n","    metadata_context = \"\\n\".join(\n","        [f\"{row['Name']} ({row['Designation']}), {row['Bank']} {row['Quarter']} {row['Year']}\" for _, row in filtered_metadata.iterrows()]\n","    )\n","    return metadata_context\n","\n","\n","def prepare_context_with_limit(\n","    documents: List[Document],\n","    tokenizer,\n","    summarization_pipeline=None,\n","    token_limit: int = 4000,\n","    prompt_tokens: int = 200,\n","    max_new_tokens: int = 500\n",") -> str:\n","    \"\"\"\n","    Prepares the context for the LLM by including entire documents or summaries if they exceed the token limit.\n","\n","    Args:\n","        documents (List[Document]): List of Document objects.\n","        tokenizer: Tokenizer for token estimation.\n","        summarization_pipeline: Optional summarization pipeline for condensing long documents.\n","        token_limit (int): Maximum token limit for the model.\n","        prompt_tokens (int): Estimated token count for the prompt.\n","        max_new_tokens (int): Estimated token count for the model's response.\n","\n","    Returns:\n","        str: Prepared context string within the token budget.\n","    \"\"\"\n","    # Calculate the context token budget\n","    context_token_limit = token_limit - prompt_tokens - max_new_tokens\n","    logger.info(f\"Context token budget: {context_token_limit} tokens.\")\n","\n","    context = \"\"\n","\n","    for doc in documents:\n","        try:\n","            # Calculate token count for the document content\n","            doc_tokens = len(tokenizer.encode(doc.page_content, truncation=False))\n","            logger.info(f\"Document token count: {doc_tokens}\")\n","\n","            # If the document fits within the token budget, add it directly\n","            if doc_tokens <= context_token_limit:\n","                context += doc.page_content + \"\\n\"\n","                context_token_limit -= doc_tokens\n","            else:\n","                # Summarize if the document exceeds the token limit and summarization is available\n","                if summarization_pipeline:\n","                    logger.info(f\"Summarizing document with {doc_tokens} tokens...\")\n","                    summary = summarization_pipeline(\n","                        doc.page_content, max_length=200, min_length=50, do_sample=False\n","                    )\n","                    summary_text = summary[0][\"summary_text\"]\n","                    summary_tokens = len(tokenizer.encode(summary_text, truncation=False))\n","\n","                    if summary_tokens <= context_token_limit:\n","                        context += summary_text + \"\\n\"\n","                        context_token_limit -= summary_tokens\n","                    else:\n","                        logger.warning(\"Summarized content still exceeds token limit. Skipping document.\")\n","                else:\n","                    logger.warning(\"No summarization pipeline provided. Skipping document.\")\n","        except Exception as e:\n","            logger.error(f\"Error processing document for context: {e}\")\n","            continue\n","\n","\n","    return context.strip()\n","\n","def prepare_context_with_topics_and_sentiments(\n","    documents: List[Document],\n","    tokenizer,\n","    summarization_pipeline=None,\n","    sentiment_pipeline=None,\n","    lda_model=None,\n","    dictionary=None,\n","    bow_corpus=None,\n","    num_topics=3,\n","    token_limit: int = 4000,\n","    prompt_tokens: int = 200,\n","    max_new_tokens: int = 500\n","):\n","    \"\"\"\n","    This function is for future use to integrate LDA and sentiment analysis.\n","    Prepares the context for the LLM by including document content, summaries, LDA topics, and sentiment analysis\n","\n","    Args:\n","        documents (List[Document]): List of Document objects.\n","        tokenizer: Tokenizer for token estimation.\n","        summarization_pipeline: Optional summarization pipeline for condensing long documents.\n","        sentiment_pipeline: Hugging Face sentiment-analysis pipeline.\n","        lda_model (LdaModel): Trained LDA model for topic extraction.\n","        dictionary (Dictionary): Gensim dictionary for the corpus.\n","        bow_corpus (list): Bag-of-words representation of the corpus.\n","        num_topics (int): Number of topics to extract for each document.\n","        token_limit (int): Maximum token limit for the model.\n","        prompt_tokens (int): Estimated token count for the prompt.\n","        max_new_tokens (int): Estimated token count for the model's response.\n","\n","    Returns:\n","        str: Prepared context string within the token budget.\n","    \"\"\"\n","    context_token_limit = token_limit - prompt_tokens - max_new_tokens\n","    logger.info(f\"Context token budget: {context_token_limit} tokens.\")\n","\n","    context = \"\"\n","    topic_context = \"\"\n","    sentiment_context = \"\"\n","\n","    # Step 1: Extract topics using LDA\n","    if lda_model and dictionary and bow_corpus:\n","        logger.info(\"Extracting topics from LDA model...\")\n","        topics_for_documents = extract_topics_from_lda(lda_model, bow_corpus, dictionary, num_topics)\n","        topic_context = generate_topic_context(topics_for_documents)\n","\n","    # Step 2: Perform sentiment analysis\n","    if sentiment_pipeline:\n","        logger.info(\"Analyzing sentiments...\")\n","        sentiments = []\n","        for doc in documents:\n","            try:\n","                sentiment_result = sentiment_pipeline(doc.page_content)\n","                sentiment_label = sentiment_result[0]['label']\n","                sentiment_score = sentiment_result[0]['score']\n","                sentiments.append(f\"{doc.metadata.get('source', 'Unknown')}: {sentiment_label} (Confidence: {sentiment_score:.2f})\")\n","            except Exception as e:\n","                logger.error(f\"Error during sentiment analysis for document: {e}\")\n","        sentiment_context = \"\\n\".join(sentiments)\n","\n","    # Step 3: Add document content or summaries\n","    for doc in documents:\n","        try:\n","            # Tokenize document content\n","            doc_tokens = len(tokenizer.encode(doc.page_content, truncation=False))\n","            logger.info(f\"Document token count: {doc_tokens}\")\n","\n","            # Add document content if within token budget\n","            if doc_tokens <= context_token_limit:\n","                context += doc.page_content + \"\\n\"\n","                context_token_limit -= doc_tokens\n","            else:\n","                # Summarize if content exceeds token budget\n","                if summarization_pipeline:\n","                    logger.info(f\"Summarizing document with {doc_tokens} tokens...\")\n","                    summary = summarization_pipeline(\n","                        doc.page_content, max_length=200, min_length=50, do_sample=False\n","                    )\n","                    summary_text = summary[0][\"summary_text\"]\n","                    summary_tokens = len(tokenizer.encode(summary_text, truncation=False))\n","\n","                    if summary_tokens <= context_token_limit:\n","                        context += summary_text + \"\\n\"\n","                        context_token_limit -= summary_tokens\n","                    else:\n","                        logger.warning(\"Summarized content still exceeds token limit. Skipping document.\")\n","                else:\n","                    logger.warning(\"No summarization pipeline provided. Skipping document.\")\n","        except Exception as e:\n","            logger.error(f\"Error processing document for context: {e}\")\n","            continue\n","\n","    # Step 4: Append topic and sentiment context if space allows\n","    topic_tokens = len(tokenizer.encode(topic_context, truncation=False))\n","    if topic_tokens <= context_token_limit:\n","        context += \"\\n\" + topic_context\n","        context_token_limit -= topic_tokens\n","    else:\n","        logger.warning(\"Topic context exceeds token limit. Skipping topic context.\")\n","\n","    sentiment_tokens = len(tokenizer.encode(sentiment_context, truncation=False))\n","    if sentiment_tokens <= context_token_limit:\n","        context += \"\\n\" + sentiment_context\n","    else:\n","        logger.warning(\"Sentiment context exceeds token limit. Skipping sentiment context.\")\n","\n","    return context.strip()\n"]},{"cell_type":"markdown","source":["### Filter function\n","\n","This function filters number of documents based on metadata to prepare the context for the query."],"metadata":{"id":"Q64tNkjYE3sW"}},{"cell_type":"code","source":["\n","def filter_documents(documents: List[Document], bank: str = None, year: str = None, quarter: str = None, designation: str = None) -> List[Document]:\n","    \"\"\"\n","    Filters documents by bank, year, quarter, and optionally designation, ensuring metadata keys and values are normalized.\n","\n","    Args:\n","        documents (List[Document]): List of Document objects with metadata.\n","        bank (str, optional): Bank name to filter by.\n","        year (str, optional): Year to filter by.\n","        quarter (str, optional): Quarter to filter by (e.g., Q1, Q2).\n","        designation (str, optional): Designation to filter by (e.g., CFO). Default is None.\n","\n","    Returns:\n","        List[Document]: Filtered list of documents.\n","    \"\"\"\n","    try:\n","        logger.info(f\"Starting filtering with criteria bank: {bank}, year: {year}, quarter: {quarter}, designation: {designation}\")\n","\n","        filtered_docs = []\n","        for doc in documents:\n","            # Normalize metadata keys and values\n","            metadata = {key.lower(): str(value).strip().lower() for key, value in doc.metadata.items()}\n","            logger.debug(f\"Document Metadata: {metadata}\")\n","\n","            # Extract metadata fields for filtering\n","            doc_bank = metadata.get(\"bank\", \"\")\n","            doc_year = metadata.get(\"year\", \"\")\n","            doc_quarter = metadata.get(\"quarter\", \"\")\n","            doc_designation = metadata.get(\"designation\", \"\")\n","\n","            # Apply strict matching criteria\n","            bank_match = not bank or doc_bank == bank.strip().lower()\n","            year_match = not year or doc_year == str(year).strip().lower()\n","            quarter_match = not quarter or doc_quarter == quarter.strip().lower()\n","            designation_match = not designation or doc_designation == designation.strip().lower()\n","\n","            # Log the match results for debugging\n","            logger.debug(f\"Bank Match: {bank_match}, Year Match: {year_match}, Quarter Match: {quarter_match}, Designation Match: {designation_match}\")\n","\n","            if bank_match and year_match and quarter_match and designation_match:\n","                filtered_docs.append(doc)\n","\n","        logger.info(f\"Filtered {len(filtered_docs)} documents for Bank: {bank}, Year: {year}, Quarter: {quarter}, Designation: {designation}\")\n","        return filtered_docs\n","\n","    except Exception as e:\n","        logger.error(f\"Error filtering documents: {e}\")\n","        raise"],"metadata":{"id":"GhMq9sV7E24W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwjASjqtHe31"},"source":["## RAG Query Handling\n","\n","Purpose: These functions handle user queries by retrieving relevant documents, preparing context, and invoking the appropriate QA chain.\n","\n","**Functions:**\n","\n","- handle_generic_query\n","- handle_compare_quarters_same_year\n","- handle_compare_quarters_diff_years\n","- handle_year_comparison\n","- handle_sentiment_single_quarter\n","- handle_summarize_single_quarter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QawBBzPZHe32"},"outputs":[],"source":["def handle_generic_query(pipeline, tokenizer, summarization_pipeline, model_token_limit, filters, query):\n","    \"\"\"\n","    Handles generic queries with optional filters and retrieves relevant documents.\n","    \"\"\"\n","    logger.info(\"Handling a generic query.\")\n","\n","    try:\n","        # Step 1: Apply filters if provided\n","        logger.info(\"Retrieving relevant documents with filters...\")\n","        relevant_docs = pipeline[\"retriever\"].get_relevant_documents(query)\n","        if filters.get(\"bank\"):\n","            relevant_docs = [\n","                doc for doc in relevant_docs if doc.metadata.get(\"bank\", \"\").lower() == filters[\"bank\"].lower()\n","            ]\n","\n","        if not relevant_docs:\n","            logger.warning(\"No documents found matching the query criteria.\")\n","            return \"[ERROR] No documents found for the query.\"\n","\n","        logger.info(f\"Filtered {len(relevant_docs)} documents for the generic query.\")\n","\n","        # Step 2: Prepare context\n","        logger.info(\"Preparing context for the query...\")\n","        context = prepare_context_with_limit(\n","            documents=relevant_docs,\n","            tokenizer=tokenizer,\n","            summarization_pipeline=summarization_pipeline,\n","            token_limit=model_token_limit,\n","            prompt_tokens=200,\n","            max_new_tokens=500,\n","        )\n","\n","        if not context.strip():\n","            logger.error(\"No relevant context could be prepared within the token limit.\")\n","            return \"[ERROR] Unable to prepare context.\"\n","\n","        # Step 3: Ensure relevant_docs are in Document format\n","        input_documents = [\n","            doc if isinstance(doc, Document) else Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"])\n","            for doc in relevant_docs\n","        ]\n","\n","        # Step 4: Invoke the QA chain\n","        logger.info(\"Invoking the QA chain...\")\n","        response = pipeline[\"qa_chains\"][\"generic_question\"].invoke(\n","            {\n","                \"input_documents\": input_documents,\n","                \"metadata\": f\"Filters: {filters}\",\n","                \"context\": context,\n","                \"question\": query,\n","            },\n","            repetition_penalty=1.2,\n","            no_repeat_ngram_size=3,\n","        )\n","\n","        # Extract and return the response\n","        return extract_answer(response)\n","\n","    except Exception as e:\n","        logger.exception(f\"Error handling the generic query: {e}\")\n","        return \"[ERROR] An unexpected error occurred while handling the query.\"\n","\n","\n","def handle_compare_quarters_same_year(pipeline, tokenizer, model_token_limit, year, quarter1, quarter2, bank, query):\n","    logger.info(f\"Comparing {quarter1} and {quarter2} in {year} for {bank}.\")\n","    try:\n","        # Retrieve documents for Q1\n","        filters_q1 = {\"year\": year, \"quarter\": quarter1, \"bank\": bank.lower()}\n","        relevant_docs_q1 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters_q1)\n","\n","        # Retrieve documents for Q2\n","        filters_q2 = {\"year\": year, \"quarter\": quarter2, \"bank\": bank.lower()}\n","        relevant_docs_q2 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters_q2)\n","\n","        # Combine documents\n","        input_documents = relevant_docs_q1 + relevant_docs_q2\n","\n","        if not input_documents:\n","            logger.warning(\"No relevant documents found for the comparison.\")\n","            return \"[ERROR] No relevant documents found for the comparison.\"\n","\n","        # Prepare metadata and invoke QA chain\n","        metadata = f\"Year: {year}, Bank: {bank}, Quarter 1: {quarter1}, Quarter 2: {quarter2}\"\n","        response = pipeline[\"qa_chains\"][\"compare_two_quarters_same_year\"].invoke(\n","            {\n","                \"input_documents\": input_documents,\n","                \"metadata\": metadata,\n","                \"context\": \"N/A\",\n","                \"question\": query,\n","                \"year\": year,\n","                \"quarter1\": quarter1,\n","                \"quarter2\": quarter2,\n","                \"bank\": bank,\n","            }\n","        )\n","        return extract_answer(response)\n","\n","    except Exception as e:\n","        logger.exception(f\"Error comparing two quarters in the same year: {e}\")\n","        return \"[ERROR] An unexpected error occurred.\"\n","\n","\n","\n","def handle_compare_quarters_diff_years(pipeline, tokenizer, model_token_limit, year1, year2, quarter1, quarter2, bank, query):\n","    \"\"\"\n","    Handles comparison of two quarters across different years.\n","    \"\"\"\n","    logger.info(f\"Comparing {quarter1} ({year1}) and {quarter2} ({year2}) for {bank}.\")\n","\n","    try:\n","        # Retrieve documents for Quarter 1 of Year 1\n","        filters = {\"year\": year1, \"quarter\": quarter1, \"bank\": bank.lower()}\n","        logger.info(f\"Retrieving documents for {quarter1} {year1}...\")\n","        relevant_docs_q1 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n","\n","        # Retrieve documents for Quarter 2 of Year 2\n","        filters[\"year\"] = year2\n","        filters[\"quarter\"] = quarter2\n","        logger.info(f\"Retrieving documents for {quarter2} {year2}...\")\n","        relevant_docs_q2 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n","\n","        # Combine documents from both quarters\n","        input_documents = relevant_docs_q1 + relevant_docs_q2\n","\n","        if not input_documents:\n","            logger.warning(\"No relevant documents found for the comparison.\")\n","            return \"[ERROR] No relevant documents found for the comparison.\"\n","\n","        # Prepare metadata\n","        metadata = f\"Bank: {bank}, Quarter 1: {quarter1} ({year1}), Quarter 2: {quarter2} ({year2})\"\n","\n","        # Invoke the QA chain\n","        response = pipeline[\"qa_chains\"][\"compare_two_quarters_diff_years\"].invoke(\n","            {\n","                \"input_documents\": input_documents,\n","                \"metadata\": metadata,\n","                \"context\": \"N/A\",\n","                \"question\": query,\n","                \"year1\": year1,\n","                \"year2\": year2,\n","                \"quarter1\": quarter1,\n","                \"quarter2\": quarter2,\n","                \"bank\": bank,\n","            }\n","        )\n","\n","        # Extract and return the response\n","        return extract_answer(response)\n","\n","    except Exception as e:\n","        logger.exception(f\"Error comparing two quarters across different years: {e}\")\n","        return \"[ERROR] An unexpected error occurred.\"\n","\n","\n","\n","def handle_year_comparison(pipeline, tokenizer, model_token_limit, year1, year2, bank, query):\n","    \"\"\"\n","    Handles year-over-year comparison.\n","    \"\"\"\n","    logger.info(f\"Comparing performance for {year1} and {year2} for {bank}.\")\n","\n","    try:\n","        # Retrieve documents for Year 1\n","        filters = {\"year\": year1, \"bank\": bank.lower()}\n","        logger.info(f\"Retrieving documents for {year1}...\")\n","        relevant_docs_y1 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n","\n","        # Retrieve documents for Year 2\n","        filters[\"year\"] = year2\n","        logger.info(f\"Retrieving documents for {year2}...\")\n","        relevant_docs_y2 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n","\n","        # Combine documents from both years\n","        input_documents = relevant_docs_y1 + relevant_docs_y2\n","\n","        if not input_documents:\n","            logger.warning(\"No relevant documents found for the comparison.\")\n","            return \"[ERROR] No relevant documents found for the comparison.\"\n","\n","        # Prepare metadata\n","        metadata = f\"Year 1: {year1}, Year 2: {year2}, Bank: {bank}\"\n","\n","        # Invoke the QA chain\n","        response = pipeline[\"qa_chains\"][\"year_comparison\"].invoke(\n","            {\n","                \"input_documents\": input_documents,\n","                \"metadata\": metadata,\n","                \"context\": \"N/A\",\n","                \"question\": query,\n","                \"year1\": year1,\n","                \"year2\": year2,\n","                \"bank\": bank,\n","            }\n","        )\n","\n","        # Extract and return the response\n","        return extract_answer(response)\n","\n","    except Exception as e:\n","        logger.exception(f\"Error performing year-over-year comparison: {e}\")\n","        return \"[ERROR] An unexpected error occurred.\"\n","\n","\n","def handle_sentiment_single_quarter(pipeline, tokenizer, model_token_limit, year, quarter, bank, query):\n","    \"\"\"\n","    Handles sentiment analysis for a single quarter.\n","    \"\"\"\n","    logger.info(f\"Analyzing sentiment for {bank} in {quarter} {year}.\")\n","\n","    try:\n","        # Retrieve documents for the given quarter\n","        filters = {\"year\": year, \"quarter\": quarter, \"bank\": bank.lower()}\n","        logger.info(f\"Retrieving documents for {quarter} {year}...\")\n","        input_documents = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n","\n","        if not input_documents:\n","            logger.warning(\"No relevant documents found for sentiment analysis.\")\n","            return \"[ERROR] No relevant documents found for sentiment analysis.\"\n","\n","        # Prepare metadata\n","        metadata = f\"Year: {year}, Quarter: {quarter}, Bank: {bank}\"\n","\n","        # Invoke the QA chain\n","        response = pipeline[\"qa_chains\"][\"single_quarter_sentiment\"].invoke(\n","            {\n","                \"input_documents\": input_documents,\n","                \"metadata\": metadata,\n","                \"context\": \"N/A\",\n","                \"question\": query,\n","                \"year\": year,\n","                \"quarter\": quarter,\n","                \"bank\": bank,\n","            }\n","        )\n","\n","        # Extract and return the response\n","        return extract_answer(response)\n","\n","    except Exception as e:\n","        logger.exception(f\"Error analyzing sentiment for a single quarter: {e}\")\n","        return \"[ERROR] An unexpected error occurred.\"\n","\n","\n","def handle_summarize_single_quarter(pipeline, tokenizer, model_token_limit, year, quarter, bank, query):\n","    \"\"\"\n","    Handles summarization for a single quarter.\n","    \"\"\"\n","    logger.info(f\"Summarizing performance for {bank} in {quarter} {year}.\")\n","\n","    try:\n","        # Retrieve documents for the given quarter\n","        filters = {\"year\": year, \"quarter\": quarter, \"bank\": bank.lower()}\n","        logger.info(f\"Retrieving documents for {quarter} {year}...\")\n","        input_documents = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n","\n","        if not input_documents:\n","            logger.warning(\"No relevant documents found for summarization.\")\n","            return \"[ERROR] No relevant documents found for summarization.\"\n","\n","        # Prepare metadata\n","        metadata = f\"Year: {year}, Quarter: {quarter}, Bank: {bank}\"\n","\n","        # Invoke the QA chain\n","        response = pipeline[\"qa_chains\"][\"summarize_single_quarter\"].invoke(\n","            {\n","                \"input_documents\": input_documents,\n","                \"metadata\": metadata,\n","                \"context\": \"N/A\",\n","                \"question\": query,\n","                \"year\": year,\n","                \"quarter\": quarter,\n","                \"bank\": bank,\n","            }\n","        )\n","\n","        # Extract and return the response\n","        return extract_answer(response)\n","\n","    except Exception as e:\n","        logger.exception(f\"Error summarizing the single quarter: {e}\")\n","        return \"[ERROR] An unexpected error occurred while summarizing the quarter.\"\n","\n","\n","# ==============================Topic Modeling=======================================\n","\n","def handle_bertopic(documents, nlp_model=None):\n","    \"\"\"\n","    Perform BERTopic modeling on earnings call transcripts.\n","\n","    Args:\n","        documents (List[Document]): A list of filtered documents to perform topic modeling on.\n","        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n","    \"\"\"\n","    logger.info(\"Starting BERTopic modeling for earnings call transcripts...\")\n","\n","    try:\n","        # Combine all document content into a single corpus\n","        corpus = [doc.page_content for doc in documents if doc.page_content]\n","\n","        if not corpus:\n","            logger.warning(\"No valid content found in documents for BERTopic.\")\n","            print(\"[ERROR] No valid content to analyze with BERTopic.\")\n","            return\n","\n","        # Preprocess the corpus for earnings call transcripts\n","        logger.info(\"Preprocessing the corpus for earnings calls...\")\n","        redundant_terms = [\n","            \"credit suisse\", \"q1\", \"q2\", \"q3\", \"q4\",\n","            \"first quarter\", \"second quarter\", \"third quarter\", \"fourth quarter\",\n","            \"earnings call\", \"company\", \"presentation\", \"analyst\", \"operator\", \"seeking_alpha\"\n","        ]\n","        cleaned_corpus = preprocess_corpus_bertopic(\n","            corpus, nlp_model, custom_stopwords=redundant_terms, redundant_terms=redundant_terms\n","        )\n","\n","        # Custom vectorizer for bigrams and trigrams with stopwords\n","        vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words=\"english\")\n","\n","        # Initialize BERTopic model with custom vectorizer\n","        topic_model = BERTopic(vectorizer_model=vectorizer)\n","\n","        # Fit the model on the cleaned corpus\n","        logger.info(\"Fitting BERTopic model to the cleaned corpus...\")\n","        topics, probs = topic_model.fit_transform(cleaned_corpus)\n","\n","        # Check if any topics were identified\n","        topic_info = topic_model.get_topic_info()\n","        if topic_info.empty or topic_info[\"Count\"].sum() == 0:\n","            logger.warning(\"No meaningful topics were identified by BERTopic.\")\n","            print(\"[ERROR] BERTopic did not identify any meaningful topics. Check the input data.\")\n","            return\n","\n","        # Diagnostics: Display document lengths and topic info\n","        logger.info(\"Diagnostics:\")\n","        doc_lengths = [len(doc.split()) for doc in cleaned_corpus]\n","        logger.info(f\"Average document length: {sum(doc_lengths) / len(doc_lengths):.2f}\")\n","        logger.info(f\"Total number of documents: {len(cleaned_corpus)}\")\n","        logger.info(f\"Topic Info: \\n{topic_info.head(6)}\")\n","\n","        # Display top topics\n","        logger.info(\"Displaying top topics for earnings calls...\")\n","        print(\"\\nTop Topics Identified by BERTopic:\")\n","        print(topic_info.head(6))  # Display the top 6 topics for readability\n","\n","        # Try visualizing topics if valid topics exist\n","        try:\n","            fig = topic_model.visualize_barchart(top_n_topics=6)\n","            fig.show()\n","        except Exception as e:\n","            logger.warning(f\"Visualization failed: {e}\")\n","            print(\"[WARNING] Unable to generate visualization. Skipping...\")\n","\n","        # Save the model for later use\n","        topic_model.save(\"bertopic_model_earnings_calls\")\n","        logger.info(\"BERTopic model saved successfully.\")\n","\n","        print(\"[INFO] BERTopic modeling for earnings calls completed successfully.\")\n","    except Exception as e:\n","        logger.exception(f\"Error during BERTopic modeling: {e}\")\n","        print(f\"[ERROR] An error occurred during BERTopic modeling: {e}\")\n","\n","\n","def handle_gensim_lda(documents, nlp_model=None, num_topics=3, extra_redundant_terms=None):\n","    \"\"\"\n","    Perform LDA topic modeling on earnings call transcripts using Gensim, focusing on bigrams and trigrams.\n","\n","    Args:\n","        documents (List[Document]): A list of filtered documents with a `page_content` attribute.\n","        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n","        num_topics (int): Number of topics to extract.\n","        extra_redundant_terms (list): Additional terms to remove during preprocessing.\n","\n","    Returns:\n","        LdaModel, Dictionary, List: Trained LDA model, dictionary, and bow_corpus.\n","    \"\"\"\n","    logger.info(\"Starting Gensim LDA modeling for earnings call transcripts...\")\n","\n","    try:\n","        # Combine all document content into a single corpus\n","        corpus = [doc.page_content for doc in documents if doc.page_content]\n","\n","        if not corpus:\n","            logger.warning(\"No valid content found in documents for Gensim LDA.\")\n","            print(\"[ERROR] No valid content to analyze with Gensim LDA.\")\n","            return\n","\n","        # Preprocess the corpus for earnings call transcripts\n","        logger.info(\"Preprocessing the corpus for earnings calls...\")\n","        redundant_terms = [\n","            \"credit suisse\", \"q1\", \"q2\", \"q3\", \"q4\",\n","            \"first quarter\", \"second quarter\", \"third quarter\", \"fourth quarter\",\n","            \"earnings call\", \"company\", \"presentation\", \"analyst\", \"operator\", \"thomas\", \"gottstein\", \"transcript_seeking alpha\", \"question\", \"next slide\", \"thank you\",\n","            \"take question\", \"transcript seeking alpha\", \"group ag cs results\", \"morning\", \"Transcript _ Seeking Alpha\", \"Group AG\", \"Results\"\n","        ]\n","\n","        # Add any extra redundant terms\n","        if extra_redundant_terms:\n","            redundant_terms.extend(extra_redundant_terms)\n","\n","        processed_corpus = preprocess_corpus_gensim(\n","            corpus,\n","            nlp_model,\n","            custom_stopwords=redundant_terms,\n","            redundant_terms=redundant_terms,\n","        )\n","\n","        # Create a dictionary and a bag-of-words representation of the corpus\n","        logger.info(\"Creating dictionary and bag-of-words representation...\")\n","        dictionary = Dictionary(processed_corpus)\n","        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n","\n","        # Log dictionary size\n","        logger.info(f\"Vocabulary size after preprocessing: {len(dictionary)}\")\n","\n","        # Train LDA model\n","        logger.info(f\"Training Gensim LDA model with {num_topics} topics...\")\n","        lda_model = LdaModel(\n","            corpus=bow_corpus,\n","            id2word=dictionary,\n","            num_topics=num_topics,\n","            random_state=42,\n","            passes=10,\n","            iterations=50,\n","        )\n","\n","        # Display topics\n","        logger.info(\"Displaying top topics for earnings calls...\")\n","        print(\"\\nTop Topics Identified by Gensim LDA:\")\n","        for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\n","            print(f\"Topic #{idx + 1}: {topic}\")\n","\n","        # Visualize topics in Colab Notebook\n","        try:\n","            logger.info(\"Preparing visualization for LDA topics...\")\n","            vis_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n","\n","            # Check if running in a Colab or IPython environment\n","            from IPython import get_ipython\n","            if \"google.colab\" in str(get_ipython()):  # Specifically for Google Colab\n","                from google.colab import output\n","                pyLDAvis.enable_notebook()\n","                display(vis_data)\n","            elif \"IPython.core.interactiveshell\" in str(type(get_ipython())):  # Jupyter notebook\n","                pyLDAvis.enable_notebook()\n","                display(vis_data)\n","            else:\n","                logger.warning(\"Visualization not supported in this environment. Saving as HTML.\")\n","                pyLDAvis.save_html(vis_data, \"gensim_lda_topics.html\")\n","\n","            logger.info(\"LDA visualization successfully displayed or saved.\")\n","        except ImportError as e:\n","            logger.warning(f\"pyLDAvis not installed or visualization failed: {e}\")\n","            print(\"[WARNING] Unable to generate visualization. Skipping...\")\n","        except Exception as e:\n","            logger.exception(f\"Unexpected error during visualization: {e}\")\n","            print(\"[ERROR] Unable to display visualization.\")\n","\n","        print(\"[INFO] Gensim LDA modeling for earnings calls completed successfully.\")\n","        return lda_model, dictionary, bow_corpus\n","\n","    except Exception as e:\n","        logger.exception(f\"Error during Gensim LDA modeling: {e}\")\n","        print(f\"[ERROR] An error occurred during Gensim LDA modeling: {e}\")\n","\n"]},{"cell_type":"markdown","source":["## Display Functions"],"metadata":{"id":"g7Hdy1qDF_Z3"}},{"cell_type":"code","source":["import textwrap\n","\n","def display_wrapped_output(output_text, width=50):\n","    \"\"\"\n","    Displays the output text wrapped to fit within the specified width.\n","\n","    Args:\n","        output_text (str): The text to display.\n","        width (int): The maximum width of each line.\n","    \"\"\"\n","    wrapper = textwrap.TextWrapper(width=width)\n","    wrapped_text = wrapper.fill(output_text)\n","    print(\"[RESULT] Response from the AI Assistant:\")\n","    print(wrapped_text)\n","\n","def display_title():\n","    print(\"\\n\\n\" + \"=\" * 70)\n","    print(\"   Welcome to the Earning Call Transcript-Based Risk Analyzer\")\n","    print(\"             An Early Warning System for Investors\")\n","    print(\"=\" * 70)\n","\n","\n","def display_query_menu():\n","    \"\"\"\n","    Displays a menu for selecting the query type and returns the user's choice.\n","\n","    Returns:\n","        int: The selected numeric choice, or None if the input is invalid.\n","    \"\"\"\n","    # Define the menu options\n","    menu_options = {\n","        \"1\": \"Ask a generic question\",\n","        \"2\": \"Compare two quarters of the same year\",\n","        \"3\": \"Compare two quarters of different years\",\n","        \"4\": \"Year-over-year comparison\",\n","        \"5\": \"Analyze all quarters of a year\",\n","        \"6\": \"Analyze sentiment for a single quarter\",\n","        \"7\": \"Summarize a single quarter\",\n","        \"8\": \"Perform Topic Modeling with BERTopic\",\n","        \"9\": \"Perform Topic Modeling with Gensim LDA\",\n","        \"10\": \"Exit\"\n","    }\n","\n","    # Display the menu\n","    print(\"\\n\" + \"=\" * 50)\n","    print(\"   Query Selection Menu\")\n","    print(\"=\" * 50)\n","    for key, value in menu_options.items():\n","        print(f\"{key}. {value}\")\n","    print(\"=\" * 50)\n","\n","    # Get the user's choice\n","    choice = input(\"Select an option (1-10): \").strip()\n","\n","    # Validate the choice and return as an integer\n","    if choice in menu_options:\n","        return int(choice)\n","    else:\n","        print(\"[ERROR] Invalid choice. Please enter a number between 1 and 10.\")\n","        return None\n","\n","def remove_repetitions(text):\n","    sentences = text.split('. ')\n","    seen = set()\n","    unique_sentences = []\n","    for sentence in sentences:\n","        if sentence not in seen:\n","            unique_sentences.append(sentence)\n","            seen.add(sentence)\n","    return '. '.join(unique_sentences)"],"metadata":{"id":"jy8NIeTRJLFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 'main' function\n","\n","**Flow of main function**\n","\n","1. **Initialize Logging**\n","   - Start the logger to track the application's workflow.\n","\n","2. **Initialize Pipeline Configuration**\n","   - **Set File Paths**: Specify paths for document folder and metadata CSV.\n","   - **Select Model**: Prompt user to select a language model.\n","   - **Set Token Limits**: Retrieve the token limit for the selected model.\n","   - **Configure Pipeline**: Call `configure_rag_pipeline` to set up the RAG pipeline with:\n","     - Document preprocessing\n","     - Embedding initialization\n","     - Vector store setup\n","     - Summarization and sentiment analysis pipelines\n","     - LDA topic modeling\n","   - **Setup Retriever**: Add a retriever to the pipeline for retrieving relevant documents.\n","   - **Load Tokenizer**: Initialize tokenizer for the selected model.\n","\n","3. **Define Helper Functions**\n","   - **`get_filters`**: Collect filter criteria for queries (e.g., bank, year, quarter).\n","\n","4. **Interactive Query Interface**\n","   - Display a menu for users to choose an analysis type:\n","     1. **Ask a Generic Question**: Retrieve and analyze documents based on a user-provided question and filters.\n","     2. **Compare Quarters (Same Year)**: Compare performance between two quarters in the same year.\n","     3. **Compare Quarters (Different Years)**: Compare performance across two years for specific quarters.\n","     4. **Year-over-Year Comparison**: Analyze year-on-year performance for a specific bank.\n","     5. **Analyze All Quarters in a Year**: Summarize and analyze performance for all quarters in a year.\n","     6. **Analyze Sentiment for a Quarter**: Perform sentiment analysis for a specific quarter and bank.\n","     7. **Summarize a Quarter**: Summarize financial performance for a given quarter and bank.\n","     8. **Topic Modeling (BERTopic)**: Perform topic modeling on filtered documents using BERTopic.\n","     9. **Topic Modeling (Gensim LDA)**: Perform topic modeling on filtered documents using Gensim LDA.\n","    10. **Exit**: Exit the application.\n","\n","5. **Process User Selection**\n","   - For each choice:\n","     - Prompt user for relevant inputs (e.g., year, quarter, bank).\n","     - Call the corresponding handler function (e.g., `handle_generic_query`, `handle_sentiment_single_quarter`).\n","     - Display results using `display_wrapped_output`.\n","\n","6. **Error Handling**\n","   - Log and display errors encountered during pipeline configuration or query handling.\n","\n","7. **Exit Application**\n","   - Exit gracefully when the user selects the exit option."],"metadata":{"id":"bKqc59WMGnys"}},{"cell_type":"code","source":["def main():\n","    \"\"\"\n","    Main function to configure the RAG pipeline and start the interactive user interface.\n","    \"\"\"\n","    logger = logging.getLogger(\"RAGApplication\")\n","    logger.info(\"Starting Transcript-Based Risk Analyzer\")\n","\n","    # Step 1: Initialize pipeline, tokenizer, etc.\n","    try:\n","        folder_path = \"/content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse\"\n","        metadata_path = \"/content/drive/MyDrive/Colab Notebooks/Employer_Project/data/metadata/company_participants.csv\"\n","\n","        display_title()\n","        # Select model\n","        print(\"\\nStep 1: Select a Language Model\")\n","        selected_model = select_model()\n","        model_token_limit = get_model_token_limit(selected_model)\n","\n","        # Configure pipeline\n","        print(\"\\n[INFO] Configuring the RAG pipeline...\")\n","        pipeline = configure_rag_pipeline(\n","            folder_path=folder_path,\n","            metadata_path=metadata_path,\n","            model_name=selected_model,\n","            embeddings_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n","            model_token_limit=model_token_limit,\n","            top_k=3,\n","            max_new_tokens=500,\n","            chunk_size=None,\n","            chunk_overlap=200,\n","            required_metadata_fields={\"year\", \"quarter\", \"bank\", \"source\", \"designation\", \"name\"},\n","            persist_directory=\"qa_index\",\n","            device=\"cuda\",\n","        )\n","\n","        if pipeline is None:\n","            raise ValueError(\"Pipeline configuration returned no results.\")\n","\n","        # Configure retriever\n","        retriever = setup_retriever(pipeline[\"vector_store\"], top_k=3)\n","        pipeline[\"retriever\"] = retriever  # Add retriever to pipeline for later use\n","\n","        logger.info(\"Pipeline configured successfully.\")\n","        print(\"\\n[INFO] Pipeline configured successfully.\")\n","\n","        tokenizer = AutoTokenizer.from_pretrained(selected_model)\n","\n","    except Exception as e:\n","        logger.error(f\"Pipeline configuration failed: {e}\")\n","        print(f\"[ERROR] Pipeline configuration failed: {e}\")\n","        return\n","\n","    # Helper function to collect filter inputs\n","    def get_filters():\n","        filters = {}\n","        filter_bank = input(\"Do you want to filter by a specific bank? (yes/no): \").strip().lower()\n","        if filter_bank == \"yes\":\n","            filters[\"bank\"] = input(\"Enter the bank name: \").strip().lower()\n","        year = input(\"Enter the year to filter (or press Enter to skip): \").strip()\n","        if year:\n","            filters[\"year\"] = year\n","        quarter = input(\"Enter the quarter to filter (e.g., Q1, or press Enter to skip): \").strip()\n","        if quarter:\n","            filters[\"quarter\"] = quarter\n","        return filters\n","\n","    # Step 2: Interactive Query Interface\n","    try:\n","        while True:\n","            print('\\nRAG Based Analysis:')\n","            print(\"-------------------\\n\")\n","            print(\"\\n--- Query Selection Menu ---\")\n","            print(\"1. Ask a generic question\")\n","            print(\"2. Compare two quarters of the same year\")\n","            print(\"3. Compare two quarters of different years\")\n","            print(\"4. Year-over-year comparison\")\n","            print(\"5. Analyze all quarters of a year\")\n","            print(\"6. Analyze sentiment for a single quarter\")\n","            print(\"7. Summarize a single quarter\")\n","            print(\"8. Perform Topic Modeling with BERTopic\")\n","            print(\"9. Perform Topic Modeling with Gensim LDA\")\n","            print(\"10. Exit\") #adjust exit accordingly\n","\n","            choice = input(\"\\nSelect an option (1-10): \").strip()\n","\n","            if not choice.isdigit() or not (1 <= int(choice) <= 10):\n","                print(\"[ERROR] Invalid option. Please enter a number between 1 and 10.\")\n","                continue\n","\n","            choice = int(choice)\n","\n","            if choice == 1:\n","                query = input(\"Enter your question: \").strip()\n","                filters = get_filters()\n","                answer = handle_generic_query(\n","                    pipeline=pipeline,\n","                    tokenizer=tokenizer,\n","                    summarization_pipeline=pipeline[\"summarization_pipeline\"],\n","                    model_token_limit=model_token_limit,\n","                    filters=filters,\n","                    query=query,\n","                )\n","            elif choice == 2:\n","                year = input(\"Enter the year (e.g., 2023): \").strip()\n","                quarter1 = input(\"Enter Quarter 1 (e.g., Q1): \").strip()\n","                quarter2 = input(\"Enter Quarter 2 (e.g., Q2): \").strip()\n","                bank = input(\"Enter the bank name: \").strip().lower()\n","                query = \"Compare the financial performance of these quarters.\"\n","                answer = handle_compare_quarters_same_year(\n","                    pipeline=pipeline,\n","                    tokenizer=tokenizer,\n","                    model_token_limit=model_token_limit,\n","                    year=year,\n","                    quarter1=quarter1,\n","                    quarter2=quarter2,\n","                    bank=bank,\n","                    query=query,\n","                )\n","            elif choice == 3:\n","                year1 = input(\"Enter Year 1 (e.g., 2022): \").strip()\n","                quarter1 = input(\"Enter Quarter 1 (e.g., Q1): \").strip()\n","                year2 = input(\"Enter Year 2 (e.g., 2023): \").strip()\n","                quarter2 = input(\"Enter Quarter 2 (e.g., Q2): \").strip()\n","                bank = input(\"Enter the bank name: \").strip().lower()\n","                query = \"Compare the performance of these quarters across years.\"\n","                answer = handle_compare_quarters_diff_years(\n","                    pipeline=pipeline,\n","                    tokenizer=tokenizer,\n","                    model_token_limit=model_token_limit,\n","                    year1=year1,\n","                    year2=year2,\n","                    quarter1=quarter1,\n","                    quarter2=quarter2,\n","                    bank=bank,\n","                    query=query,\n","                )\n","            elif choice == 4:\n","                year1 = input(\"Enter Year 1 (e.g., 2022): \").strip()\n","                year2 = input(\"Enter Year 2 (e.g., 2023): \").strip()\n","                bank = input(\"Enter the bank name: \").strip().lower()\n","                query = \"Provide a year-over-year performance comparison.\"\n","                answer = handle_year_comparison(\n","                    pipeline=pipeline,\n","                    tokenizer=tokenizer,\n","                    model_token_limit=model_token_limit,\n","                    year1=year1,\n","                    year2=year2,\n","                    bank=bank,\n","                    query=query,\n","                )\n","            elif choice == 5:\n","                year = input(\"Enter the year (e.g., 2023): \").strip()\n","                bank = input(\"Enter the bank name: \").strip().lower()\n","                query = \"Analyze all quarters of the given year.\"\n","                answer = handle_compare_quarters_same_year(\n","                    pipeline=pipeline,\n","                    tokenizer=tokenizer,\n","                    model_token_limit=model_token_limit,\n","                    year=year,\n","                    quarter1=\"Q1\",\n","                    quarter2=\"Q4\",  # Analyzing from Q1 to Q4\n","                    bank=bank,\n","                    query=query,\n","                )\n","            elif choice == 6:\n","                year = input(\"Enter the year (e.g., 2023): \").strip()\n","                quarter = input(\"Enter the quarter (e.g., Q1): \").strip()\n","                bank = input(\"Enter the bank name: \").strip().lower()\n","                query = \"Analyze the sentiment for this quarter.\"\n","                answer = handle_sentiment_single_quarter(\n","                    pipeline=pipeline,\n","                    tokenizer=tokenizer,\n","                    model_token_limit=model_token_limit,\n","                    year=year,\n","                    quarter=quarter,\n","                    bank=bank,\n","                    query=query,\n","                )\n","            elif choice == 7:\n","                year = input(\"Enter the year (e.g., 2023): \").strip()\n","                quarter = input(\"Enter the quarter (e.g., Q1): \").strip()\n","                bank = input(\"Enter the bank name: \").strip().lower()\n","                query = \"Summarize the performance for this quarter.\"\n","                answer = handle_summarize_single_quarter(\n","                    pipeline=pipeline,\n","                    tokenizer=tokenizer,\n","                    model_token_limit=model_token_limit,\n","                    year=year,\n","                    quarter=quarter,\n","                    bank=bank,\n","                    query=query,\n","                )\n","            elif choice == 8:\n","                print(\"\\n[INFO] Performing Topic Modeling with BERTopic...\")\n","                filters = get_filters()\n","                filtered_docs = filter_documents(pipeline[\"documents\"], **filters)\n","                if not filtered_docs:\n","                    print(\"[ERROR] No documents matched the specified filters for BERTopic.\")\n","                    continue\n","                handle_bertopic(filtered_docs)\n","                answer = \"BERTopic modeling completed successfully.\"\n","            elif choice == 9:\n","                print(\"\\n[INFO] Performing Topic Modeling with Gensim LDA...\")\n","                filters = get_filters()\n","                filtered_docs = filter_documents(pipeline[\"documents\"], **filters)\n","                if not filtered_docs:\n","                    print(\"[ERROR] No documents matched the specified filters for Gensim LDA.\")\n","                    continue\n","                handle_gensim_lda(filtered_docs, nlp_model=nlp_model, num_topics=6)\n","                answer = \"Gensim LDA modeling completed successfully.\"\n","            elif choice == 10:\n","                print(\"Exiting the application. Goodbye!\")\n","                break\n","\n","            display_wrapped_output(remove_repetitions(answer), width=100)\n","\n","    except Exception as e:\n","        logger.exception(f\"An unexpected error occurred in the main function: {e}\")\n","        print(f\"[ERROR] An unexpected error occurred: {e}\")\n"],"metadata":{"id":"iJJOhNrgIlQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install --upgrade transformers"],"metadata":{"id":"bWzBgmTgwgpl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5f892d6-26d8-44e2-822b-809476d35c38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}]},{"cell_type":"markdown","source":["## Main call"],"metadata":{"id":"5SKHDM7SnsSn"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9a930fb2c6614cda8f32e1e0712d6c3a","b1746b352fd84fd0a83058c7af1d0217","f78425841e9e41e3b02f1929894b8fc0","1889fc7501d14e3f861108833654746f","477b80477b414620beec28af88c41b0b","3d087bcc43814867bbdc2fdc3e59b42a","39a133227495426b94271de0dd813fd3","821a1d3b9da941afacfc385133995003","59138ebbaa5f48de80ed8b519d90e7f1","78a5681573254c6386f51144da3d9f0a","ddea114f55974ae5a2bbbb93f662e8be","3771ba6993564ae69d0cd9c6dd98ad13","a79f2817243a43c5a32f2625319ea05b","27722135e6b64ae6aadad443c314cefc","2289794145d9458d9639dbd6485e1e70","b1bb491ed8ff4272bf04f04e870c3003","5b4dc13d7dc04d068c52d3d80830f0d6","d9f24573a3aa45919f2a4872f0346165","51c5ff76bc1d4c17a02f82fc944be234","d68cb0c2d4714859a0810174ab47d897","7c590925a1b14988b8e7f9827489263e","22f3847518ed42f1bc37ba9ef2f7ef81","b99db0af44f14c7e95c1f5145b4374b7","b995255b58f34fcfa6096f4632c941a8","e1d8ffbe2d8b405daf19e03c1e5940cd","93acd38b7a53495398475f36d9eb7c5e","5781de1d07b64d288915b72b20151d43","51c925138ba641349b444d2a14ca08ff","5eedcddc423f4e27afd0b5fe4adb7072","b323c38b30604a258ae80d31e04db5a5","d19292a73b3f4690bb1f6d237b16a20a","27b5ec1afef64d00905abf8be20c4951","67d06f08d23b415197cece74066f7bd5"]},"id":"_NsYINgjklbR","outputId":"421c48ae-2636-4588-ff0a-42fdcc782d3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","======================================================================\n","   Welcome to the Earning Call Transcript-Based Risk Analyzer\n","             An Early Warning System for Investors\n","======================================================================\n","\n","Step 1: Select a Language Model\n","\n","Available Language Models:\n","1. microsoft/Phi-3-mini-4k-instruct\n","2. microsoft/Phi-3.5-MoE-instruct\n","3. gemini-1.5-flash-8b\n","4. openai/gpt-4\n","\n","Select a model (1-4) or press Enter for default [microsoft/Phi-3-mini-4k-instruct]: 1\n","2024-11-25 23:05:39,021 - INFO - User selected model: microsoft/Phi-3-mini-4k-instruct\n","[INFO] Selected model: microsoft/Phi-3-mini-4k-instruct\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a930fb2c6614cda8f32e1e0712d6c3a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","[INFO] Configuring the RAG pipeline...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3771ba6993564ae69d0cd9c6dd98ad13"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2024-11-25 23:05:48,671 - INFO - Model token limit for 'microsoft/Phi-3-mini-4k-instruct': 4096\n","2024-11-25 23:05:48,673 - INFO - Dynamic chunk size calculated based on model_token_limit=4096, prompt_tokens=200, max_new_tokens=500: 1698\n","2024-11-25 23:05:48,674 - INFO - Starting document preprocessing...\n","2024-11-25 23:05:48,676 - INFO - Starting the preprocessing pipeline...\n","2024-11-25 23:05:48,677 - INFO - Step 1: Loading metadata...\n","2024-11-25 23:05:48,678 - INFO - Loading metadata from: /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/metadata/company_participants.csv\n","2024-11-25 23:05:48,684 - INFO - Loaded 105 rows of metadata.\n","2024-11-25 23:05:48,685 - INFO - Starting metadata normalization...\n","2024-11-25 23:05:48,690 - INFO - Metadata normalization completed successfully for 105 rows.\n","2024-11-25 23:05:48,691 - INFO - Loaded data with meta data completed successfully for 105 rows.\n","2024-11-25 23:05:48,692 - INFO - Metadata loaded successfully with 105 rows.\n","2024-11-25 23:05:48,694 - INFO - Step 2: Loading documents from the folder...\n","2024-11-25 23:05:48,695 - INFO - Loading PDF files from folder: /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse\n","2024-11-25 23:05:49,612 - INFO - Loaded 44 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q4 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:49,917 - INFO - Loaded 16 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) Q3 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:50,753 - INFO - Loaded 40 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q4 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:51,517 - INFO - Loaded 40 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) Q4 2022 Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:51,940 - INFO - Loaded 20 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q3 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:52,712 - INFO - Loaded 37 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q2 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:53,534 - INFO - Loaded 40 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q3 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:54,396 - INFO - Loaded 42 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q1 2021 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:55,120 - INFO - Loaded 34 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q2 2022 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:55,873 - INFO - Loaded 36 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q1 2022 Results - Earnings Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:56,766 - INFO - Loaded 43 pages from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse/Credit Suisse Group AG (CS) CEO Thomas Gottstein on Q2 2020 Results - Earnings Call Transcript _ Seeking Alpha.pdf\n","2024-11-25 23:05:56,768 - INFO - Successfully loaded 392 documents from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse.\n","2024-11-25 23:05:56,769 - INFO - Loaded 392 documents from /content/drive/MyDrive/Colab Notebooks/Employer_Project/data/raw/credit suisse.\n","2024-11-25 23:05:56,770 - INFO - Step 3: Attaching metadata to documents...\n","2024-11-25 23:05:56,933 - INFO - Metadata successfully attached to 392 documents.\n","2024-11-25 23:05:56,934 - INFO - Step 4: Validating metadata for enriched documents...\n","2024-11-25 23:05:56,936 - INFO - Metadata validation completed for 392 documents.\n","2024-11-25 23:05:56,937 - INFO - Step 5: Splitting documents into chunks...\n","2024-11-25 23:05:57,963 - INFO - Generated 775 enriched chunks.\n","2024-11-25 23:05:57,971 - INFO - Generated 775 document chunks.\n","2024-11-25 23:05:57,972 - INFO - Preprocessing pipeline completed successfully.\n","2024-11-25 23:05:57,974 - INFO - Enriched document metadata (preview):\n","2024-11-25 23:05:57,975 - INFO - Document 1: {'quarter': 'q4', 'name': 'kinner lakhani', 'year': '2020', 'source': 'credit suisse group ag (cs) ceo thomas gottstein on q4 2020 results - earnings call transcript _ seeking alpha.pdf', 'bank': 'credit suisse', 'designation': 'head of ir and group strategy and development'}\n","2024-11-25 23:05:57,977 - INFO - Document 2: {'quarter': 'q4', 'name': 'kinner lakhani', 'year': '2020', 'source': 'credit suisse group ag (cs) ceo thomas gottstein on q4 2020 results - earnings call transcript _ seeking alpha.pdf', 'bank': 'credit suisse', 'designation': 'head of ir and group strategy and development'}\n","2024-11-25 23:05:57,978 - INFO - Document 3: {'quarter': 'q4', 'name': 'kinner lakhani', 'year': '2020', 'source': 'credit suisse group ag (cs) ceo thomas gottstein on q4 2020 results - earnings call transcript _ seeking alpha.pdf', 'bank': 'credit suisse', 'designation': 'head of ir and group strategy and development'}\n","2024-11-25 23:05:57,980 - INFO - Initializing embeddings and vector store...\n","2024-11-25 23:05:57,981 - INFO - Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n","2024-11-25 23:06:00,556 - INFO - Loading existing vector store from: qa_index\n","2024-11-25 23:06:00,566 - INFO - Loaded vector store with 775 embeddings.\n","2024-11-25 23:06:00,566 - INFO - Vector store initialized successfully.\n","2024-11-25 23:06:00,569 - INFO - Setting up retriever...\n","2024-11-25 23:06:00,570 - INFO - Setting up retriever with top_k=3...\n","2024-11-25 23:06:00,571 - INFO - Retriever configured successfully.\n","2024-11-25 23:06:00,572 - INFO - Loading the language model...\n","2024-11-25 23:06:00,575 - INFO - Loading model 'microsoft/Phi-3-mini-4k-instruct' on cuda with precision 'float16' and max_new_tokens=500.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b99db0af44f14c7e95c1f5145b4374b7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2024-11-25 23:06:06,815 - INFO - Model 'microsoft/Phi-3-mini-4k-instruct' loaded successfully with 3,821,079,552 parameters.\n","2024-11-25 23:06:06,817 - INFO - Initializing QA chains...\n","2024-11-25 23:06:06,818 - INFO - Template 'generic_question' validated successfully.\n","2024-11-25 23:06:06,820 - INFO - Template 'compare_two_quarters_same_year' validated successfully.\n","2024-11-25 23:06:06,820 - INFO - Template 'compare_two_quarters_diff_years' validated successfully.\n","2024-11-25 23:06:06,822 - INFO - Template 'year_comparison' validated successfully.\n","2024-11-25 23:06:06,823 - INFO - Template 'all_quarters_same_year' validated successfully.\n","2024-11-25 23:06:06,824 - INFO - Template 'single_quarter_sentiment' validated successfully.\n","2024-11-25 23:06:06,826 - INFO - Template 'year_sentiment_trends' validated successfully.\n","2024-11-25 23:06:06,827 - INFO - Template 'summarize_single_quarter' validated successfully.\n","2024-11-25 23:06:06,828 - INFO - Template 'aggregated_summaries' validated successfully.\n","2024-11-25 23:06:06,829 - INFO - Template 'trend_analysis' validated successfully.\n","2024-11-25 23:06:06,832 - INFO - QA chains initialized successfully.\n","2024-11-25 23:06:06,833 - INFO - Loading metadata...\n","2024-11-25 23:06:06,838 - INFO - Loaded metadata with 105 rows.\n","2024-11-25 23:06:06,838 - INFO - Initializing summarization pipeline...\n","2024-11-25 23:06:08,330 - INFO - Initializing sentiment analysis pipeline...\n","2024-11-25 23:06:09,093 - INFO - Initializing BERTopic model...\n","2024-11-25 23:06:09,097 - INFO - BERTopic model initialized successfully.\n","2024-11-25 23:06:09,098 - INFO - RAG pipeline configured successfully.\n","2024-11-25 23:06:09,100 - INFO - Setting up retriever with top_k=3...\n","2024-11-25 23:06:09,101 - INFO - Retriever configured successfully.\n","\n","[INFO] Pipeline configured successfully.\n","\n","RAG Based Analysis:\n","-------------------\n","\n","\n","--- Query Selection Menu ---\n","1. Ask a generic question\n","2. Compare two quarters of the same year\n","3. Compare two quarters of different years\n","4. Year-over-year comparison\n","5. Analyze all quarters of a year\n","6. Analyze sentiment for a single quarter\n","7. Summarize a single quarter\n","8. Perform Topic Modeling with BERTopic\n","9. Perform Topic Modeling with Gensim LDA\n","10. Exit\n","\n","Select an option (1-10): 1\n","Enter your question: What is the credit suisse strategy for sustaining or accelerating growth for year 2022?\n","Do you want to filter by a specific bank? (yes/no): yes\n","Enter the bank name: credit suisse\n","Enter the year to filter (or press Enter to skip): 2022\n","Enter the quarter to filter (e.g., Q1, or press Enter to skip): \n","2024-11-25 23:08:16,256 - INFO - Handling a generic query.\n","2024-11-25 23:08:16,257 - INFO - Retrieving relevant documents with filters...\n","2024-11-25 23:08:16,274 - INFO - Filtered 3 documents for the generic query.\n","2024-11-25 23:08:16,275 - INFO - Preparing context for the query...\n","2024-11-25 23:08:16,276 - INFO - Context token budget: 3396 tokens.\n","2024-11-25 23:08:16,279 - INFO - Document token count: 350\n","2024-11-25 23:08:16,281 - INFO - Document token count: 445\n","2024-11-25 23:08:16,282 - INFO - Document token count: 340\n","2024-11-25 23:08:16,283 - INFO - Invoking the QA chain...\n","[RESULT] Response from the AI Assistant:\n","The Credit Suisse strategy for sustaining or accelerating growth in 2022, as outlined by CEO Thomas\n","Gottstein, focuses on leveraging the benefits of strategic capital allocation towards the Wealth\n","Management businesses and implementing structural cost-saving measures. The strategy is designed to\n","materialize from 2023 onwards, acknowledging the end of an extremely favorable business environment\n","that was driven by substantial measures taken by central banks and governments during the pandemic.\n","Key components of the strategy include:  1. Capital Allocation: Credit Suisse has strategically\n","allocated capital towards its Wealth Management businesses, aiming to push them into profitability\n","and growth mode. This focus on Wealth Management is expected to drive growth in the coming years.\n","2. Cost-Saving Measures: The bank has implemented structural cost-saving measures to improve its\n","financial performance. These measures are expected to contribute to the overall growth strategy by\n","optimizing resources and reducing expenses.  3. Adapting to a New Environment: Credit Suisse\n","recognizes the shift to a more normal trading environment, driven by the tightening of the interest\n","rate environment in various markets. The bank expects this shift to be reflected in its results for\n","the first quarter of 2022.  4. Client Acquisition and Retention: Despite the challenging\n","environment, Credit Suisse continues to invest in its Private Banking businesses across Asia, EMEA,\n","and Latin America. The bank aims to attract and retain clients by improving its offerings, working\n","closely with the Investment Bank, and positioning itself as the bank for entrepreneurs in these\n","regions.  5. Focus on Stable High Net Worth Business: Credit Suisse aims to focus on the stable high\n","net worth business, which is expected to provide a solid foundation for growth.  In summary, Credit\n","Suisse's strategy for sustaining or accelerating growth in 2022 revolves around capital allocation\n","towards Wealth Management, implementing cost-saving measures, adapting to a changing environment,\n","and focusing on client acquisition and retention in key regions.\n","\n","RAG Based Analysis:\n","-------------------\n","\n","\n","--- Query Selection Menu ---\n","1. Ask a generic question\n","2. Compare two quarters of the same year\n","3. Compare two quarters of different years\n","4. Year-over-year comparison\n","5. Analyze all quarters of a year\n","6. Analyze sentiment for a single quarter\n","7. Summarize a single quarter\n","8. Perform Topic Modeling with BERTopic\n","9. Perform Topic Modeling with Gensim LDA\n","10. Exit\n","\n","Select an option (1-10): 4\n","Enter Year 1 (e.g., 2022): 2020\n","Enter Year 2 (e.g., 2023): 2021\n","Enter the bank name: credit suisse\n","2024-11-25 23:10:56,373 - INFO - Comparing performance for 2020 and 2021 for credit suisse.\n","2024-11-25 23:10:56,375 - INFO - Retrieving documents for 2020...\n","2024-11-25 23:10:56,393 - INFO - Retrieving documents for 2021...\n","[RESULT] Response from the AI Assistant:\n","1. Key Performance Highlights:    - Adjusted pretax income for the third quarter of the current year\n","was CHF1.4 billion, marking the best third quarter in the last 5 years.    - The company's 9-month\n","revenue performance over the past 2 years showcases the core strength of the business.    - Capital\n","Markets revenues increased by 90% year-on-year, driven by higher debt issuance activity and a\n","threefold increase in ECM revenues.    - Fixed income sales and trading performance remained\n","resilient, with a 5% increase in revenues year-on-year.    - Equity sales and trading revenues were\n","up by 5% year-on-year, with strong contributions from cash equities and equity derivatives.  2.\n","Challenges/Risks:    - The company released a net total of CHF144 million in provisions for credit\n","losses, with CHF188 million attributed to Archegos.    - The increase in provisions for credit\n","losses indicates potential risks associated with the Archegos situation.  3. Future\n","Strategies/Initiatives:    - The company's key growth agenda aims to deliver attractive shareholder\n","value, including the ambition to deliver pretax income in the Wealth division.    - The positive\n","trends observed in the Investment Bank last year have continued into 2021, with a strong capital\n","markets pipeline and resilient trading performance.    - The company reserves questions related to\n","the strategy review for later today, indicating a focus on refining and enhancing its strategic\n","direction.  Please note that the provided information is based on the given transcript and may not\n","reflect the most current or complete data.\n","\n","RAG Based Analysis:\n","-------------------\n","\n","\n","--- Query Selection Menu ---\n","1. Ask a generic question\n","2. Compare two quarters of the same year\n","3. Compare two quarters of different years\n","4. Year-over-year comparison\n","5. Analyze all quarters of a year\n","6. Analyze sentiment for a single quarter\n","7. Summarize a single quarter\n","8. Perform Topic Modeling with BERTopic\n","9. Perform Topic Modeling with Gensim LDA\n","10. Exit\n","\n","Select an option (1-10): 6\n","Enter the year (e.g., 2023): 2022\n","Enter the quarter (e.g., Q1): Q2\n","Enter the bank name: credit suisse\n","2024-11-25 23:12:33,763 - INFO - Analyzing sentiment for credit suisse in Q2 2022.\n","2024-11-25 23:12:33,765 - INFO - Retrieving documents for Q2 2022...\n","[RESULT] Response from the AI Assistant:\n","The sentiment for Credit Suisse Group AG's Q2 2022 performance, as presented by Thomas Gottstein,\n","can be analyzed as cautiously optimistic. The sentiment is not overwhelmingly positive due to the\n","acknowledgment of challenging market and macroeconomic conditions, as well as underperformance in\n","certain business lines. However, there are also positive aspects that contribute to a more balanced\n","sentiment.  1. Strong Underlying Performance: The statement that the underlying performance has been\n","very strong, both year-on-year and multiyear, indicates a positive sentiment. The mention of the\n","best third quarter in the last five years, with an adjusted pretax income of CHF1.4 billion,\n","highlights the company's resilience and ability to perform well despite market challenges.  2. Core\n","Business Strength: The reference to the core strength of the business, as evidenced by the 9-month\n","revenue performance over the past two years, adds to the positive sentiment. This suggests that the\n","company has a solid foundation to invest in and grow, which is a positive sign for investors.  3.\n","Equity Capital Markets (ECM) Performance: The mention of strong performance in ECM, particularly in\n","January, contributes to a positive sentiment. The comparison with U.S. peers and the acknowledgment\n","of a strong start in January indicate that the company is well-positioned in this area.  4. Concerns\n","and Challenges: The acknowledgment of challenging market and macroeconomic conditions, as well as\n","underperformance in certain business lines, adds a note of caution to the sentiment. This indicates\n","that the company is aware of the risks and challenges it faces, which is a realistic and balanced\n","perspective.  In summary, the sentiment for Credit Suisse Group AG's Q2 2022 performance is\n","cautiously optimistic. The company has demonstrated strong underlying performance and core business\n","strength, particularly in ECM, which contributes to a positive sentiment. However, the\n","acknowledgment of challenges and underperformance in certain areas adds a note of caution, making\n","the sentiment more balanced.\n","\n","RAG Based Analysis:\n","-------------------\n","\n","\n","--- Query Selection Menu ---\n","1. Ask a generic question\n","2. Compare two quarters of the same year\n","3. Compare two quarters of different years\n","4. Year-over-year comparison\n","5. Analyze all quarters of a year\n","6. Analyze sentiment for a single quarter\n","7. Summarize a single quarter\n","8. Perform Topic Modeling with BERTopic\n","9. Perform Topic Modeling with Gensim LDA\n","10. Exit\n","\n","Select an option (1-10): 9\n","\n","[INFO] Performing Topic Modeling with Gensim LDA...\n","Do you want to filter by a specific bank? (yes/no): yes\n","Enter the bank name: credit suisse\n","Enter the year to filter (or press Enter to skip): 2022\n","Enter the quarter to filter (e.g., Q1, or press Enter to skip): Q2\n","2024-11-25 23:14:50,448 - INFO - Starting filtering with criteria bank: credit suisse, year: 2022, quarter: Q2, designation: None\n","2024-11-25 23:14:50,453 - INFO - Filtered 34 documents for Bank: credit suisse, Year: 2022, Quarter: Q2, Designation: None\n","2024-11-25 23:14:50,454 - INFO - Starting Gensim LDA modeling for earnings call transcripts...\n","2024-11-25 23:14:50,456 - INFO - Preprocessing the corpus for earnings calls...\n","2024-11-25 23:14:50,457 - INFO - Starting corpus preprocessing for bigrams and trigrams...\n","2024-11-25 23:14:50,458 - INFO - Preprocessing individual documents...\n","2024-11-25 23:14:53,580 - INFO - Sample preprocessed tokens:\n","2024-11-25 23:14:53,581 - INFO - [['cs', 'ceo', 'transcript', 'jul', 'pm', 'et', 'cs', 'stock', 'csgkf', 'stock', 'sa', 'transcripts', 'k', 'followers', 'welcome', 'seeking', 'alpha', 'articles', 'cs', 'available', 'free', 'next', 'days', 'continue', 'receiving', 'professional', 'grade', 'analyses', 'cs', 'gain', 'access', 'similar', 'insights', 'across', 'entire', 'market', 'subscribe', 'premium', 'trial', 'expires', 'start', 'today', 'first', 'month', 'transcripts', 'switzerland', 'join', 'premium', 'play', 'cs', 'earnings', 'conference', 'call', 'july', 'et', 'participants', 'head', 'investor', 'relations', 'group', 'strategy', 'development', 'axel', 'lehmann', 'independent', 'chairman', 'board', 'member', 'executive', 'board', 'group', 'ceo', 'member', 'executive', 'board', 'conference', 'call', 'participants', 'cs', 'ceo'], ['paribas', 'magdalena', 'stoklosa', 'morgan', 'stanley', 'andrew', 'coombs', 'citi', 'autonomous', 'research', 'kian', 'abouhossein', 'jpmorgan', 'amit', 'goel', 'barclays', 'alastair', 'ryan', 'bank', 'america', 'chris', 'hallam', 'goldman', 'sachs', 'anke', 'reingen', 'royal', 'bank', 'canada', 'general', 'piers', 'brown', 'hsbc', 'good', 'conference', 'welcome', 'joining', 'credit', 'suisse', 'group', 'conference', 'call', 'analysts', 'investors', 'instructions', 'conference', 'recorded', 'instructions', 'turn', 'conference', 'head', 'investor', 'relations', 'group', 'strategy', 'development', 'please', 'go', 'ahead', 'everyone', 'welcome', 'everyone', 'begin', 'let', 'remind', 'important', 'cautionary', 'statements', 'slides', 'including', 'relation', 'forward', 'looking', 'statements', 'non', 'gaap', 'financial', 'measures', 'basel', 'iii', 'disclosures', 'detailed', 'discussion', 'refer', 'earnings', 'release', 'published', 'let', 'remind', 'financial', 'report', 'accompanying', 'financial', 'statements', 'period', 'published', 'around', 'july', 'hand', 'axel', 'lehmann', 'group', 'chairman', 'elaborate', 'announcements', 'followed', 'group', 'ceo', 'group', 'cfo', 'run', 'numbers', 'axel', 'lehmann', 'good', 'joining', 'analysts', 'call', 'cs', 'ceo']]\n","2024-11-25 23:14:53,582 - INFO - Building bigram and trigram models...\n","2024-11-25 23:14:53,628 - INFO - Applying bigram and trigram models...\n","2024-11-25 23:14:53,640 - INFO - Sample processed documents with bigrams/trigrams:\n","2024-11-25 23:14:53,641 - INFO - [['cs_ceo', 'across_entire', 'conference_call', 'group_strategy', 'axel_lehmann', 'board_member', 'executive_board', 'group_ceo', 'executive_board', 'conference_call', 'cs_ceo'], ['magdalena_stoklosa', 'kian_abouhossein', 'anke_reingen', 'piers_brown', 'conference_call', 'group_strategy', 'axel_lehmann', 'group_ceo', 'axel_lehmann', 'cs_ceo']]\n","2024-11-25 23:14:53,642 - INFO - Creating dictionary and bag-of-words representation...\n","2024-11-25 23:14:53,645 - INFO - Vocabulary size after preprocessing: 181\n","2024-11-25 23:14:53,646 - INFO - Training Gensim LDA model with 6 topics...\n","2024-11-25 23:14:53,828 - INFO - Displaying top topics for earnings calls...\n","\n","Top Topics Identified by Gensim LDA:\n","Topic #1: 0.079*\"investment_bank\" + 0.063*\"strategy_review\" + 0.034*\"short_term\" + 0.034*\"give_us\" + 0.032*\"last_year\" + 0.028*\"cs_ceo\" + 0.019*\"axel_lehmann\" + 0.018*\"cost_program\" + 0.018*\"think_quite\" + 0.018*\"lot_work\"\n","Topic #2: 0.073*\"cs_ceo\" + 0.044*\"axel_lehmann\" + 0.041*\"cet_ratio\" + 0.035*\"mean_think\" + 0.030*\"next_comes_line\" + 0.029*\"chf_billion\" + 0.028*\"think_terms\" + 0.025*\"securitized_products\" + 0.024*\"piers_brown\" + 0.024*\"cost_target\"\n","Topic #3: 0.080*\"chf_million\" + 0.050*\"chf_billion\" + 0.030*\"year_year\" + 0.025*\"net_new\" + 0.025*\"partly_offset\" + 0.024*\"cs_ceo\" + 0.023*\"lower_year_year\" + 0.019*\"chf_billion_chf_billion\" + 0.017*\"net_revenues\" + 0.017*\"wealth_management_division\"\n","Topic #4: 0.037*\"cs_ceo\" + 0.027*\"board_directors\" + 0.027*\"board_director\" + 0.027*\"compensation_costs\" + 0.027*\"right_direction\" + 0.027*\"corporate_functions\" + 0.027*\"tax_charge\" + 0.020*\"last_year\" + 0.020*\"new_leadership\" + 0.020*\"leveraged_finance\"\n","Topic #5: 0.095*\"third_party\" + 0.042*\"cs_ceo\" + 0.042*\"next_comes_line\" + 0.042*\"kian_abouhossein\" + 0.029*\"look_think\" + 0.029*\"give_us\" + 0.029*\"securitized_products\" + 0.029*\"would_like\" + 0.029*\"questions_first\" + 0.029*\"restructuring_costs\"\n","Topic #6: 0.040*\"chf_billion\" + 0.030*\"wealth_management\" + 0.030*\"investment_bank\" + 0.027*\"loss_chf_billion\" + 0.025*\"adjusted_pretax\" + 0.024*\"asset_management\" + 0.022*\"cs_ceo\" + 0.017*\"funding_costs\" + 0.017*\"strategic_review\" + 0.017*\"major_litigation_provisions\"\n","2024-11-25 23:14:53,831 - INFO - Preparing visualization for LDA topics...\n"]},{"output_type":"display_data","data":{"text/plain":["PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n","topic                                                \n","2     -0.156095  0.045167       1        1  31.567563\n","5     -0.131828  0.003817       2        1  31.066802\n","1      0.109722  0.110618       3        1  16.809997\n","3      0.059011 -0.133566       4        1  10.521565\n","4      0.102145  0.043971       5        1   5.561307\n","0      0.017045 -0.070007       6        1   4.472766, topic_info=                    Term       Freq      Total Category  logprob  loglift\n","34       investment_bank  15.000000  15.000000  Default  30.0000  30.0000\n","142          third_party   6.000000   6.000000  Default  29.0000  29.0000\n","4                 cs_ceo  29.000000  29.000000  Default  28.0000  28.0000\n","1           axel_lehmann   9.000000   9.000000  Default  27.0000  27.0000\n","161      next_comes_line   8.000000   8.000000  Default  26.0000  26.0000\n","..                   ...        ...        ...      ...      ...      ...\n","15   corporate_functions   0.103572   3.092101   Topic6  -5.9114  -0.2892\n","118           tax_charge   0.103232   3.093766   Topic6  -5.9147  -0.2930\n","99           chf_million   0.111493  28.686890   Topic6  -5.8377  -2.4431\n","48            swiss_bank   0.105319   7.840899   Topic6  -5.8947  -1.2029\n","32           chf_billion   0.107298  29.488544   Topic6  -5.8761  -2.5090\n","\n","[271 rows x 6 columns], token_table=      Topic      Freq              Term\n","term                                   \n","164       2  0.273244  ability_actually\n","164       4  0.273244  ability_actually\n","164       5  0.273244  ability_actually\n","164       6  0.273244  ability_actually\n","24        2  0.775195     absolute_cost\n","...     ...       ...               ...\n","52        2  0.359723        would_like\n","52        5  0.179862        would_like\n","136       1  0.886106        would_note\n","145       1  0.785792         year_year\n","145       4  0.196448         year_year\n","\n","[317 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 6, 2, 4, 5, 1])"],"text/html":["\n","<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n","\n","\n","<div id=\"ldavis_el652133977166984496120428035\" style=\"background-color:white;\"></div>\n","<script type=\"text/javascript\">\n","\n","var ldavis_el652133977166984496120428035_data = {\"mdsDat\": {\"x\": [-0.156094595244274, -0.13182825479867108, 0.10972179390883727, 0.059010665031958905, 0.10214528072769953, 0.017045110374449262], \"y\": [0.04516678530480107, 0.003817449974405991, 0.11061788573774999, -0.13356587932182196, 0.04397109828631484, -0.07000733998144962], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [31.567562780825714, 31.066802181344798, 16.809996908613584, 10.52156507886128, 5.561307388018097, 4.472765662336527]}, \"tinfo\": {\"Term\": [\"investment_bank\", \"third_party\", \"cs_ceo\", \"axel_lehmann\", \"next_comes_line\", \"cet_ratio\", \"strategy_review\", \"chf_million\", \"give_us\", \"mean_think\", \"securitized_products\", \"last_year\", \"look_think\", \"kian_abouhossein\", \"would_like\", \"year_year\", \"board_directors\", \"restructuring_costs\", \"short_term\", \"questions_first\", \"think_terms\", \"chf_billion_chf_billion\", \"loss_chf_billion\", \"cost_program\", \"leverage_exposure\", \"market_environment\", \"board_director\", \"compensation_costs\", \"right_direction\", \"corporate_functions\", \"lower_year_year\", \"net_new\", \"partly_offset\", \"assets_management\", \"quarter_quarter\", \"adversely_affected\", \"would_note\", \"totaled_chf\", \"minus_basis\", \"challenging_environment\", \"nonspecific_provisions\", \"year_year\", \"net_revenues\", \"offset_higher\", \"risk_compliance\", \"revenues_higher\", \"due_market\", \"higher_technology\", \"chf_million\", \"net_asset\", \"revenues_lower\", \"client_business\", \"let_turn\", \"chf_billion_chf_billion\", \"operating_expenses\", \"wealth_management_division\", \"compliance_costs\", \"driven_outflows\", \"asset_outflows\", \"next_comes\", \"first_half_year\", \"back_june\", \"credit_losses\", \"chf_billion\", \"first_half\", \"totaling_chf\", \"interest_rates\", \"adjusted_pretax\", \"cs_ceo\", \"net_interest_income\", \"swiss_bank\", \"investment_bank\", \"loss_chf_billion\", \"funding_costs\", \"reported_pretax\", \"advisory_led\", \"management_swiss\", \"growth_wealth\", \"risk_management\", \"strategic_options\", \"party_capital\", \"attracting_third\", \"leading_universal\", \"bank_switzerland\", \"multi_specialist\", \"expenses_chf\", \"market_leading\", \"loss_chf\", \"wealth_management\", \"major_litigation_provisions\", \"bank_franchises\", \"major_litigation\", \"million_quarter\", \"absolute_cost\", \"risk_culture\", \"transform_investment\", \"world_class\", \"banking_business\", \"credit_spreads\", \"equity_derivatives\", \"loss_investment\", \"bank_reported\", \"markets_business\", \"asset_management\", \"adjusted_pretax\", \"investment_bank\", \"swiss_bank\", \"strategic_review\", \"chf_billion\", \"leveraged_finance\", \"capital_light\", \"securitized_products\", \"cs_ceo\", \"chf_million\", \"board_directors\", \"medium_term\", \"market_environment\", \"piers_brown\", \"cost_target\", \"current_perimeter\", \"magdalena_stoklosa\", \"cet_ratio\", \"conference_call\", \"best_wishes\", \"could_talk\", \"think_terms\", \"mean_think\", \"axel_lehmann\", \"leverage_ratio\", \"board_member\", \"half_year\", \"across_entire\", \"group_strategy\", \"look_think\", \"capital_ratio\", \"next_comes_line\", \"group_ceo\", \"going_forward\", \"exit_prime\", \"going_comment\", \"sp_business\", \"cs_ceo\", \"primarily_due\", \"detailed_work\", \"leverage_exposure\", \"securitized_products\", \"capital_base\", \"markets_business\", \"strategic_review\", \"executive_board\", \"third_party\", \"chf_billion\", \"last_year\", \"basis_points\", \"chf_million\", \"board_director\", \"compensation_costs\", \"right_direction\", \"corporate_functions\", \"tax_charge\", \"million_mark\", \"market_losses\", \"cost_efficiency\", \"investor_day\", \"new_leadership\", \"strategic_direction\", \"little_bit\", \"think_second\", \"cost_program\", \"covid_pandemic\", \"anke_reingen\", \"board_directors\", \"detailed_work\", \"think_know\", \"lot_work\", \"amongst_others\", \"speed_transformation\", \"investment_banking\", \"next_months\", \"absolute_priority\", \"supply_chain\", \"regulatory_remediation\", \"less_complex\", \"well_look\", \"questions_first\", \"capital_light\", \"executive_board\", \"mean_think\", \"leveraged_finance\", \"last_year\", \"next_comes_line\", \"cs_ceo\", \"year_year\", \"chf_billion\", \"interest_rates\", \"cost_base\", \"third_party\", \"kian_abouhossein\", \"products_business\", \"leverage_exposure\", \"questions_first\", \"restructuring_costs\", \"give_us\", \"sp_business\", \"strategic_direction\", \"would_like\", \"next_months\", \"next_comes_line\", \"capital_base\", \"risk_appetite\", \"risk_weighted\", \"look_think\", \"well_look\", \"ability_actually\", \"give_details\", \"axel_lehmann\", \"securitized_products\", \"world_class\", \"cet_ratio\", \"board_directors\", \"cost_efficiency\", \"cs_ceo\", \"half_year\", \"board_member\", \"leverage_ratio\", \"comes_line\", \"quite_clearly\", \"would_note\", \"partly_offset\", \"revenues_lower\", \"lower_year_year\", \"totaling_chf\", \"chf_million\", \"strategy_review\", \"short_term\", \"give_us\", \"investor_day\", \"think_know\", \"lot_work\", \"speed_transformation\", \"amongst_others\", \"back_june\", \"sp_business\", \"detailed_work\", \"investment_bank\", \"well_look\", \"going_forward\", \"ability_actually\", \"cost_program\", \"think_quite\", \"cost_base\", \"think_second\", \"market_environment\", \"last_year\", \"chf_billion_chf_billion\", \"axel_lehmann\", \"products_business\", \"cost_efficiency\", \"comes_line\", \"half_year\", \"board_member\", \"leverage_ratio\", \"lower_year\", \"cs_ceo\", \"magdalena_stoklosa\", \"current_perimeter\", \"corporate_functions\", \"tax_charge\", \"chf_million\", \"swiss_bank\", \"chf_billion\"], \"Freq\": [15.0, 6.0, 29.0, 9.0, 8.0, 7.0, 3.0, 28.0, 4.0, 7.0, 10.0, 9.0, 6.0, 3.0, 5.0, 10.0, 7.0, 3.0, 2.0, 3.0, 5.0, 7.0, 7.0, 3.0, 2.0, 4.0, 3.0, 3.0, 3.0, 3.0, 6.2429037272458165, 6.803038145090123, 6.7905030034526215, 3.6314864362805124, 3.61901589581813, 3.487577389728392, 2.7593235666487224, 2.7597677311484707, 2.756188775773586, 2.7428877205929414, 2.7410218275076925, 7.997605370425993, 4.657919677127491, 1.8882396874612843, 1.8875490581669667, 1.8869780792981612, 1.8868159479442632, 1.8863985539703907, 21.524922522112043, 3.1552154038641294, 3.632295836217802, 3.628464760303445, 3.613731670513254, 5.2021948835297245, 4.173992274310517, 4.650146924065109, 2.1890817042459028, 2.18400133712399, 1.0185859576350915, 1.0185856434270413, 3.634534505733796, 1.8982248422466637, 2.5366800155135407, 13.556638544196904, 3.7181900111157136, 3.1250054309873376, 3.813693422287757, 4.111590806909214, 6.504011622419503, 2.8981665739660185, 2.930202472664075, 2.928662350485263, 7.267946431036195, 4.609708888115933, 3.7076829984360256, 3.704446539245068, 3.7034664237276353, 3.7014972870495018, 2.8206801868121953, 2.8180631645863192, 2.816870055767395, 2.8166434566220215, 2.8150602311521657, 2.8145743788361033, 2.8139952646447077, 2.8110917775603794, 2.809875662496346, 2.7792570714252474, 8.07693951831756, 4.575811536048354, 1.927178110945116, 1.9229053812083419, 1.9072252645822747, 3.7067506270616684, 2.9945826619371103, 2.8168089531594176, 3.901392861083739, 2.8184589709537833, 2.8198079285298965, 2.7663837167024594, 1.0389334568433675, 1.0388248574712946, 3.708751180870219, 6.292617696859909, 6.76428035464842, 7.980036224403179, 4.422349491255201, 4.596991629529313, 10.641991378629848, 4.32235148630521, 3.70343327494436, 4.515976992720857, 5.75227471818904, 4.505995250891374, 2.8244391104082056, 2.8226876672321772, 2.820427365494978, 3.447118439848546, 3.4339506099990453, 3.412250341233504, 3.3220196390706427, 5.862234751952048, 2.6198008800349006, 2.6039455139693444, 2.5427915496324087, 4.067046494393704, 5.101766141824891, 6.2532036747256985, 0.9651742601859606, 0.9651703114700341, 0.9597101743270298, 1.95276973096731, 1.7926104822595688, 3.206637356567642, 2.623320725255411, 4.279406295248135, 2.0846031671490595, 1.5051644425862734, 1.468791013147157, 1.350526971146424, 1.0783582456834688, 10.561362511529449, 1.465117770352008, 1.064099633254795, 0.9632950729692454, 3.532401869447137, 0.9953557684152743, 1.7945355816648538, 2.301415781020888, 1.9558030140728007, 1.801799478857637, 4.126619633812241, 1.852350404730461, 1.291485369048037, 2.2166297557742674, 2.4571344263852732, 2.456768972781498, 2.4539971554895903, 2.450176565109177, 2.439296067948451, 1.6634985571614849, 1.6616521870285186, 0.9015757999188592, 1.6790810229492346, 1.8242939537625005, 1.6774839856738646, 1.6786202263374135, 1.6788564893274744, 1.6738366548194965, 1.317983874123315, 1.6817001908908262, 2.457842545105839, 0.9063405206662924, 0.9051035749977601, 0.9052525379751816, 0.9061634909861509, 0.9060407515250022, 0.9818263784283927, 0.9053034769460883, 0.9021776002938976, 0.9053046498829184, 0.9052350277039323, 0.9019322889340045, 0.905019961358015, 0.9051235987050738, 1.6784436155632827, 1.7827977945875655, 1.6772178965758384, 1.7941769574634845, 1.8401270929070808, 1.6807770896055452, 3.37152849888937, 1.6846846448737889, 0.9441831491779002, 0.906984965673225, 0.9060873338733972, 4.5105431116071, 1.993017033746534, 0.7338564164513738, 1.3638635749384562, 1.3644046322409813, 1.364290203359845, 1.3650725178390073, 0.7360083790979776, 0.7329668470379572, 1.3648049561904678, 0.7362207190689719, 1.9936970530571885, 0.7337661221446257, 0.7380627849237055, 0.7371429697959637, 1.3656810633825742, 0.7344236453749328, 0.7344037177447039, 0.7364496653984901, 1.36392087794627, 1.3648939662721569, 0.7332863976595828, 0.7359171105515294, 0.7328667660505856, 0.10512227926693403, 1.9967294184073034, 0.10512243425961358, 0.10512302101761477, 0.10512251175595336, 0.1051318334585382, 0.10559383342358308, 0.11087813169861681, 0.11533905350531086, 0.11225765511392706, 0.11362817788291729, 0.11190040805845192, 0.11587764199586868, 2.3986793559903257, 1.285997984842771, 1.2826718989391481, 0.6897250359075521, 0.6924534178916714, 0.6926107683812653, 0.6875646542874088, 0.6855232293524722, 0.6839327713481014, 0.6054702239535074, 0.616542740705183, 3.0343434222166166, 0.6925403203802656, 0.5866687280830457, 0.6897429862576552, 0.697567914271635, 0.6937080191470898, 0.6830025585624028, 0.640825860354145, 0.6870572007392567, 1.2391190784545358, 0.5951160417479208, 0.7299729974898538, 0.0988982018241417, 0.10132509628123215, 0.10391676711679852, 0.10002004528192282, 0.09889890523567849, 0.09889850455822083, 0.10055782566164777, 1.0889802844775303, 0.1844413787873702, 0.1189208023145616, 0.10357246052546133, 0.10323152852871197, 0.1114929456611208, 0.1053193964329628, 0.10729823554901316], \"Total\": [15.0, 6.0, 29.0, 9.0, 8.0, 7.0, 3.0, 28.0, 4.0, 7.0, 10.0, 9.0, 6.0, 3.0, 5.0, 10.0, 7.0, 3.0, 2.0, 3.0, 5.0, 7.0, 7.0, 3.0, 2.0, 4.0, 3.0, 3.0, 3.0, 3.0, 6.878998764612247, 7.760652485097463, 7.759601224407015, 4.260136941212381, 4.25960463658994, 4.252199321179476, 3.3855991481656202, 3.3870966099014383, 3.3853806839881235, 3.3851851981974135, 3.3852135898492417, 10.18081110025836, 6.022062692180926, 2.5124963668539912, 2.512209649860633, 2.512183516256255, 2.512102069645349, 2.5119629220457873, 28.68689029688477, 4.270618410997425, 5.150051460942591, 5.151482163096364, 5.151534059729474, 7.4589416907038455, 6.0318039975149, 6.903246674440471, 3.3986843158546924, 3.3979161992090825, 1.6395840154001977, 1.6395840298333713, 5.928177709772353, 3.1100826411907576, 4.28182020911209, 29.488544177113013, 6.934997842632654, 6.053332678701448, 8.533824758054253, 11.383704873888695, 29.27488705391219, 6.057853072201308, 7.840898850400585, 15.218015677822315, 7.90126243672348, 5.2266416725859965, 4.333285649159708, 4.332815017425244, 4.332769925665297, 4.332420160945696, 3.441156970920743, 3.4414733398804396, 3.4412531384928307, 3.441301060589597, 3.4406452493680226, 3.4405362954230667, 3.4406032604060464, 3.440871281954397, 3.4407738357406767, 3.4387303547446675, 10.489722707205718, 6.051199603670878, 2.549331167775738, 2.549138899234523, 2.5485313691969482, 5.1599939855355395, 4.3188695242660575, 4.216044384778199, 5.842241049693515, 4.268663445239234, 4.314588387819153, 4.312571477772747, 1.6576409244304355, 1.6576330912694328, 5.987799070693942, 10.356825751654751, 11.383704873888695, 15.218015677822315, 7.840898850400585, 8.50049241527829, 29.488544177113013, 8.457741415510029, 6.75768429691878, 10.663963369425755, 29.27488705391219, 28.68689029688477, 7.27420137213059, 5.142794992894531, 4.81365425015836, 4.074865947061767, 4.075580234247745, 4.067175503006723, 4.041296193935749, 7.190892601790619, 3.2475469267494472, 3.2483993154798645, 3.2517980363517918, 5.689863135433802, 7.280656497880738, 9.250392904830447, 1.5929067161840555, 1.5929056725397441, 1.5926432110307842, 3.299082433260081, 3.2942347575570765, 6.125627044869001, 5.012773516618928, 8.347407385952872, 4.151829927585068, 3.0796964744487036, 3.3114232369262977, 3.319263991343224, 2.851504282915339, 29.27488705391219, 4.088100402357354, 2.9923068632265633, 2.8549440591301902, 10.663963369425755, 3.112716166316121, 5.987799070693942, 8.50049241527829, 7.4644028379860545, 6.838102388631517, 29.488544177113013, 9.571747035188391, 4.194549106275688, 28.68689029688477, 3.0934326335093187, 3.093485468710598, 3.0928281546949705, 3.0921010519690983, 3.0937656449401407, 2.319934569951274, 2.3202325419819987, 1.5408572323104366, 2.912327401250551, 3.173418292534148, 2.9484296048581715, 3.2089638321766825, 3.7588929398212256, 3.7833821759621524, 3.237607711485081, 4.929133028918588, 7.27420137213059, 2.9923068632265633, 3.007920125075707, 3.0088662005258207, 3.0292979458468574, 3.0291768670656483, 3.298050847445385, 3.0460838075668324, 3.260293656943983, 3.2848430980624745, 3.325579577772741, 3.324298481936583, 3.5933083405684023, 3.630969988810563, 6.75768429691878, 7.4644028379860545, 7.280656497880738, 8.457741415510029, 9.571747035188391, 8.347407385952872, 29.27488705391219, 10.18081110025836, 29.488544177113013, 8.533824758054253, 3.887351619658989, 6.838102388631517, 3.486421953019385, 1.3969522477158407, 2.8549440591301902, 3.630969988810563, 3.77724352145573, 4.883598225764441, 2.851504282915339, 2.9484296048581715, 5.55983296508833, 3.0460838075668324, 8.347407385952872, 3.112716166316121, 3.143454636670867, 3.161674826106483, 6.125627044869001, 3.5933083405684023, 3.659730805376795, 3.9893847738361776, 9.250392904830447, 10.663963369425755, 5.842241049693515, 7.190892601790619, 7.27420137213059, 1.5408572323104366, 29.27488705391219, 1.5926432110307842, 1.5929056725397441, 1.5929067161840555, 1.631869029727323, 1.6573894666180788, 3.3855991481656202, 7.759601224407015, 5.150051460942591, 6.878998764612247, 6.053332678701448, 28.68689029688477, 3.173708950617885, 2.825194211081467, 4.883598225764441, 2.912327401250551, 3.007920125075707, 3.0088662005258207, 3.0291768670656483, 3.0292979458468574, 3.1100826411907576, 2.851504282915339, 2.9923068632265633, 15.218015677822315, 3.5933083405684023, 3.0796964744487036, 3.659730805376795, 3.7833821759621524, 3.933507204945272, 3.887351619658989, 3.7588929398212256, 4.81365425015836, 9.571747035188391, 7.4589416907038455, 9.250392904830447, 1.3969522477158407, 1.5408572323104366, 1.631869029727323, 1.5926432110307842, 1.5929056725397441, 1.5929067161840555, 1.6386980189960088, 29.27488705391219, 4.041296193935749, 4.067175503006723, 3.0921010519690983, 3.0937656449401407, 28.68689029688477, 7.840898850400585, 29.488544177113013], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.7666, -3.6807, -3.6825, -4.3084, -4.3119, -4.3489, -4.5831, -4.5829, -4.5842, -4.5891, -4.5897, -3.5189, -4.0595, -4.9624, -4.9628, -4.9631, -4.9632, -4.9634, -2.5288, -4.449, -4.3082, -4.3093, -4.3133, -3.949, -4.1692, -4.0612, -4.8146, -4.8169, -5.5796, -5.5796, -4.3076, -4.9571, -4.6672, -2.9912, -4.2848, -4.4586, -4.2595, -4.1843, -3.7256, -4.534, -4.523, -4.5235, -3.5986, -4.0539, -4.2717, -4.2725, -4.2728, -4.2733, -4.5451, -4.546, -4.5464, -4.5465, -4.5471, -4.5473, -4.5475, -4.5485, -4.5489, -4.5599, -3.4931, -4.0613, -4.926, -4.9282, -4.9364, -4.2719, -4.4853, -4.5465, -4.2207, -4.5459, -4.5454, -4.5645, -5.5439, -5.544, -4.2714, -3.7427, -3.6704, -3.5051, -4.0954, -4.0567, -3.2173, -4.1183, -4.2728, -4.0744, -3.8325, -4.0767, -4.5438, -4.5444, -4.5452, -3.7304, -3.7342, -3.7405, -3.7673, -3.1994, -4.0048, -4.0109, -4.0346, -3.565, -3.3383, -3.1348, -5.0034, -5.0034, -5.009, -4.2987, -4.3842, -3.8027, -4.0035, -3.5141, -4.2333, -4.559, -4.5835, -4.6674, -4.8925, -2.6107, -4.586, -4.9058, -5.0053, -3.7059, -4.9726, -4.3832, -4.1344, -4.2971, -4.3791, -3.5504, -4.3514, -4.7121, -4.1719, -3.6004, -3.6005, -3.6016, -3.6032, -3.6076, -3.9904, -3.9915, -4.603, -3.9811, -3.8982, -3.9821, -3.9814, -3.9812, -3.9842, -4.2233, -3.9796, -3.6001, -4.5977, -4.5991, -4.5989, -4.5979, -4.598, -4.5177, -4.5988, -4.6023, -4.5988, -4.5989, -4.6026, -4.5992, -4.599, -3.9815, -3.9212, -3.9822, -3.9148, -3.8895, -3.9801, -3.284, -3.9778, -4.5568, -4.597, -4.598, -2.3553, -3.1721, -4.1712, -3.5514, -3.551, -3.5511, -3.5506, -4.1683, -4.1724, -3.5508, -4.168, -3.1718, -4.1713, -4.1655, -4.1667, -3.5501, -4.1704, -4.1705, -4.1677, -3.5514, -3.5507, -4.172, -4.1684, -4.1726, -6.1144, -3.1703, -6.1144, -6.1144, -6.1144, -6.1143, -6.1099, -6.0611, -6.0216, -6.0487, -6.0366, -6.0519, -6.017, -2.769, -3.3924, -3.395, -4.0154, -4.0115, -4.0112, -4.0185, -4.0215, -4.0238, -4.1457, -4.1276, -2.5339, -4.0113, -4.1772, -4.0154, -4.0041, -4.0096, -4.0252, -4.0889, -4.0193, -3.4295, -4.1629, -3.9587, -5.9576, -5.9334, -5.9081, -5.9463, -5.9576, -5.9576, -5.941, -3.5587, -5.3344, -5.7732, -5.9114, -5.9147, -5.8377, -5.8947, -5.8761], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.056, 1.0213, 1.0196, 0.9934, 0.9901, 0.9548, 0.9485, 0.9482, 0.9474, 0.9426, 0.942, 0.9117, 0.8962, 0.8674, 0.8672, 0.8669, 0.8668, 0.8666, 0.8658, 0.8503, 0.8039, 0.8026, 0.7985, 0.7927, 0.7849, 0.7579, 0.7131, 0.711, 0.677, 0.677, 0.6638, 0.6593, 0.6295, 0.3759, 0.5297, 0.4919, 0.3476, 0.1347, -0.3513, 0.4158, 0.1688, -0.4949, 1.0855, 1.0434, 1.0131, 1.0123, 1.0121, 1.0116, 0.9702, 0.9692, 0.9688, 0.9687, 0.9684, 0.9682, 0.968, 0.9669, 0.9665, 0.9561, 0.9076, 0.8896, 0.8893, 0.8871, 0.8792, 0.8383, 0.8028, 0.7657, 0.7652, 0.7539, 0.7437, 0.725, 0.7018, 0.7017, 0.69, 0.6708, 0.6485, 0.5235, 0.5963, 0.5543, 0.1498, 0.4977, 0.5676, 0.3098, -0.4581, -0.682, 0.223, 0.5691, 0.6345, 1.6159, 1.6119, 1.6076, 1.5872, 1.5789, 1.5684, 1.5621, 1.5373, 1.4474, 1.4276, 1.3916, 1.2822, 1.2822, 1.2767, 1.2588, 1.1747, 1.1359, 1.1356, 1.1151, 1.0942, 1.0673, 0.9703, 0.8839, 0.8108, 0.7637, 0.7571, 0.7493, 0.6967, 0.6783, 0.643, 0.5782, 0.4766, 0.4439, 0.4495, -0.1833, 0.1408, 0.6052, -0.7773, 2.0215, 2.0213, 2.0204, 2.0191, 2.0141, 1.9191, 1.9179, 1.7158, 1.701, 1.6981, 1.6878, 1.6038, 1.4457, 1.4362, 1.353, 1.1764, 1.1667, 1.0574, 1.0508, 1.0506, 1.0449, 1.0448, 1.0401, 1.0384, 0.967, 0.9629, 0.9505, 0.9473, 0.8729, 0.8626, 0.8589, 0.8198, 0.7837, 0.7012, 0.6028, 0.649, 0.0904, 0.4528, -1.1897, 0.0101, 0.7954, 2.4732, 2.3301, 2.2456, 2.1506, 1.9106, 1.871, 1.6147, 1.535, 1.4974, 1.4848, 1.4693, 1.4574, 1.4443, 1.4403, 1.4333, 1.3885, 1.3016, 1.2833, 1.1998, 0.975, 0.8335, 0.814, 0.6099, 0.5942, 0.2044, 0.2041, 0.1713, 0.1712, 0.1711, 0.1471, 0.1359, -0.5295, -1.3195, -0.9366, -1.214, -1.1014, -2.6223, 2.8272, 2.3201, 1.7702, 1.6667, 1.6384, 1.6383, 1.6243, 1.6213, 1.5926, 1.5576, 1.5275, 1.4947, 1.4607, 1.449, 1.4383, 1.4164, 1.3719, 1.3682, 1.338, 1.1604, 1.0627, 0.5788, 0.5677, 0.4592, 0.3854, 0.3533, 0.3394, 0.3279, 0.3279, 0.3162, -0.1843, 0.0202, -0.4251, -0.2892, -0.293, -2.4431, -1.2029, -2.509]}, \"token.table\": {\"Topic\": [2, 4, 5, 6, 2, 3, 2, 3, 4, 2, 3, 1, 2, 1, 2, 2, 4, 6, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 1, 3, 5, 6, 1, 6, 2, 2, 2, 2, 3, 1, 3, 3, 4, 1, 2, 4, 5, 3, 2, 3, 5, 1, 2, 4, 1, 2, 3, 3, 5, 1, 1, 2, 3, 4, 1, 3, 6, 1, 2, 3, 1, 2, 1, 4, 1, 2, 3, 4, 1, 4, 6, 4, 1, 4, 6, 3, 3, 1, 4, 1, 2, 1, 2, 1, 2, 3, 4, 5, 6, 3, 3, 4, 6, 1, 2, 1, 1, 2, 1, 2, 3, 4, 1, 3, 2, 1, 2, 1, 2, 4, 2, 1, 2, 3, 5, 2, 4, 5, 6, 1, 3, 1, 3, 6, 1, 3, 1, 3, 2, 3, 1, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 4, 4, 6, 3, 5, 1, 2, 3, 4, 6, 2, 2, 4, 1, 2, 3, 5, 3, 1, 2, 3, 4, 2, 4, 3, 4, 5, 2, 2, 2, 1, 4, 6, 1, 1, 3, 2, 2, 3, 2, 2, 4, 6, 2, 4, 2, 3, 3, 4, 1, 2, 3, 4, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 4, 1, 3, 4, 5, 1, 4, 5, 1, 1, 1, 2, 1, 2, 3, 1, 3, 4, 5, 1, 3, 4, 5, 2, 2, 4, 2, 1, 5, 1, 1, 2, 4, 1, 5, 1, 1, 2, 2, 1, 2, 5, 1, 2, 3, 5, 1, 6, 3, 5, 6, 2, 4, 6, 4, 5, 2, 2, 3, 4, 6, 1, 4, 1, 2, 4, 1, 4, 6, 1, 3, 6, 3, 4, 6, 3, 4, 3, 5, 1, 1, 2, 2, 4, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 5, 1, 2, 5, 1, 1, 4], \"Freq\": [0.2732441409435968, 0.2732441409435968, 0.2732441409435968, 0.2732441409435968, 0.7751947020118188, 0.1937986755029547, 0.30672083720745086, 0.30672083720745086, 0.30672083720745086, 0.3031145842002565, 0.606229168400513, 0.3513794537290734, 0.6149140440258785, 0.7055172566952622, 0.923187346774149, 0.33010949001269146, 0.33010949001269146, 0.33010949001269146, 0.40575086699146845, 0.20287543349573423, 0.40575086699146845, 0.19310936072091894, 0.5793280821627569, 0.09655468036045947, 0.09655468036045947, 0.6099108009148986, 0.9389369532477166, 0.8717633090451002, 0.10810351628175834, 0.64862109769055, 0.10810351628175834, 0.10810351628175834, 0.6430697286018933, 0.32153486430094663, 0.7845194948700904, 0.6032698099880411, 0.8719570852924555, 0.7027960949570404, 0.23426536498568012, 0.4768092944740338, 0.2384046472370169, 0.9235317793917309, 0.6465309696209924, 0.137472135955882, 0.41241640786764605, 0.274944271911764, 0.137472135955882, 0.6277835638601188, 0.3212628285294298, 0.3212628285294298, 0.3212628285294298, 0.14797968594892158, 0.5919187437956863, 0.29595937189784316, 0.19949036131089584, 0.19949036131089584, 0.5984710839326876, 0.834388765381634, 0.13906479423027232, 0.8862144386066317, 0.47476063639878974, 0.3730262143133348, 0.13564589611393993, 0.03391147402848498, 0.6703363838105277, 0.13406727676210556, 0.13406727676210556, 0.7669008307390178, 0.1742956433497768, 0.06971825733991072, 0.7764755605007763, 0.19411889012519407, 0.6127942756331952, 0.6465199271919075, 0.5884630092504032, 0.2942315046252016, 0.9237741802249418, 0.6468093915386008, 0.5144890906924048, 0.2572445453462024, 0.2572445453462024, 0.6489893930669691, 0.2643137683402787, 0.5286275366805574, 0.2643137683402787, 0.7360915078521889, 0.9225665205720202, 0.30887003278766684, 0.30887003278766684, 0.7006366109477778, 0.2335455369825926, 0.23177181925932425, 0.6953154577779728, 0.23911279271919667, 0.20495382233074, 0.37574867427302333, 0.10247691116537, 0.06831794077691333, 0.034158970388456666, 0.7376126252191977, 0.33419032395685305, 0.33419032395685305, 0.33419032395685305, 0.5885960343770488, 0.2942980171885244, 0.7961459942916865, 0.2318802146594115, 0.6956406439782344, 0.2679383794537559, 0.13396918972687796, 0.2679383794537559, 0.2679383794537559, 0.3019849558488367, 0.3019849558488367, 0.8718721957817659, 0.5767846062489221, 0.4325884546866916, 0.6747436051733348, 0.1686859012933337, 0.1686859012933337, 0.9566372277298549, 0.2506652169924446, 0.2506652169924446, 0.2506652169924446, 0.2506652169924446, 0.20476704957510455, 0.20476704957510455, 0.20476704957510455, 0.20476704957510455, 0.30127160798539704, 0.30127160798539704, 0.32470732369137445, 0.6494146473827489, 0.32470732369137445, 0.4817153002130099, 0.4817153002130099, 0.3035606365654327, 0.6071212731308654, 0.9232714860063033, 0.6278870201900298, 0.7961900959792688, 0.4687230067883438, 0.35154225509125786, 0.11718075169708594, 0.11718075169708594, 0.1971347686526564, 0.5256927164070837, 0.06571158955088546, 0.1971347686526564, 0.30320939435320815, 0.30320939435320815, 0.30320939435320815, 0.6867359758869149, 0.34336798794345746, 0.2868270144794031, 0.5736540289588062, 0.20894827168409763, 0.20894827168409763, 0.20894827168409763, 0.20894827168409763, 0.10447413584204882, 0.8719294732727937, 0.6016306931725615, 0.30081534658628073, 0.7764677382740734, 0.19411693456851836, 0.3502695602045064, 0.3502695602045064, 0.6277831525474296, 0.11823487511289642, 0.47293950045158567, 0.11823487511289642, 0.23646975022579284, 0.311627071010547, 0.623254142021094, 0.4897457808034339, 0.16324859360114463, 0.16324859360114463, 0.8724150167403154, 0.885934375178504, 0.6032669592442642, 0.3323511028257896, 0.3323511028257896, 0.3323511028257896, 0.6102405619631347, 0.8722199560299247, 0.742336086254136, 0.7845786671728939, 0.8262824443878563, 0.16525648887757124, 0.9231969545176807, 0.6232271459673917, 0.2077423819891306, 0.2077423819891306, 0.8718968880888989, 0.8619825658903791, 0.6680250878118442, 0.3340125439059221, 0.6867512567658433, 0.27470050270633734, 0.19444679427852668, 0.58334038283558, 0.19444679427852668, 0.8620932787953611, 0.7847656984619371, 0.8861632649436256, 0.8719401142594836, 0.7024743752039729, 0.23415812506799097, 0.49522495251108123, 0.49522495251108123, 0.9019860138618344, 0.8302802968976101, 0.166056059379522, 0.31511761382122916, 0.6302352276424583, 0.6099107955458853, 0.47919070138247394, 0.23959535069123697, 0.23959535069123697, 0.32829037648796194, 0.32829037648796194, 0.32829037648796194, 0.8862070059613588, 0.7960210515664503, 0.6631515217749114, 0.16578788044372786, 0.902108213754881, 0.8717754490196886, 0.7362205380432668, 0.24461238755862305, 0.24461238755862305, 0.24461238755862305, 0.7158440824552893, 0.9390542881938055, 0.27540850050583343, 0.27540850050583343, 0.27540850050583343, 0.6033584864277621, 0.6013989300894947, 0.30069946504474737, 0.9230870807641455, 0.529486645126129, 0.2647433225630645, 0.7961201827247362, 0.7766912681039303, 0.19417281702598257, 0.6466573310786643, 0.6362426792066376, 0.3181213396033188, 0.7961119009756815, 0.2315420723829203, 0.6946262171487608, 0.8717998119095673, 0.3162880609172174, 0.3162880609172174, 0.3162880609172174, 0.09377376547138769, 0.46886882735693847, 0.37509506188555075, 0.09377376547138769, 0.353957967235536, 0.353957967235536, 0.35069209118550354, 0.35069209118550354, 0.35069209118550354, 0.3301226847703667, 0.3301226847703667, 0.3301226847703667, 0.6783272005899582, 0.3391636002949791, 0.87171966879285, 0.5882012189097765, 0.2352804875639106, 0.1176402437819553, 0.6301775087506443, 0.6088570870187608, 0.3044285435093804, 0.38260919535350624, 0.5101455938046751, 0.6464613773415591, 0.33245563659202243, 0.33245563659202243, 0.33245563659202243, 0.5084521003255228, 0.2542260501627614, 0.2542260501627614, 0.26603577596108935, 0.5320715519221787, 0.26603577596108935, 0.703004607455296, 0.175751151863824, 0.29247880279257593, 0.7311970069814399, 0.8857143286761158, 0.49559476725200496, 0.33039651150133664, 0.7115674613937506, 0.23718915379791686, 0.09533140464362025, 0.762651237148962, 0.09533140464362025, 0.7242968759196577, 0.2897187503678631, 0.2782950710658514, 0.2782950710658514, 0.2782950710658514, 0.2782950710658514, 0.17116719277655623, 0.6846687711062249, 0.17116719277655623, 0.35972303710534687, 0.35972303710534687, 0.17986151855267343, 0.8861060830622712, 0.7857920082415617, 0.19644800206039043], \"Term\": [\"ability_actually\", \"ability_actually\", \"ability_actually\", \"ability_actually\", \"absolute_cost\", \"absolute_cost\", \"absolute_priority\", \"absolute_priority\", \"absolute_priority\", \"across_entire\", \"across_entire\", \"adjusted_pretax\", \"adjusted_pretax\", \"adversely_affected\", \"advisory_led\", \"amongst_others\", \"amongst_others\", \"amongst_others\", \"anke_reingen\", \"anke_reingen\", \"anke_reingen\", \"asset_management\", \"asset_management\", \"asset_management\", \"asset_management\", \"asset_outflows\", \"assets_management\", \"attracting_third\", \"axel_lehmann\", \"axel_lehmann\", \"axel_lehmann\", \"axel_lehmann\", \"back_june\", \"back_june\", \"bank_franchises\", \"bank_reported\", \"bank_switzerland\", \"banking_business\", \"banking_business\", \"basis_points\", \"basis_points\", \"best_wishes\", \"board_director\", \"board_directors\", \"board_directors\", \"board_directors\", \"board_directors\", \"board_member\", \"capital_base\", \"capital_base\", \"capital_base\", \"capital_light\", \"capital_light\", \"capital_light\", \"capital_ratio\", \"capital_ratio\", \"capital_ratio\", \"cet_ratio\", \"cet_ratio\", \"challenging_environment\", \"chf_billion\", \"chf_billion\", \"chf_billion\", \"chf_billion\", \"chf_billion_chf_billion\", \"chf_billion_chf_billion\", \"chf_billion_chf_billion\", \"chf_million\", \"chf_million\", \"chf_million\", \"client_business\", \"client_business\", \"comes_line\", \"compensation_costs\", \"compliance_costs\", \"compliance_costs\", \"conference_call\", \"corporate_functions\", \"cost_base\", \"cost_base\", \"cost_base\", \"cost_efficiency\", \"cost_program\", \"cost_program\", \"cost_program\", \"cost_target\", \"could_talk\", \"covid_pandemic\", \"covid_pandemic\", \"credit_losses\", \"credit_losses\", \"credit_spreads\", \"credit_spreads\", \"cs_ceo\", \"cs_ceo\", \"cs_ceo\", \"cs_ceo\", \"cs_ceo\", \"cs_ceo\", \"current_perimeter\", \"detailed_work\", \"detailed_work\", \"detailed_work\", \"driven_outflows\", \"driven_outflows\", \"due_market\", \"equity_derivatives\", \"equity_derivatives\", \"executive_board\", \"executive_board\", \"executive_board\", \"executive_board\", \"exit_prime\", \"exit_prime\", \"expenses_chf\", \"first_half\", \"first_half\", \"first_half_year\", \"first_half_year\", \"first_half_year\", \"funding_costs\", \"give_details\", \"give_details\", \"give_details\", \"give_details\", \"give_us\", \"give_us\", \"give_us\", \"give_us\", \"going_comment\", \"going_comment\", \"going_forward\", \"going_forward\", \"going_forward\", \"group_ceo\", \"group_ceo\", \"group_strategy\", \"group_strategy\", \"growth_wealth\", \"half_year\", \"higher_technology\", \"interest_rates\", \"interest_rates\", \"interest_rates\", \"interest_rates\", \"investment_bank\", \"investment_bank\", \"investment_bank\", \"investment_bank\", \"investment_banking\", \"investment_banking\", \"investment_banking\", \"investor_day\", \"investor_day\", \"kian_abouhossein\", \"kian_abouhossein\", \"last_year\", \"last_year\", \"last_year\", \"last_year\", \"last_year\", \"leading_universal\", \"less_complex\", \"less_complex\", \"let_turn\", \"let_turn\", \"leverage_exposure\", \"leverage_exposure\", \"leverage_ratio\", \"leveraged_finance\", \"leveraged_finance\", \"leveraged_finance\", \"leveraged_finance\", \"little_bit\", \"little_bit\", \"look_think\", \"look_think\", \"look_think\", \"loss_chf\", \"loss_chf_billion\", \"loss_investment\", \"lot_work\", \"lot_work\", \"lot_work\", \"lower_year\", \"lower_year_year\", \"magdalena_stoklosa\", \"major_litigation\", \"major_litigation_provisions\", \"major_litigation_provisions\", \"management_swiss\", \"market_environment\", \"market_environment\", \"market_environment\", \"market_leading\", \"market_losses\", \"markets_business\", \"markets_business\", \"mean_think\", \"mean_think\", \"medium_term\", \"medium_term\", \"medium_term\", \"million_mark\", \"million_quarter\", \"minus_basis\", \"multi_specialist\", \"net_asset\", \"net_asset\", \"net_interest_income\", \"net_interest_income\", \"net_new\", \"net_revenues\", \"net_revenues\", \"new_leadership\", \"new_leadership\", \"next_comes\", \"next_comes_line\", \"next_comes_line\", \"next_comes_line\", \"next_months\", \"next_months\", \"next_months\", \"nonspecific_provisions\", \"offset_higher\", \"operating_expenses\", \"operating_expenses\", \"partly_offset\", \"party_capital\", \"piers_brown\", \"primarily_due\", \"primarily_due\", \"primarily_due\", \"products_business\", \"quarter_quarter\", \"questions_first\", \"questions_first\", \"questions_first\", \"quite_clearly\", \"regulatory_remediation\", \"regulatory_remediation\", \"reported_pretax\", \"restructuring_costs\", \"restructuring_costs\", \"revenues_higher\", \"revenues_lower\", \"revenues_lower\", \"right_direction\", \"risk_appetite\", \"risk_appetite\", \"risk_compliance\", \"risk_culture\", \"risk_culture\", \"risk_management\", \"risk_weighted\", \"risk_weighted\", \"risk_weighted\", \"securitized_products\", \"securitized_products\", \"securitized_products\", \"securitized_products\", \"short_term\", \"short_term\", \"sp_business\", \"sp_business\", \"sp_business\", \"speed_transformation\", \"speed_transformation\", \"speed_transformation\", \"strategic_direction\", \"strategic_direction\", \"strategic_options\", \"strategic_review\", \"strategic_review\", \"strategic_review\", \"strategy_review\", \"supply_chain\", \"supply_chain\", \"swiss_bank\", \"swiss_bank\", \"tax_charge\", \"think_know\", \"think_know\", \"think_know\", \"think_quite\", \"think_quite\", \"think_quite\", \"think_second\", \"think_second\", \"think_second\", \"think_terms\", \"think_terms\", \"third_party\", \"third_party\", \"totaled_chf\", \"totaling_chf\", \"totaling_chf\", \"transform_investment\", \"transform_investment\", \"wealth_management\", \"wealth_management\", \"wealth_management\", \"wealth_management_division\", \"wealth_management_division\", \"well_look\", \"well_look\", \"well_look\", \"well_look\", \"world_class\", \"world_class\", \"world_class\", \"would_like\", \"would_like\", \"would_like\", \"would_note\", \"year_year\", \"year_year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 6, 2, 4, 5, 1]};\n","\n","function LDAvis_load_lib(url, callback){\n","  var s = document.createElement('script');\n","  s.src = url;\n","  s.async = true;\n","  s.onreadystatechange = s.onload = callback;\n","  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n","  document.getElementsByTagName(\"head\")[0].appendChild(s);\n","}\n","\n","if(typeof(LDAvis) !== \"undefined\"){\n","   // already loaded: just create the visualization\n","   !function(LDAvis){\n","       new LDAvis(\"#\" + \"ldavis_el652133977166984496120428035\", ldavis_el652133977166984496120428035_data);\n","   }(LDAvis);\n","}else if(typeof define === \"function\" && define.amd){\n","   // require.js is available: use it to load d3/LDAvis\n","   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n","   require([\"d3\"], function(d3){\n","      window.d3 = d3;\n","      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n","        new LDAvis(\"#\" + \"ldavis_el652133977166984496120428035\", ldavis_el652133977166984496120428035_data);\n","      });\n","    });\n","}else{\n","    // require.js not available: dynamically load d3 & LDAvis\n","    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n","         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n","                 new LDAvis(\"#\" + \"ldavis_el652133977166984496120428035\", ldavis_el652133977166984496120428035_data);\n","            })\n","         });\n","}\n","</script>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2024-11-25 23:14:55,951 - INFO - LDA visualization successfully displayed or saved.\n","[INFO] Gensim LDA modeling for earnings calls completed successfully.\n","[RESULT] Response from the AI Assistant:\n","Gensim LDA modeling completed successfully.\n","\n","RAG Based Analysis:\n","-------------------\n","\n","\n","--- Query Selection Menu ---\n","1. Ask a generic question\n","2. Compare two quarters of the same year\n","3. Compare two quarters of different years\n","4. Year-over-year comparison\n","5. Analyze all quarters of a year\n","6. Analyze sentiment for a single quarter\n","7. Summarize a single quarter\n","8. Perform Topic Modeling with BERTopic\n","9. Perform Topic Modeling with Gensim LDA\n","10. Exit\n","\n","Select an option (1-10): 10\n","Exiting the application. Goodbye!\n"]}]},{"cell_type":"markdown","source":["#Enhanced RAG Pipeline for future implementation\n"],"metadata":{"id":"jbKIDX3ZVmVR"}},{"cell_type":"code","source":["def extract_lda_topics_for_document(row, dictionary, lda_model):\n","    if \"lda_topics\" not in row or not isinstance(row[\"lda_topics\"], str) or not row[\"lda_topics\"]:\n","        logger.warning(f\"Skipping row with missing or empty 'lda_topics': {row}\")\n","        return []\n","\n","    doc_bow = dictionary.doc2bow(row[\"lda_topics\"].split())  # Tokenized content\n","    topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0.05)\n","    return [f\"Topic {topic[0]}: {topic[1]:.2f}\" for topic in topic_distribution]\n","\n","def extract_lda_topics_for_context(context, dictionary, lda_model):\n","    \"\"\"\n","    Extract topics for a given context using the LDA model.\n","\n","    Args:\n","        context (str): The text content to analyze.\n","        dictionary (Dictionary): Gensim dictionary for the corpus.\n","        lda_model (LdaModel): Trained Gensim LDA model.\n","\n","    Returns:\n","        list: A list of topics and their probabilities.\n","    \"\"\"\n","    if not isinstance(context, str) or not context.strip():\n","        logger.warning(f\"Invalid or empty context: {context}\")\n","        return []\n","\n","    doc_bow = dictionary.doc2bow(context.split())  # Tokenized content\n","    topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0.05)\n","    return [f\"Topic {topic[0]}: {topic[1]:.2f}\" for topic in topic_distribution]\n","\n","\n","def configure_rag_pipeline_future(\n","    folder_path: str,\n","    metadata_path: str,\n","    context: str = None,\n","    model_name: str = \"microsoft/Phi-3-mini-4k-instruct\",\n","    embeddings_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n","    model_token_limit: int = 4096,\n","    top_k: int = 3,\n","    max_new_tokens: int = 500,\n","    chunk_size: int = None,\n","    chunk_overlap: int = 200,\n","    required_metadata_fields: Set[str] = None,\n","    persist_directory: str = \"qa_index\",\n","    device: str = \"cuda\",\n","    num_topics: int = 3,\n","    chain_type: str = \"stuff\",\n",") -> dict:\n","    \"\"\"\n","    Configures a Retrieval-Augmented Generation (RAG) pipeline with QA chains, LDA, and other components.\n","\n","    Returns:\n","        dict: A dictionary containing all pipeline components.\n","    \"\"\"\n","    try:\n","        # Step 1: Validate file paths\n","        if not os.path.exists(folder_path):\n","            raise FileNotFoundError(f\"The specified folder path does not exist: {folder_path}\")\n","        if not os.path.exists(metadata_path):\n","            raise FileNotFoundError(f\"The metadata file does not exist: {metadata_path}\")\n","\n","        # Step 2: Calculate dynamic chunk size\n","        if chunk_size is None:\n","            prompt_tokens = 200\n","            token_budget = model_token_limit - prompt_tokens - max_new_tokens\n","            chunk_size = min(2000, token_budget // 2)\n","            logger.info(f\"Dynamic chunk size set to: {chunk_size}\")\n","\n","        # Step 3: Adjust chunk overlap if necessary\n","        if chunk_overlap is None or chunk_overlap >= chunk_size:\n","            chunk_overlap = chunk_size // 5\n","            logger.info(f\"Dynamic chunk overlap set to: {chunk_overlap}\")\n","\n","        # Step 4: Load and preprocess documents\n","        logger.info(\"Loading and preprocessing documents...\")\n","        enriched_documents, document_chunks = load_and_preprocess_data(\n","            folder_path=folder_path,\n","            metadata_path=metadata_path,\n","            chunk_size=chunk_size,\n","            chunk_overlap=chunk_overlap,\n","            required_metadata_fields=required_metadata_fields,\n","        )\n","\n","        # Step 5: Initialize embeddings and vector store\n","        logger.info(\"Initializing embeddings and vector store...\")\n","        vector_store = initialize_embeddings_and_vector_store(\n","            chunks=document_chunks,\n","            embeddings_model_name=embeddings_model_name,\n","            persist_directory=persist_directory,\n","            use_existing=True,\n","        )\n","\n","        # Step 6: Setup retriever and language model\n","        retriever = setup_retriever(vector_store, top_k=top_k)\n","        llm_pipeline = load_model(model_name, max_new_tokens=max_new_tokens, device=device)\n","\n","        # Step 7: Load and validate metadata\n","        metadata_df = pd.read_csv(metadata_path)\n","        if metadata_df.empty:\n","            raise ValueError(\"Metadata file is empty. Ensure the metadata CSV is correctly populated.\")\n","        logger.info(f\"Loaded metadata with {len(metadata_df)} entries.\")\n","\n","        # Step 8: Initialize summarization and sentiment pipelines\n","        summarization_pipeline = hf_pipeline(\n","            \"summarization\", model=\"sshleifer/distilbart-cnn-6-6\", device=0 if device == \"cuda\" else -1\n","        )\n","        sentiment_pipeline = hf_pipeline(\n","            \"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\", device=0 if device == \"cuda\" else -1\n","        )\n","\n","        # Step 9: Perform LDA on metadata\n","        lda_model_metadata, dictionary_metadata, topics_metadata = None, None, None\n","        try:\n","            logger.info(\"Performing LDA on metadata...\")\n","            corpus = [doc.page_content for doc in enriched_documents if doc.page_content]\n","            processed_corpus = preprocess_corpus_gensim(corpus, custom_stopwords=None, redundant_terms=None)\n","            dictionary_metadata = Dictionary(processed_corpus)\n","            bow_corpus_metadata = [dictionary_metadata.doc2bow(doc) for doc in processed_corpus]\n","            lda_model_metadata = LdaModel(\n","                corpus=bow_corpus_metadata,\n","                id2word=dictionary_metadata,\n","                num_topics=num_topics,\n","                random_state=42,\n","                passes=10,\n","                iterations=50,\n","            )\n","            topics_metadata = extract_topics_from_lda(lda_model_metadata, bow_corpus_metadata, dictionary_metadata)\n","            metadata_df[\"lda_topics\"] = topics_metadata\n","        except Exception as e:\n","            logger.warning(f\"Error performing LDA on metadata: {e}\")\n","            metadata_df[\"lda_topics\"] = None\n","\n","        # Step 10: Perform LDA on context\n","        lda_model_context, dictionary_context, topics_context = None, None, None\n","        if context:\n","            try:\n","                logger.info(\"Performing LDA on context...\")\n","                processed_context = preprocess_context(context, nlp_model=None)\n","                dictionary_context = Dictionary([processed_context])\n","                bow_corpus_context = [dictionary_context.doc2bow(processed_context)]\n","                lda_model_context = LdaModel(\n","                    corpus=bow_corpus_context,\n","                    id2word=dictionary_context,\n","                    num_topics=num_topics,\n","                    random_state=42,\n","                    passes=10,\n","                    iterations=50,\n","                )\n","                topics_context = extract_lda_topics_for_context(context, dictionary_context, lda_model_context)\n","            except Exception as e:\n","                logger.warning(f\"Error performing LDA on context: {e}\")\n","\n","        # Step 11: Initialize QA Chains\n","        qa_chains = None\n","        try:\n","            logger.info(\"Initializing QA chains...\")\n","            qa_chains = initialize_chains(llm_pipeline, chain_type=chain_type)\n","        except Exception as e:\n","            logger.error(f\"Error initializing QA chains: {e}\")\n","\n","        # Step 12: Return pipeline components\n","        logger.info(\"RAG pipeline configured successfully.\")\n","        return {\n","            \"retriever\": retriever,\n","            \"vector_store\": vector_store,\n","            \"llm_pipeline\": llm_pipeline,\n","            \"metadata_df\": metadata_df,\n","            \"documents\": enriched_documents,\n","            \"summarization_pipeline\": summarization_pipeline,\n","            \"sentiment_pipeline\": sentiment_pipeline,\n","            \"lda_model_metadata\": lda_model_metadata,\n","            \"lda_topics_metadata\": topics_metadata,\n","            \"lda_model_context\": lda_model_context,\n","            \"lda_topics_context\": topics_context,\n","            \"qa_chains\": qa_chains,\n","        }\n","\n","    except Exception as e:\n","        logger.exception(f\"Pipeline configuration failed: {e}\")\n","        return None\n"],"metadata":{"id":"iC1PZ5EPWX3j"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"collapsed_sections":["qjtoeCRRDDA0","EUxV-Ea-HuNY","fAxYdyz-He30"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9a930fb2c6614cda8f32e1e0712d6c3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1746b352fd84fd0a83058c7af1d0217","IPY_MODEL_f78425841e9e41e3b02f1929894b8fc0","IPY_MODEL_1889fc7501d14e3f861108833654746f"],"layout":"IPY_MODEL_477b80477b414620beec28af88c41b0b"}},"b1746b352fd84fd0a83058c7af1d0217":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d087bcc43814867bbdc2fdc3e59b42a","placeholder":"​","style":"IPY_MODEL_39a133227495426b94271de0dd813fd3","value":"Loading checkpoint shards: 100%"}},"f78425841e9e41e3b02f1929894b8fc0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_821a1d3b9da941afacfc385133995003","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59138ebbaa5f48de80ed8b519d90e7f1","value":2}},"1889fc7501d14e3f861108833654746f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78a5681573254c6386f51144da3d9f0a","placeholder":"​","style":"IPY_MODEL_ddea114f55974ae5a2bbbb93f662e8be","value":" 2/2 [00:02&lt;00:00,  1.41s/it]"}},"477b80477b414620beec28af88c41b0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d087bcc43814867bbdc2fdc3e59b42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39a133227495426b94271de0dd813fd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"821a1d3b9da941afacfc385133995003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59138ebbaa5f48de80ed8b519d90e7f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78a5681573254c6386f51144da3d9f0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddea114f55974ae5a2bbbb93f662e8be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3771ba6993564ae69d0cd9c6dd98ad13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a79f2817243a43c5a32f2625319ea05b","IPY_MODEL_27722135e6b64ae6aadad443c314cefc","IPY_MODEL_2289794145d9458d9639dbd6485e1e70"],"layout":"IPY_MODEL_b1bb491ed8ff4272bf04f04e870c3003"}},"a79f2817243a43c5a32f2625319ea05b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b4dc13d7dc04d068c52d3d80830f0d6","placeholder":"​","style":"IPY_MODEL_d9f24573a3aa45919f2a4872f0346165","value":"Loading checkpoint shards: 100%"}},"27722135e6b64ae6aadad443c314cefc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51c5ff76bc1d4c17a02f82fc944be234","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d68cb0c2d4714859a0810174ab47d897","value":2}},"2289794145d9458d9639dbd6485e1e70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c590925a1b14988b8e7f9827489263e","placeholder":"​","style":"IPY_MODEL_22f3847518ed42f1bc37ba9ef2f7ef81","value":" 2/2 [00:03&lt;00:00,  1.58s/it]"}},"b1bb491ed8ff4272bf04f04e870c3003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b4dc13d7dc04d068c52d3d80830f0d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9f24573a3aa45919f2a4872f0346165":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51c5ff76bc1d4c17a02f82fc944be234":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d68cb0c2d4714859a0810174ab47d897":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c590925a1b14988b8e7f9827489263e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22f3847518ed42f1bc37ba9ef2f7ef81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b99db0af44f14c7e95c1f5145b4374b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b995255b58f34fcfa6096f4632c941a8","IPY_MODEL_e1d8ffbe2d8b405daf19e03c1e5940cd","IPY_MODEL_93acd38b7a53495398475f36d9eb7c5e"],"layout":"IPY_MODEL_5781de1d07b64d288915b72b20151d43"}},"b995255b58f34fcfa6096f4632c941a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51c925138ba641349b444d2a14ca08ff","placeholder":"​","style":"IPY_MODEL_5eedcddc423f4e27afd0b5fe4adb7072","value":"Loading checkpoint shards: 100%"}},"e1d8ffbe2d8b405daf19e03c1e5940cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b323c38b30604a258ae80d31e04db5a5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d19292a73b3f4690bb1f6d237b16a20a","value":2}},"93acd38b7a53495398475f36d9eb7c5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27b5ec1afef64d00905abf8be20c4951","placeholder":"​","style":"IPY_MODEL_67d06f08d23b415197cece74066f7bd5","value":" 2/2 [00:02&lt;00:00,  1.33s/it]"}},"5781de1d07b64d288915b72b20151d43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51c925138ba641349b444d2a14ca08ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eedcddc423f4e27afd0b5fe4adb7072":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b323c38b30604a258ae80d31e04db5a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d19292a73b3f4690bb1f6d237b16a20a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"27b5ec1afef64d00905abf8be20c4951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67d06f08d23b415197cece74066f7bd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}